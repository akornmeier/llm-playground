{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "setup-heading",
   "metadata": {},
   "source": [
    "# 02 - Supervised Fine-Tuning Training\n",
    "\n",
    "## Setup\n",
    "\n",
    "We use **SmolLM-135M** (`HuggingFaceTB/SmolLM-135M`) -- a 135-million\n",
    "parameter language model small enough to train on a CPU. This model is a\n",
    "base model: it has been trained only on next-token prediction and has no\n",
    "instruction-following ability. Our goal is to give it that ability through\n",
    "supervised fine-tuning.\n",
    "\n",
    "The training loop in this notebook is **real and functional**. It will run\n",
    "on a CPU, though slowly (expect a few minutes per epoch). The point is to\n",
    "understand every component: data loading, tokenization, forward pass, loss\n",
    "masking, backward pass, and parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "num_params = sum(p.numel() for p in base_model.parameters())\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Parameters: {num_params:,}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token!r} (id={tokenizer.pad_token_id})\")\n",
    "print(f\"EOS token: {tokenizer.eos_token!r} (id={tokenizer.eos_token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-heading",
   "metadata": {},
   "source": [
    "## Before SFT: Baseline\n",
    "\n",
    "Let's see what the base model does when given legal prompts in instruction\n",
    "format. Since it has never been instruction-tuned, it should not follow\n",
    "instructions -- it will simply continue the text as a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=80):\n",
    "    \"\"\"Generate text from a prompt using greedy decoding.\n",
    "\n",
    "    Args:\n",
    "        model: A HuggingFace causal LM.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        prompt: Input text.\n",
    "        max_new_tokens: Maximum tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        The generated text (prompt + completion).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Legal prompts in instruction format\n",
    "test_prompts = [\n",
    "    (\n",
    "        \"<|im_start|>system\\n\"\n",
    "        \"You are a legal research assistant. Answer the question about the \"\n",
    "        \"provided court opinion accurately and concisely.<|im_end|>\\n\"\n",
    "        \"<|im_start|>user\\n\"\n",
    "        \"What court issued this opinion?\\n\\n\"\n",
    "        \"Before the Court is the appeal of plaintiff James Henderson from the \"\n",
    "        \"district court's grant of summary judgment.<|im_end|>\\n\"\n",
    "        \"<|im_start|>assistant\\n\"\n",
    "    ),\n",
    "    (\n",
    "        \"<|im_start|>system\\n\"\n",
    "        \"You are a legal research assistant. Answer the question about the \"\n",
    "        \"provided court opinion accurately and concisely.<|im_end|>\\n\"\n",
    "        \"<|im_start|>user\\n\"\n",
    "        \"List the key legal citations in this opinion.\\n\\n\"\n",
    "        \"We review the district court's grant of summary judgment de novo. \"\n",
    "        \"Anderson v. Liberty Lobby, Inc., 477 U.S. 242 (1986).<|im_end|>\\n\"\n",
    "        \"<|im_start|>assistant\\n\"\n",
    "    ),\n",
    "    (\n",
    "        \"<|im_start|>system\\n\"\n",
    "        \"You are a legal research assistant. Answer the question about the \"\n",
    "        \"provided court opinion accurately and concisely.<|im_end|>\\n\"\n",
    "        \"<|im_start|>user\\n\"\n",
    "        \"Summarize this court opinion.\\n\\n\"\n",
    "        \"Plaintiff Maria Thompson brings this action under the Individuals \"\n",
    "        \"with Disabilities Education Act challenging the Board of Education's \"\n",
    "        \"determination that D.T. is not eligible for special education services.<|im_end|>\\n\"\n",
    "        \"<|im_start|>assistant\\n\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(\"BASE MODEL OUTPUT (before SFT)\")\n",
    "print(\"=\" * 70)\n",
    "print(\"The base model has no instruction-following ability.\")\n",
    "print(\"It will just continue the text, ignoring the chat template.\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "base_outputs = []\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    output = generate_text(base_model, tokenizer, prompt, max_new_tokens=60)\n",
    "    generated = output[len(prompt):]\n",
    "    base_outputs.append(generated)\n",
    "    print(f\"\\n--- Prompt {i + 1} ---\")\n",
    "    print(f\"Instruction: {prompt.split(chr(10))[3][:60]}\")\n",
    "    print(f\"Generated: {generated[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-heading",
   "metadata": {},
   "source": [
    "## Manual Training Loop\n",
    "\n",
    "Now we implement SFT from scratch. The steps are:\n",
    "\n",
    "1. Load the instruction dataset (from notebook 01).\n",
    "2. Tokenize each example with the ChatML template.\n",
    "3. Create a PyTorch `Dataset` and `DataLoader`.\n",
    "4. For each batch: forward pass, compute masked cross-entropy loss,\n",
    "   backward pass, optimizer step.\n",
    "5. Repeat for several epochs.\n",
    "\n",
    "The key detail is **loss masking**: we only compute loss on the assistant\n",
    "response tokens. The instruction tokens provide context but do not\n",
    "contribute to the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the instruction dataset built in notebook 01\n",
    "dataset_path = Path(\"sft_dataset.json\")\n",
    "\n",
    "if not dataset_path.exists():\n",
    "    # If notebook 01 has not been run, build the dataset here\n",
    "    print(\"sft_dataset.json not found -- building dataset from court opinions...\")\n",
    "    opinions_path = Path(\"../../datasets/sample/court_opinions.jsonl\")\n",
    "    opinions = []\n",
    "    with open(opinions_path) as f:\n",
    "        for line in f:\n",
    "            opinions.append(json.loads(line))\n",
    "\n",
    "    raw_dataset = []\n",
    "    for op in opinions:\n",
    "        text = op[\"text\"]\n",
    "        sentences = [s.strip() for s in text.split(\".\") if s.strip()]\n",
    "        summary = \". \".join(sentences[:2]) + \".\" if len(sentences) >= 2 else text[:300]\n",
    "\n",
    "        paragraphs = text.split(\"\\n\\n\")\n",
    "        holding = paragraphs[-1].strip()\n",
    "        for keyword in [\"REVERSE\", \"AFFIRM\", \"REMAND\", \"GRANTED\", \"DENIED\"]:\n",
    "            for sent in sentences:\n",
    "                if keyword in sent:\n",
    "                    holding = sent.strip() + \".\"\n",
    "                    break\n",
    "\n",
    "        citation_list = \"\\n\".join(f\"- {c}\" for c in op[\"citations\"])\n",
    "        issue_summary = f\"In {op['case_name']}, the key legal issue is: {sentences[0]}.\" if sentences else \"\"\n",
    "\n",
    "        raw_dataset.append({\"instruction\": \"Summarize this court opinion.\", \"input\": text, \"output\": summary})\n",
    "        raw_dataset.append({\"instruction\": \"What was the holding in this case?\", \"input\": text, \"output\": holding})\n",
    "        raw_dataset.append({\"instruction\": \"List the key legal citations in this opinion.\", \"input\": text, \"output\": citation_list})\n",
    "        raw_dataset.append({\"instruction\": \"What court issued this opinion?\", \"input\": text, \"output\": op[\"court\"]})\n",
    "        raw_dataset.append({\"instruction\": \"What are the key legal issues in this case?\", \"input\": text, \"output\": issue_summary})\n",
    "        raw_dataset.append({\"instruction\": \"What is the name of this case?\", \"input\": text, \"output\": op[\"case_name\"]})\n",
    "else:\n",
    "    with open(dataset_path) as f:\n",
    "        raw_dataset = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(raw_dataset)} instruction pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "format-and-tokenize",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHATML_TEMPLATE = (\n",
    "    \"<|im_start|>system\\n\"\n",
    "    \"You are a legal research assistant. Answer the question about the \"\n",
    "    \"provided court opinion accurately and concisely.<|im_end|>\\n\"\n",
    "    \"<|im_start|>user\\n\"\n",
    "    \"{instruction}\\n\\n\"\n",
    "    \"{input}<|im_end|>\\n\"\n",
    "    \"<|im_start|>assistant\\n\"\n",
    "    \"{output}<|im_end|>\"\n",
    ")\n",
    "\n",
    "ASSISTANT_MARKER = \"<|im_start|>assistant\\n\"\n",
    "\n",
    "\n",
    "def tokenize_example(example, tokenizer, max_length=512):\n",
    "    \"\"\"Tokenize a single instruction example with loss mask.\n",
    "\n",
    "    Formats the example using ChatML, tokenizes it, and creates a label\n",
    "    tensor where instruction tokens are set to -100 (ignored by loss).\n",
    "\n",
    "    Args:\n",
    "        example: Dict with 'instruction', 'input', 'output' fields.\n",
    "        tokenizer: A HuggingFace tokenizer.\n",
    "        max_length: Maximum sequence length (truncates if exceeded).\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'input_ids', 'labels', and 'attention_mask' tensors.\n",
    "    \"\"\"\n",
    "    text = CHATML_TEMPLATE.format(\n",
    "        instruction=example[\"instruction\"],\n",
    "        input=example[\"input\"],\n",
    "        output=example[\"output\"],\n",
    "    )\n",
    "\n",
    "    # Find where the assistant response starts\n",
    "    marker_pos = text.find(ASSISTANT_MARKER)\n",
    "    prefix = text[: marker_pos + len(ASSISTANT_MARKER)]\n",
    "    prefix_token_ids = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "    response_start = len(prefix_token_ids)\n",
    "\n",
    "    # Tokenize the full text\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # Truncate to max_length\n",
    "    token_ids = token_ids[:max_length]\n",
    "\n",
    "    # Create labels: -100 for instruction tokens, token_id for response tokens\n",
    "    labels = [-100] * min(response_start, len(token_ids))\n",
    "    labels += token_ids[len(labels):]\n",
    "\n",
    "    # Attention mask: 1 for all real tokens\n",
    "    attention_mask = [1] * len(token_ids)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": token_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test on one example\n",
    "test_tokenized = tokenize_example(raw_dataset[3], tokenizer)  # \"What court issued this opinion?\"\n",
    "print(f\"Input IDs length: {len(test_tokenized['input_ids'])}\")\n",
    "print(f\"Labels length:    {len(test_tokenized['labels'])}\")\n",
    "print(f\"Masked tokens:    {sum(1 for l in test_tokenized['labels'] if l == -100)}\")\n",
    "print(f\"Trained tokens:   {sum(1 for l in test_tokenized['labels'] if l != -100)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pytorch-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFTDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for SFT training.\n",
    "\n",
    "    Tokenizes instruction pairs and provides them as padded tensors\n",
    "    with loss masks (labels set to -100 for instruction tokens).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, examples, tokenizer, max_length=512):\n",
    "        self.tokenized = []\n",
    "        for ex in examples:\n",
    "            self.tokenized.append(tokenize_example(ex, tokenizer, max_length))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.tokenized[idx]\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(item[\"input_ids\"], dtype=torch.long),\n",
    "            \"labels\": torch.tensor(item[\"labels\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(item[\"attention_mask\"], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pad a batch of variable-length sequences to the same length.\n",
    "\n",
    "    Pads input_ids with pad_token_id, labels with -100, and\n",
    "    attention_mask with 0.\n",
    "    \"\"\"\n",
    "    max_len = max(item[\"input_ids\"].size(0) for item in batch)\n",
    "\n",
    "    padded_input_ids = []\n",
    "    padded_labels = []\n",
    "    padded_attention_mask = []\n",
    "\n",
    "    for item in batch:\n",
    "        seq_len = item[\"input_ids\"].size(0)\n",
    "        pad_len = max_len - seq_len\n",
    "\n",
    "        padded_input_ids.append(\n",
    "            F.pad(item[\"input_ids\"], (0, pad_len), value=tokenizer.pad_token_id)\n",
    "        )\n",
    "        padded_labels.append(\n",
    "            F.pad(item[\"labels\"], (0, pad_len), value=-100)\n",
    "        )\n",
    "        padded_attention_mask.append(\n",
    "            F.pad(item[\"attention_mask\"], (0, pad_len), value=0)\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.stack(padded_input_ids),\n",
    "        \"labels\": torch.stack(padded_labels),\n",
    "        \"attention_mask\": torch.stack(padded_attention_mask),\n",
    "    }\n",
    "\n",
    "\n",
    "# Create dataset and dataloader\n",
    "sft_dataset = SFTDataset(raw_dataset, tokenizer, max_length=512)\n",
    "dataloader = DataLoader(sft_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Dataset size: {len(sft_dataset)} examples\")\n",
    "print(f\"Batch size: 2\")\n",
    "print(f\"Batches per epoch: {len(dataloader)}\")\n",
    "\n",
    "# Inspect one batch\n",
    "sample_batch = next(iter(dataloader))\n",
    "print(f\"\\nSample batch shapes:\")\n",
    "for key, val in sample_batch.items():\n",
    "    print(f\"  {key}: {val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the base model for manual training\n",
    "# (we keep the original base_model untouched for comparison later)\n",
    "manual_model = copy.deepcopy(base_model)\n",
    "manual_model.train()\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 5e-5\n",
    "num_epochs = 3\n",
    "optimizer = torch.optim.AdamW(manual_model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Optimizer: AdamW\")\n",
    "print(f\"  Batch size: 2\")\n",
    "print(f\"  Total steps: {num_epochs * len(dataloader)}\")\n",
    "print()\n",
    "print(\"Starting training...\")\n",
    "print(\"(This will take a few minutes on CPU.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    manual_model.train()\n",
    "\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        outputs = manual_model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"],  # labels with -100 masking\n",
    "        )\n",
    "\n",
    "        # The model computes cross-entropy loss internally, respecting -100 mask.\n",
    "        # This is equivalent to:\n",
    "        #   logits = outputs.logits  # (batch, seq_len, vocab_size)\n",
    "        #   shift_logits = logits[:, :-1, :].contiguous()\n",
    "        #   shift_labels = batch[\"labels\"][:, 1:].contiguous()\n",
    "        #   loss = F.cross_entropy(\n",
    "        #       shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        #       shift_labels.view(-1),\n",
    "        #       ignore_index=-100,\n",
    "        #   )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_val = loss.item()\n",
    "        epoch_losses.append(loss_val)\n",
    "        loss_history.append(loss_val)\n",
    "\n",
    "        if (step + 1) % 5 == 0 or step == 0:\n",
    "            print(\n",
    "                f\"  Epoch {epoch + 1}/{num_epochs}, \"\n",
    "                f\"Step {step + 1}/{len(dataloader)}, \"\n",
    "                f\"Loss: {loss_val:.4f}\"\n",
    "            )\n",
    "\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    print(f\"Epoch {epoch + 1} complete. Average loss: {avg_loss:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Training complete.\")\n",
    "print(f\"Final loss: {loss_history[-1]:.4f}\")\n",
    "print(f\"Total steps: {len(loss_history)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss curve\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Per-step loss\n",
    "ax = axes[0]\n",
    "ax.plot(loss_history, color=\"steelblue\", alpha=0.7, linewidth=1)\n",
    "ax.set_xlabel(\"Training step\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Training Loss (per step)\")\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Mark epoch boundaries\n",
    "steps_per_epoch = len(dataloader)\n",
    "for e in range(1, num_epochs):\n",
    "    ax.axvline(x=e * steps_per_epoch, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Smoothed loss (rolling average)\n",
    "ax = axes[1]\n",
    "window = max(3, len(loss_history) // 10)\n",
    "if len(loss_history) >= window:\n",
    "    smoothed = np.convolve(loss_history, np.ones(window) / window, mode=\"valid\")\n",
    "    ax.plot(range(window - 1, len(loss_history)), smoothed, color=\"steelblue\", linewidth=2)\n",
    "else:\n",
    "    ax.plot(loss_history, color=\"steelblue\", linewidth=2)\n",
    "ax.set_xlabel(\"Training step\")\n",
    "ax.set_ylabel(\"Loss (smoothed)\")\n",
    "ax.set_title(f\"Training Loss (rolling average, window={window})\")\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial loss: {loss_history[0]:.4f}\")\n",
    "print(f\"Final loss:   {loss_history[-1]:.4f}\")\n",
    "print(f\"Reduction:    {loss_history[0] - loss_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-heading",
   "metadata": {},
   "source": [
    "## After SFT: Results\n",
    "\n",
    "Let's run the same legal prompts through the fine-tuned model and compare\n",
    "with the base model. Even with a tiny dataset and a few epochs, the model\n",
    "should start showing some instruction-following behavior -- attempting to\n",
    "respond in the format it learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-sft",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BEFORE vs AFTER SFT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "manual_model.eval()\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    # Base model output (already computed above)\n",
    "    base_out = base_outputs[i] if i < len(base_outputs) else \"\"\n",
    "\n",
    "    # Fine-tuned model output\n",
    "    sft_out = generate_text(manual_model, tokenizer, prompt, max_new_tokens=60)\n",
    "    sft_generated = sft_out[len(prompt):]\n",
    "\n",
    "    instruction_line = prompt.split(\"\\n\")[3] if len(prompt.split(\"\\n\")) > 3 else \"N/A\"\n",
    "\n",
    "    print(f\"\\n--- Prompt {i + 1}: {instruction_line[:60]} ---\")\n",
    "    print(f\"  BASE:      {base_out[:150]}\")\n",
    "    print(f\"  AFTER SFT: {sft_generated[:150]}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"The fine-tuned model should show some tendency to follow the instruction\")\n",
    "print(\"format, even if the content is not perfect. With a 135M parameter model\")\n",
    "print(\"and only ~30 training examples, the results are modest but the behavioral\")\n",
    "print(\"shift is visible: the model attempts to respond rather than just continue.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trl-heading",
   "metadata": {},
   "source": [
    "## Using trl SFTTrainer\n",
    "\n",
    "The manual training loop above teaches you every component. In practice,\n",
    "you would use the `trl` library's `SFTTrainer`, which handles:\n",
    "\n",
    "- Tokenization and chat template formatting\n",
    "- Padding, batching, and loss masking\n",
    "- Learning rate scheduling\n",
    "- Logging and checkpointing\n",
    "- Gradient accumulation\n",
    "- Metrics during training\n",
    "- Distributed training support (multi-GPU)\n",
    "\n",
    "Below we show how to achieve the same result with much less code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trl-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "\n",
    "# Prepare data in the format trl expects:\n",
    "# a HuggingFace Dataset with a \"text\" column containing the formatted ChatML text.\n",
    "formatted_texts = []\n",
    "for ex in raw_dataset:\n",
    "    formatted_texts.append(\n",
    "        CHATML_TEMPLATE.format(\n",
    "            instruction=ex[\"instruction\"],\n",
    "            input=ex[\"input\"],\n",
    "            output=ex[\"output\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "trl_dataset = HFDataset.from_dict({\"text\": formatted_texts})\n",
    "\n",
    "print(f\"trl dataset: {trl_dataset}\")\n",
    "print(f\"Sample text (first 200 chars): {trl_dataset[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trl-train-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone a fresh base model for trl training\n",
    "trl_model = copy.deepcopy(base_model)\n",
    "\n",
    "# Configure SFTTrainer\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./sft_trl_output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"no\",\n",
    "    max_seq_length=512,\n",
    "    report_to=\"none\",  # disable wandb / tensorboard\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=trl_model,\n",
    "    args=training_args,\n",
    "    train_dataset=trl_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Starting trl SFTTrainer...\")\n",
    "print(\"(Same hyperparameters as the manual loop.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trl-train-execute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "trl_result = trainer.train()\n",
    "\n",
    "print(f\"\\ntrl training complete.\")\n",
    "print(f\"Training loss: {trl_result.training_loss:.4f}\")\n",
    "print(f\"Total steps: {trl_result.global_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-heading",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "Let's compare outputs from the manual training loop and the trl trainer.\n",
    "They should produce similar results since they use the same model,\n",
    "data, and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"COMPARISON: Manual Loop vs trl SFTTrainer\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "manual_model.eval()\n",
    "trl_model.eval()\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    manual_out = generate_text(manual_model, tokenizer, prompt, max_new_tokens=60)\n",
    "    manual_generated = manual_out[len(prompt):]\n",
    "\n",
    "    trl_out = generate_text(trl_model, tokenizer, prompt, max_new_tokens=60)\n",
    "    trl_generated = trl_out[len(prompt):]\n",
    "\n",
    "    instruction_line = prompt.split(\"\\n\")[3] if len(prompt.split(\"\\n\")) > 3 else \"N/A\"\n",
    "\n",
    "    print(f\"\\n--- Prompt {i + 1}: {instruction_line[:60]} ---\")\n",
    "    print(f\"  MANUAL: {manual_generated[:150]}\")\n",
    "    print(f\"  TRL:    {trl_generated[:150]}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nWhy use trl in practice?\")\n",
    "print(\"  - Handles edge cases in tokenization and padding.\")\n",
    "print(\"  - Built-in learning rate scheduling and warmup.\")\n",
    "print(\"  - Logging integration (wandb, tensorboard).\")\n",
    "print(\"  - Gradient accumulation for effective larger batch sizes.\")\n",
    "print(\"  - Metrics tracked during training.\")\n",
    "print(\"  - Checkpoint saving and resumption.\")\n",
    "print(\"  - Distributed training support (multi-GPU).\")\n",
    "print(\"\\nThe manual loop is for understanding. trl is for production.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise (a): Experiment with Hyperparameters\n",
    "\n",
    "Try different learning rates and batch sizes. For each configuration,\n",
    "train for 3 epochs and record the final loss and sample outputs.\n",
    "\n",
    "Suggested configurations:\n",
    "\n",
    "| Learning Rate | Batch Size | Expected Behavior |\n",
    "|--------------|-----------|------------------|\n",
    "| 1e-5 | 2 | Slow convergence, stable |\n",
    "| 5e-5 | 2 | Moderate convergence |\n",
    "| 1e-4 | 2 | Fast convergence, possible instability |\n",
    "| 5e-5 | 1 | Noisier gradients, more updates per epoch |\n",
    "| 5e-5 | 4 | Smoother gradients, fewer updates per epoch |\n",
    "\n",
    "```python\n",
    "# Starter code\n",
    "configs = [\n",
    "    {\"lr\": 1e-5, \"batch_size\": 2},\n",
    "    {\"lr\": 5e-5, \"batch_size\": 2},\n",
    "    {\"lr\": 1e-4, \"batch_size\": 2},\n",
    "]\n",
    "\n",
    "for cfg in configs:\n",
    "    model_copy = copy.deepcopy(base_model)\n",
    "    opt = torch.optim.AdamW(model_copy.parameters(), lr=cfg[\"lr\"])\n",
    "    loader = DataLoader(\n",
    "        sft_dataset, batch_size=cfg[\"batch_size\"],\n",
    "        shuffle=True, collate_fn=collate_fn,\n",
    "    )\n",
    "    # Train for 3 epochs, record losses...\n",
    "```\n",
    "\n",
    "Questions:\n",
    "- Which learning rate converges fastest?\n",
    "- Does a higher learning rate always mean better results?\n",
    "- How does batch size affect loss smoothness vs training speed?\n",
    "\n",
    "### Exercise (b): Observe Overfitting\n",
    "\n",
    "Train the model for 20+ epochs on this small dataset and observe what\n",
    "happens.\n",
    "\n",
    "```python\n",
    "overfit_model = copy.deepcopy(base_model)\n",
    "overfit_model.train()\n",
    "opt = torch.optim.AdamW(overfit_model.parameters(), lr=5e-5)\n",
    "\n",
    "overfit_losses = []\n",
    "for epoch in range(25):\n",
    "    for batch in dataloader:\n",
    "        outputs = overfit_model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"labels\"],\n",
    "        )\n",
    "        outputs.loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        overfit_losses.append(outputs.loss.item())\n",
    "\n",
    "# Plot the loss -- it should approach 0\n",
    "plt.plot(overfit_losses)\n",
    "plt.title(\"Overfitting: Loss approaches 0\")\n",
    "plt.show()\n",
    "\n",
    "# Test the overfit model -- outputs may be memorized/repetitive\n",
    "overfit_model.eval()\n",
    "for prompt in test_prompts:\n",
    "    output = generate_text(overfit_model, tokenizer, prompt)\n",
    "    print(output[len(prompt):][:200])\n",
    "```\n",
    "\n",
    "You should observe:\n",
    "- Loss drops close to 0 (the model has memorized the training data).\n",
    "- Outputs become repetitive or exactly reproduce training examples.\n",
    "- On prompts not seen during training, the model may produce garbled text.\n",
    "- This demonstrates why SFT needs diverse, large datasets in practice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
