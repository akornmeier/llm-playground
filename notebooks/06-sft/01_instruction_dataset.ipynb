{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "context",
   "metadata": {},
   "source": [
    "# 01 - Building an Instruction Dataset for SFT\n",
    "\n",
    "## Context\n",
    "\n",
    "Supervised fine-tuning (SFT) is how base language models learn to follow\n",
    "instructions. A base model -- one trained only on next-token prediction --\n",
    "has no concept of \"question\" and \"answer.\" It simply continues whatever\n",
    "text you give it. If you prompt it with a legal question, it might continue\n",
    "with more questions, a textbook-style paragraph, or something entirely\n",
    "off-topic. It has knowledge embedded in its weights, but no mechanism to\n",
    "retrieve that knowledge on command.\n",
    "\n",
    "SFT solves this by training the model on instruction-response pairs. After\n",
    "enough examples of \"here is an instruction\" followed by \"here is a good\n",
    "response,\" the model learns the pattern: given an instruction, produce a\n",
    "response in the same style.\n",
    "\n",
    "**CoCounsel context:** The difference between a model that rambles about law\n",
    "and one that answers legal questions precisely is SFT. A base model might\n",
    "continue a court opinion passage with plausible-sounding but directionless\n",
    "text. After SFT, the same model can summarize an opinion, extract holdings,\n",
    "or list citations -- because it has learned the instruction-response pattern.\n",
    "\n",
    "In this notebook, we build an instruction-tuning dataset from court opinions,\n",
    "format it using chat templates, and understand loss masking -- which tokens\n",
    "the model actually learns to generate during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instruction-tuning-format",
   "metadata": {},
   "source": [
    "## Instruction Tuning Format\n",
    "\n",
    "Modern instruction-tuned models use a **chat template** that delineates\n",
    "roles: system, user, and assistant.\n",
    "\n",
    "```\n",
    "<|system|>\n",
    "You are a legal research assistant.\n",
    "<|user|>\n",
    "Summarize this court opinion: [text]\n",
    "<|assistant|>\n",
    "The court held that...\n",
    "```\n",
    "\n",
    "The exact tokens vary by model family:\n",
    "\n",
    "| Format | System token | User token | Assistant token |\n",
    "|--------|-------------|------------|----------------|\n",
    "| ChatML | `<\\|im_start\\|>system` | `<\\|im_start\\|>user` | `<\\|im_start\\|>assistant` |\n",
    "| Llama  | `<\\|start_header_id\\|>system<\\|end_header_id\\|>` | `<\\|start_header_id\\|>user<\\|end_header_id\\|>` | `<\\|start_header_id\\|>assistant<\\|end_header_id\\|>` |\n",
    "\n",
    "**Why formatting matters:** The model learns the *pattern* of the template.\n",
    "It learns that text after the assistant header is what it should generate,\n",
    "and text after the user header is the instruction it should follow. If the\n",
    "formatting is inconsistent, the model cannot learn this pattern reliably.\n",
    "\n",
    "In this notebook we use the **ChatML** format because it is simple and\n",
    "widely supported. The key structure is:\n",
    "\n",
    "```\n",
    "<|im_start|>system\n",
    "You are a legal research assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "{instruction}\n",
    "{input text}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{response}<|im_end|>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-heading",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "building-heading",
   "metadata": {},
   "source": [
    "## Building the Dataset\n",
    "\n",
    "We load five court opinions from our sample data and transform each one into\n",
    "multiple instruction-response pairs. For each opinion we generate several\n",
    "types of questions:\n",
    "\n",
    "- **Summarize** -- a brief summary of the case\n",
    "- **Holding** -- what the court decided\n",
    "- **Citations** -- key legal citations referenced\n",
    "- **Court** -- which court issued the opinion\n",
    "- **Key issues** -- the central legal questions\n",
    "\n",
    "This gives us 5 questions per opinion for a total of 25 training examples,\n",
    "plus a few additional variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-opinions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load court opinions\n",
    "data_path = Path(\"../../datasets/sample/court_opinions.jsonl\")\n",
    "opinions = []\n",
    "with open(data_path) as f:\n",
    "    for line in f:\n",
    "        opinions.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(opinions)} court opinions\")\n",
    "for op in opinions:\n",
    "    print(f\"  - {op['case_name']} ({op['court']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_instruction_pairs(opinions):\n",
    "    \"\"\"Transform court opinions into instruction-response pairs.\n",
    "\n",
    "    Each opinion generates multiple training examples with different\n",
    "    instruction types: summarize, holding, citations, court, and issues.\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with 'instruction', 'input', and 'output' fields.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "\n",
    "    for op in opinions:\n",
    "        text = op[\"text\"]\n",
    "        case_name = op[\"case_name\"]\n",
    "        court = op[\"court\"]\n",
    "        citations = op[\"citations\"]\n",
    "\n",
    "        # --- 1. Summarize ---\n",
    "        # Extract the first and last sentences as a rough summary\n",
    "        sentences = [s.strip() for s in text.split(\".\") if s.strip()]\n",
    "        summary = \". \".join(sentences[:2]) + \".\" if len(sentences) >= 2 else text[:300]\n",
    "        dataset.append({\n",
    "            \"instruction\": \"Summarize this court opinion.\",\n",
    "            \"input\": text,\n",
    "            \"output\": summary,\n",
    "        })\n",
    "\n",
    "        # --- 2. Holding ---\n",
    "        # Find the disposition (typically the last paragraph with action verbs)\n",
    "        paragraphs = text.split(\"\\n\\n\")\n",
    "        holding = paragraphs[-1].strip() if paragraphs else text[-300:]\n",
    "        # Look for holding indicators in the text\n",
    "        for keyword in [\"REVERSE\", \"AFFIRM\", \"REMAND\", \"GRANTED\", \"DENIED\"]:\n",
    "            for sent in sentences:\n",
    "                if keyword in sent:\n",
    "                    holding = sent.strip() + \".\"\n",
    "                    break\n",
    "        dataset.append({\n",
    "            \"instruction\": \"What was the holding in this case?\",\n",
    "            \"input\": text,\n",
    "            \"output\": holding,\n",
    "        })\n",
    "\n",
    "        # --- 3. Citations ---\n",
    "        citation_list = \"\\n\".join(f\"- {c}\" for c in citations)\n",
    "        dataset.append({\n",
    "            \"instruction\": \"List the key legal citations in this opinion.\",\n",
    "            \"input\": text,\n",
    "            \"output\": citation_list,\n",
    "        })\n",
    "\n",
    "        # --- 4. Court ---\n",
    "        dataset.append({\n",
    "            \"instruction\": \"What court issued this opinion?\",\n",
    "            \"input\": text,\n",
    "            \"output\": court,\n",
    "        })\n",
    "\n",
    "        # --- 5. Key issues ---\n",
    "        # Extract the core legal question from the first paragraph\n",
    "        first_para = paragraphs[0] if paragraphs else text[:500]\n",
    "        issue_summary = (\n",
    "            f\"In {case_name}, the key legal issue is: {sentences[0]}.\"\n",
    "            if sentences\n",
    "            else f\"The key issue in {case_name} concerns the matters described in the opinion.\"\n",
    "        )\n",
    "        dataset.append({\n",
    "            \"instruction\": \"What are the key legal issues in this case?\",\n",
    "            \"input\": text,\n",
    "            \"output\": issue_summary,\n",
    "        })\n",
    "\n",
    "        # --- 6. Additional: case name extraction ---\n",
    "        dataset.append({\n",
    "            \"instruction\": \"What is the name of this case?\",\n",
    "            \"input\": text,\n",
    "            \"output\": case_name,\n",
    "        })\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = build_instruction_pairs(opinions)\n",
    "print(f\"Total instruction pairs: {len(dataset)}\")\n",
    "print(f\"\\nInstruction types:\")\n",
    "for i, example in enumerate(dataset):\n",
    "    print(f\"  [{i:>2}] {example['instruction'][:60]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a few examples in detail\n",
    "for idx in [0, 2, 3]:\n",
    "    ex = dataset[idx]\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"  Instruction: {ex['instruction']}\")\n",
    "    print(f\"  Input:       {ex['input'][:120]}...\")\n",
    "    print(f\"  Output:      {ex['output'][:200]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chat-template-heading",
   "metadata": {},
   "source": [
    "## Chat Template Formatting\n",
    "\n",
    "Now we format each instruction pair using the ChatML template. This is the\n",
    "actual text the model will see during training. The model learns to generate\n",
    "only the **assistant** portion -- the instruction and system message are\n",
    "context, not targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "format-chatml",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHATML_TEMPLATE = (\n",
    "    \"<|im_start|>system\\n\"\n",
    "    \"You are a legal research assistant. Answer the question about the \"\n",
    "    \"provided court opinion accurately and concisely.<|im_end|>\\n\"\n",
    "    \"<|im_start|>user\\n\"\n",
    "    \"{instruction}\\n\\n\"\n",
    "    \"{input}<|im_end|>\\n\"\n",
    "    \"<|im_start|>assistant\\n\"\n",
    "    \"{output}<|im_end|>\"\n",
    ")\n",
    "\n",
    "\n",
    "def format_chatml(example):\n",
    "    \"\"\"Format an instruction pair as a ChatML conversation.\n",
    "\n",
    "    Args:\n",
    "        example: Dict with 'instruction', 'input', 'output' fields.\n",
    "\n",
    "    Returns:\n",
    "        Formatted string in ChatML format.\n",
    "    \"\"\"\n",
    "    return CHATML_TEMPLATE.format(\n",
    "        instruction=example[\"instruction\"],\n",
    "        input=example[\"input\"],\n",
    "        output=example[\"output\"],\n",
    "    )\n",
    "\n",
    "\n",
    "# Format all examples\n",
    "formatted_examples = [format_chatml(ex) for ex in dataset]\n",
    "\n",
    "# Show one formatted example (using a short-output example for readability)\n",
    "court_example_idx = 3  # \"What court issued this opinion?\" -- short output\n",
    "print(\"Formatted ChatML example:\")\n",
    "print(\"=\" * 70)\n",
    "# Truncate the input portion for display\n",
    "ex = dataset[court_example_idx]\n",
    "display_text = format_chatml({\n",
    "    \"instruction\": ex[\"instruction\"],\n",
    "    \"input\": ex[\"input\"][:200] + \"...[truncated]\",\n",
    "    \"output\": ex[\"output\"],\n",
    "})\n",
    "print(display_text)\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Key observations:\")\n",
    "print(\"- The system message sets the role.\")\n",
    "print(\"- The user message contains both the instruction and the input text.\")\n",
    "print(\"- The assistant message contains the target response.\")\n",
    "print(\"- <|im_start|> and <|im_end|> are special tokens that mark role boundaries.\")\n",
    "print(\"- During training, the model learns to generate ONLY the assistant portion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "show-raw-formatted",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the raw text with role boundaries highlighted\n",
    "example_text = formatted_examples[court_example_idx]\n",
    "\n",
    "# Find the assistant response portion\n",
    "assistant_marker = \"<|im_start|>assistant\\n\"\n",
    "assistant_start = example_text.find(assistant_marker)\n",
    "assistant_content_start = assistant_start + len(assistant_marker)\n",
    "assistant_end = example_text.find(\"<|im_end|>\", assistant_content_start)\n",
    "\n",
    "instruction_part = example_text[:assistant_start]\n",
    "response_part = example_text[assistant_content_start:assistant_end]\n",
    "\n",
    "print(\"Parts of the formatted text:\")\n",
    "print()\n",
    "print(\"--- CONTEXT (model sees but does NOT learn to generate) ---\")\n",
    "print(f\"Length: {len(instruction_part)} characters\")\n",
    "print()\n",
    "print(\"--- RESPONSE (model LEARNS to generate this) ---\")\n",
    "print(f\"{response_part!r}\")\n",
    "print(f\"Length: {len(response_part)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loss-masking-heading",
   "metadata": {},
   "source": [
    "## Loss Masking\n",
    "\n",
    "During SFT, we do **not** want the model to learn to generate the\n",
    "instruction -- only the response. If we computed loss on all tokens,\n",
    "the model would spend training capacity learning to reproduce instructions,\n",
    "which is wasteful and can hurt response quality.\n",
    "\n",
    "The solution is a **loss mask**: an array of 0s and 1s aligned with the\n",
    "tokenized sequence. A value of 0 means \"ignore this token in the loss\n",
    "calculation\" and 1 means \"include this token.\"\n",
    "\n",
    "The mask is set to:\n",
    "- **0** for all system/user tokens (the instruction context)\n",
    "- **1** for all assistant tokens (the response the model should learn)\n",
    "\n",
    "In practice, this is implemented by setting the label to `-100` for\n",
    "masked positions, which PyTorch's `CrossEntropyLoss` ignores by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loss-mask-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loss_mask(tokenized_input, response_start_idx):\n",
    "    \"\"\"Create a loss mask that only trains on response tokens.\n",
    "\n",
    "    Args:\n",
    "        tokenized_input: List of token IDs (the full sequence).\n",
    "        response_start_idx: Index where the response tokens begin.\n",
    "\n",
    "    Returns:\n",
    "        List of 0s and 1s. 0 = masked (ignored in loss), 1 = trained.\n",
    "    \"\"\"\n",
    "    mask = [0] * response_start_idx + [1] * (len(tokenized_input) - response_start_idx)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def find_response_start(text, tokenizer):\n",
    "    \"\"\"Find the token index where the assistant response begins.\n",
    "\n",
    "    Looks for the '<|im_start|>assistant\\n' marker in the ChatML text\n",
    "    and returns the token index of the first response token.\n",
    "\n",
    "    Args:\n",
    "        text: The full ChatML-formatted string.\n",
    "        tokenizer: A HuggingFace tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        Token index where the assistant response content starts.\n",
    "    \"\"\"\n",
    "    marker = \"<|im_start|>assistant\\n\"\n",
    "    marker_pos = text.find(marker)\n",
    "    if marker_pos == -1:\n",
    "        raise ValueError(\"Could not find assistant marker in text\")\n",
    "\n",
    "    # Tokenize everything up to and including the assistant marker\n",
    "    prefix = text[: marker_pos + len(marker)]\n",
    "    prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "    return len(prefix_tokens)\n",
    "\n",
    "\n",
    "# Load a tokenizer to demonstrate\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Demonstrate on the court example\n",
    "example_text = formatted_examples[court_example_idx]\n",
    "tokens = tokenizer.encode(example_text, add_special_tokens=False)\n",
    "response_start = find_response_start(example_text, tokenizer)\n",
    "mask = create_loss_mask(tokens, response_start)\n",
    "\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"Response starts at token index: {response_start}\")\n",
    "print(f\"Masked tokens (instruction): {sum(1 for m in mask if m == 0)}\")\n",
    "print(f\"Trained tokens (response):   {sum(1 for m in mask if m == 1)}\")\n",
    "print(f\"Training ratio: {sum(mask) / len(mask):.1%} of tokens are trained on\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-mask",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize which tokens are masked vs trained\n",
    "# Use a short example for readability\n",
    "short_example = {\n",
    "    \"instruction\": \"What court issued this opinion?\",\n",
    "    \"input\": \"Before the Court is the appeal of plaintiff James Henderson...\",\n",
    "    \"output\": \"United States Court of Appeals for the Seventh Circuit\",\n",
    "}\n",
    "short_text = format_chatml(short_example)\n",
    "short_tokens = tokenizer.encode(short_text, add_special_tokens=False)\n",
    "short_response_start = find_response_start(short_text, tokenizer)\n",
    "short_mask = create_loss_mask(short_tokens, short_response_start)\n",
    "\n",
    "# Decode each token individually for display\n",
    "decoded_tokens = [tokenizer.decode([t]) for t in short_tokens]\n",
    "\n",
    "print(\"Token-level loss mask visualization\")\n",
    "print(\"=\" * 70)\n",
    "print(\"RED = masked (not trained on) | GREEN = trained on\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "for i, (token_str, m) in enumerate(zip(decoded_tokens, short_mask)):\n",
    "    color = \"\\033[92m\" if m == 1 else \"\\033[91m\"  # green or red\n",
    "    reset = \"\\033[0m\"\n",
    "    label = \"TRAIN\" if m == 1 else \"MASK \"\n",
    "    print(f\"  [{i:>3}] {color}{label}{reset}  {token_str!r}\")\n",
    "\n",
    "print()\n",
    "print(f\"Total: {len(short_tokens)} tokens\")\n",
    "print(f\"Masked: {short_mask.count(0)} tokens (instruction + template)\")\n",
    "print(f\"Trained: {short_mask.count(1)} tokens (assistant response)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-mask-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphical visualization of the mask\n",
    "fig, ax = plt.subplots(figsize=(14, 2))\n",
    "\n",
    "colors = [\"#e74c3c\" if m == 0 else \"#2ecc71\" for m in short_mask]\n",
    "ax.bar(range(len(short_mask)), [1] * len(short_mask), color=colors, width=1.0, edgecolor=\"white\", linewidth=0.3)\n",
    "\n",
    "ax.set_xlim(-0.5, len(short_mask) - 0.5)\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel(\"Token position\")\n",
    "ax.set_title(\"Loss Mask: Red = masked (instruction), Green = trained (response)\")\n",
    "\n",
    "# Mark the boundary\n",
    "ax.axvline(x=short_response_start - 0.5, color=\"black\", linewidth=2, linestyle=\"--\", label=\"Response start\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"The vertical line marks where the assistant response begins (token {short_response_start}).\")\n",
    "print(\"Everything to the left (red) is instruction context -- the model sees it but\")\n",
    "print(\"does not compute loss on it. Everything to the right (green) is what the model\")\n",
    "print(\"is trained to generate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistics-heading",
   "metadata": {},
   "source": [
    "## Dataset Statistics\n",
    "\n",
    "Before training, we analyze the dataset to understand its structure:\n",
    "how long are responses, what types of instructions are most common,\n",
    "and how many tokens each example requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics for the full dataset\n",
    "stats = []\n",
    "for i, (ex, text) in enumerate(zip(dataset, formatted_examples)):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    response_start = find_response_start(text, tokenizer)\n",
    "    response_tokens = tokens[response_start:]\n",
    "\n",
    "    stats.append({\n",
    "        \"index\": i,\n",
    "        \"instruction\": ex[\"instruction\"],\n",
    "        \"total_tokens\": len(tokens),\n",
    "        \"instruction_tokens\": response_start,\n",
    "        \"response_tokens\": len(response_tokens),\n",
    "        \"response_chars\": len(ex[\"output\"]),\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "total_tokens_list = [s[\"total_tokens\"] for s in stats]\n",
    "response_tokens_list = [s[\"response_tokens\"] for s in stats]\n",
    "instruction_tokens_list = [s[\"instruction_tokens\"] for s in stats]\n",
    "\n",
    "print(\"Dataset Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Number of examples:     {len(stats)}\")\n",
    "print(f\"Total tokens (all):     {sum(total_tokens_list):,}\")\n",
    "print()\n",
    "print(f\"Tokens per example:\")\n",
    "print(f\"  Mean:   {np.mean(total_tokens_list):.0f}\")\n",
    "print(f\"  Median: {np.median(total_tokens_list):.0f}\")\n",
    "print(f\"  Min:    {min(total_tokens_list)}\")\n",
    "print(f\"  Max:    {max(total_tokens_list)}\")\n",
    "print()\n",
    "print(f\"Response tokens:\")\n",
    "print(f\"  Mean:   {np.mean(response_tokens_list):.0f}\")\n",
    "print(f\"  Median: {np.median(response_tokens_list):.0f}\")\n",
    "print(f\"  Min:    {min(response_tokens_list)}\")\n",
    "print(f\"  Max:    {max(response_tokens_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instruction-type-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count by instruction type\n",
    "from collections import Counter\n",
    "\n",
    "instruction_counts = Counter(s[\"instruction\"] for s in stats)\n",
    "print(\"Instruction type distribution:\")\n",
    "for instr, count in instruction_counts.most_common():\n",
    "    avg_resp = np.mean(\n",
    "        [s[\"response_tokens\"] for s in stats if s[\"instruction\"] == instr]\n",
    "    )\n",
    "    print(f\"  {instr:<50} count={count}  avg_response_tokens={avg_resp:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-histograms",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Response token distribution\n",
    "ax = axes[0]\n",
    "ax.hist(response_tokens_list, bins=15, color=\"steelblue\", edgecolor=\"white\")\n",
    "ax.set_xlabel(\"Response length (tokens)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Distribution of Response Lengths\")\n",
    "ax.axvline(np.mean(response_tokens_list), color=\"red\", linestyle=\"--\", label=f\"Mean: {np.mean(response_tokens_list):.0f}\")\n",
    "ax.legend()\n",
    "\n",
    "# Total token distribution\n",
    "ax = axes[1]\n",
    "ax.hist(total_tokens_list, bins=15, color=\"#2ecc71\", edgecolor=\"white\")\n",
    "ax.set_xlabel(\"Total length (tokens)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Distribution of Total Example Lengths\")\n",
    "ax.axvline(np.mean(total_tokens_list), color=\"red\", linestyle=\"--\", label=f\"Mean: {np.mean(total_tokens_list):.0f}\")\n",
    "ax.legend()\n",
    "\n",
    "# Instruction vs response token split\n",
    "ax = axes[2]\n",
    "instruction_types = sorted(set(s[\"instruction\"] for s in stats))\n",
    "type_labels = [t.replace(\"this opinion\", \"...\").replace(\"this case\", \"...\")[:30] for t in instruction_types]\n",
    "type_response_means = [\n",
    "    np.mean([s[\"response_tokens\"] for s in stats if s[\"instruction\"] == t])\n",
    "    for t in instruction_types\n",
    "]\n",
    "ax.barh(range(len(type_labels)), type_response_means, color=\"#e74c3c\", edgecolor=\"white\")\n",
    "ax.set_yticks(range(len(type_labels)))\n",
    "ax.set_yticklabels(type_labels, fontsize=8)\n",
    "ax.set_xlabel(\"Mean response tokens\")\n",
    "ax.set_title(\"Response Length by Instruction Type\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"- Summarization tasks produce the longest responses (full summaries).\")\n",
    "print(\"- Court identification produces the shortest responses (just a name).\")\n",
    "print(\"- This imbalance is typical in instruction datasets and affects training.\")\n",
    "print(\"- Very short responses can be learned quickly; longer ones need more epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset for use in the training notebook\n",
    "output_path = Path(\"sft_dataset.json\")\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(dataset, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(dataset)} examples to {output_path}\")\n",
    "print(f\"This dataset will be loaded in notebook 02 for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise (a): Create Instruction Pairs for a New Task\n",
    "\n",
    "Create instruction pairs for a different task type. For example:\n",
    "\n",
    "- \"Is this opinion from a federal or state court?\"\n",
    "- \"What statute or regulation is at the center of this dispute?\"\n",
    "- \"Identify the standard of review used by the court.\"\n",
    "\n",
    "Add at least 5 new instruction pairs (one per opinion) to the dataset.\n",
    "Consider: how do you determine the correct output? For some tasks\n",
    "(like federal vs state), the answer can be derived from the court name.\n",
    "For others (like standard of review), you need to read the opinion text.\n",
    "\n",
    "```python\n",
    "# Example: federal vs state classification\n",
    "for op in opinions:\n",
    "    is_federal = \"United States\" in op[\"court\"]\n",
    "    dataset.append({\n",
    "        \"instruction\": \"Is this opinion from a federal or state court?\",\n",
    "        \"input\": op[\"text\"],\n",
    "        \"output\": \"Federal court\" if is_federal else \"State court\",\n",
    "    })\n",
    "```\n",
    "\n",
    "### Exercise (b): Analyze Response Length Implications\n",
    "\n",
    "The distribution of response lengths has practical implications for training:\n",
    "\n",
    "1. Compute the ratio of instruction tokens to response tokens for each example.\n",
    "   What fraction of compute is \"wasted\" on the masked instruction portion?\n",
    "2. Very short responses (like court names) are learned in fewer gradient steps\n",
    "   than long summaries. What problems might this cause?\n",
    "3. If you wanted all instruction types to converge at roughly the same rate,\n",
    "   how might you adjust the dataset? (Hint: consider oversampling or\n",
    "   weighting.)\n",
    "\n",
    "```python\n",
    "# Starter code\n",
    "for s in stats:\n",
    "    ratio = s[\"response_tokens\"] / s[\"total_tokens\"]\n",
    "    print(f\"{s['instruction'][:40]:<42} response_ratio={ratio:.2%}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
