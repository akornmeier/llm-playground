{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 01 - Byte Pair Encoding from Scratch\n",
    "\n",
    "## Why Tokenization Is the Hidden Foundation of LLMs\n",
    "\n",
    "Every large language model -- GPT, LLaMA, Claude -- operates on **tokens**, not\n",
    "characters or words. Before a single neuron fires, a tokenizer converts raw text\n",
    "into a sequence of integers drawn from a fixed vocabulary. The model never sees\n",
    "the original string; it sees only these integer IDs.\n",
    "\n",
    "This means the tokenizer quietly shapes everything downstream:\n",
    "\n",
    "- **Context window** -- a 4,096-token limit is not 4,096 words. Depending on\n",
    "  the tokenizer, it might be 3,000 words of English prose or only 2,500 words\n",
    "  of dense legal text.\n",
    "- **Model comprehension** -- if a legal citation like `42 U.S.C. ยง 1983` is\n",
    "  split into 8 tokens, the model must learn to reassemble those fragments into\n",
    "  a meaningful concept. A single-token representation would be far easier to\n",
    "  learn from.\n",
    "- **API costs** -- commercial APIs charge per token. Legal text that tokenizes\n",
    "  20% less efficiently costs 20% more to process.\n",
    "\n",
    "In this notebook, we build the most widely used tokenization algorithm --\n",
    "**Byte Pair Encoding (BPE)** -- from scratch in about 50 lines of Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Theory: Character vs Word vs Subword Tokenization\n",
    "\n",
    "There are three broad families of tokenization. Each makes a different tradeoff\n",
    "between vocabulary size and sequence length.\n",
    "\n",
    "### Character-Level\n",
    "\n",
    "Split text into individual characters. The vocabulary is tiny (a few hundred\n",
    "entries for English + punctuation + digits), but sequences become very long.\n",
    "The word \"jurisprudence\" becomes 13 tokens. A 10-page court opinion might\n",
    "exceed 50,000 tokens -- far beyond most models' context windows.\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| Tiny vocabulary | Very long sequences |\n",
    "| Handles any text | Model must learn spelling from scratch |\n",
    "| No out-of-vocabulary tokens | Poor compression ratio |\n",
    "\n",
    "### Word-Level\n",
    "\n",
    "Split on whitespace and punctuation. Common words become single tokens, but\n",
    "the vocabulary must be enormous to cover all observed words. Rare words\n",
    "(\"certiorari\", \"PFAS\", \"QD-5000\") are either unknown or require a special\n",
    "`[UNK]` token that destroys information.\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| Intuitive token boundaries | Huge vocabulary (100K+) |\n",
    "| Short sequences | Cannot handle unseen words |\n",
    "| Good compression | Morphology is ignored |\n",
    "\n",
    "### Subword (BPE)\n",
    "\n",
    "The sweet spot. Start with characters, then iteratively merge the most frequent\n",
    "adjacent pairs. Common words like \"the\" and \"court\" stay whole. Rare words\n",
    "like \"certiorari\" are split into learned subword pieces (e.g., `cert` + `ior` +\n",
    "`ari`). The vocabulary size is a tunable hyperparameter, typically 32K-100K.\n",
    "\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| Balanced vocabulary size | Tokenization is not linguistically motivated |\n",
    "| Handles unseen words gracefully | Domain-specific text may tokenize poorly |\n",
    "| Good compression ratio | Merges are corpus-dependent |\n",
    "\n",
    "**BPE is the algorithm behind GPT-2, GPT-3, GPT-4, LLaMA, and most modern\n",
    "LLMs.** Let's build it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## BPE Algorithm Explained\n",
    "\n",
    "The BPE training procedure is surprisingly simple:\n",
    "\n",
    "1. **Initialize** -- Start with a vocabulary of individual characters (or bytes).\n",
    "   Represent each word in the corpus as a sequence of characters separated by\n",
    "   spaces, with a special end-of-word marker `</w>`.\n",
    "\n",
    "2. **Count pairs** -- For every adjacent pair of symbols in the vocabulary,\n",
    "   count how many times it appears across the entire corpus.\n",
    "\n",
    "3. **Merge the top pair** -- Find the most frequent pair and merge it into a\n",
    "   single new symbol. Add this new symbol to the vocabulary.\n",
    "\n",
    "4. **Repeat** -- Go back to step 2. Each iteration adds one merge rule and\n",
    "   one new vocabulary entry.\n",
    "\n",
    "### Visual Example\n",
    "\n",
    "Corpus: `the court held that the court found`\n",
    "\n",
    "```\n",
    "Initial vocabulary:\n",
    "  t h e </w> c o u r d l a f n\n",
    "\n",
    "Initial word representations:\n",
    "  \"the\"   -> t h e </w>        (freq: 2)\n",
    "  \"court\" -> c o u r t </w>    (freq: 2)\n",
    "  \"held\"  -> h e l d </w>      (freq: 1)\n",
    "  \"that\"  -> t h a t </w>      (freq: 1)\n",
    "  \"found\" -> f o u n d </w>    (freq: 1)\n",
    "\n",
    "Step 1: Most frequent pair is (t, h) with count 3\n",
    "  Merge: t h -> th\n",
    "  \"the\"   -> th e </w>\n",
    "  \"court\" -> c o u r t </w>    (no change -- t not adjacent to h)\n",
    "  \"held\"  -> h e l d </w>      (no change -- h not adjacent to t here)\n",
    "  \"that\"  -> th a t </w>\n",
    "\n",
    "Step 2: Most frequent pair is (th, e) with count 2\n",
    "  Merge: th e -> the\n",
    "  \"the\"   -> the </w>\n",
    "  \"that\"  -> th a t </w>       (no change -- th is followed by a, not e)\n",
    "\n",
    "Step 3: Most frequent pair is (the, </w>) with count 2\n",
    "  Merge: the </w> -> the</w>\n",
    "  \"the\"   -> the</w>           (single token!)\n",
    "```\n",
    "\n",
    "After just 3 merges, \"the\" is a single token. The algorithm naturally discovers\n",
    "that common words should be atomic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Load Legal Text Corpus\n",
    "\n",
    "We use court opinions from our sample dataset as the training corpus for BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../../datasets/sample/court_opinions.jsonl\")\n",
    "\n",
    "corpus_texts = []\n",
    "with open(DATA_PATH) as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line)\n",
    "        corpus_texts.append(record[\"text\"])\n",
    "\n",
    "# Combine all opinions into one training corpus\n",
    "training_corpus = \" \".join(corpus_texts)\n",
    "print(f\"Loaded {len(corpus_texts)} court opinions\")\n",
    "print(f\"Total corpus size: {len(training_corpus):,} characters\")\n",
    "print(f\"\\nFirst 300 characters:\")\n",
    "print(training_corpus[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## BPE from Scratch\n",
    "\n",
    "Here is the complete implementation in five functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(text: str) -> dict[tuple[str, ...], int]:\n",
    "    \"\"\"Convert raw text into a word-frequency vocabulary.\n",
    "\n",
    "    Each word is represented as a tuple of characters with a special\n",
    "    end-of-word marker '</w>'. The dictionary maps each word-tuple to\n",
    "    its frequency in the corpus.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    vocab: dict[tuple[str, ...], int] = Counter()\n",
    "    for word in words:\n",
    "        # Represent each word as character tuple + end-of-word marker\n",
    "        symbols = tuple(word) + (\"</w>\",)\n",
    "        vocab[symbols] += 1\n",
    "    return dict(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(vocab: dict[tuple[str, ...], int]) -> dict[tuple[str, str], int]:\n",
    "    \"\"\"Count the frequency of every adjacent symbol pair in the vocabulary.\"\"\"\n",
    "    pairs: dict[tuple[str, str], int] = defaultdict(int)\n",
    "    for symbols, freq in vocab.items():\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
    "    return dict(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(\n",
    "    pair: tuple[str, str], vocab: dict[tuple[str, ...], int]\n",
    ") -> dict[tuple[str, ...], int]:\n",
    "    \"\"\"Merge all occurrences of `pair` in every word of the vocabulary.\"\"\"\n",
    "    new_vocab = {}\n",
    "    bigram = pair  # e.g., ('t', 'h')\n",
    "    replacement = \"\".join(pair)  # e.g., 'th'\n",
    "    for symbols, freq in vocab.items():\n",
    "        new_symbols: list[str] = []\n",
    "        i = 0\n",
    "        while i < len(symbols):\n",
    "            # Look for the bigram at position i\n",
    "            if i < len(symbols) - 1 and (symbols[i], symbols[i + 1]) == bigram:\n",
    "                new_symbols.append(replacement)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_symbols.append(symbols[i])\n",
    "                i += 1\n",
    "        new_vocab[tuple(new_symbols)] = freq\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(\n",
    "    text: str, num_merges: int\n",
    ") -> tuple[list[tuple[str, str]], dict[tuple[str, ...], int]]:\n",
    "    \"\"\"Train BPE on the given text for a specified number of merges.\n",
    "\n",
    "    Returns:\n",
    "        merges: Ordered list of merge operations (pair that was merged).\n",
    "        vocab: Final vocabulary after all merges.\n",
    "    \"\"\"\n",
    "    vocab = build_vocab(text)\n",
    "    merges: list[tuple[str, str]] = []\n",
    "\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            print(f\"No more pairs to merge at step {i}. Stopping early.\")\n",
    "            break\n",
    "        # Find the most frequent pair\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        vocab = merge_vocab(best_pair, vocab)\n",
    "        merges.append(best_pair)\n",
    "\n",
    "    return merges, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str, merges: list[tuple[str, str]]) -> list[str]:\n",
    "    \"\"\"Encode text into BPE tokens using a learned merge list.\"\"\"\n",
    "    tokens: list[str] = []\n",
    "    for word in text.split():\n",
    "        # Start with character-level representation\n",
    "        symbols = list(word) + [\"</w>\"]\n",
    "        # Apply each merge rule in order\n",
    "        for pair in merges:\n",
    "            merged = \"\".join(pair)\n",
    "            i = 0\n",
    "            new_symbols: list[str] = []\n",
    "            while i < len(symbols):\n",
    "                if (\n",
    "                    i < len(symbols) - 1\n",
    "                    and (symbols[i], symbols[i + 1]) == pair\n",
    "                ):\n",
    "                    new_symbols.append(merged)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "            symbols = new_symbols\n",
    "        tokens.extend(symbols)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def decode(tokens: list[str]) -> str:\n",
    "    \"\"\"Decode a list of BPE tokens back into text.\"\"\"\n",
    "    text = \"\".join(tokens)\n",
    "    # Replace end-of-word markers with spaces\n",
    "    text = text.replace(\"</w>\", \" \")\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Training on the Legal Corpus\n",
    "\n",
    "Let's train BPE with 500 merges and inspect the merge sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_MERGES = 500\n",
    "merges, final_vocab = train_bpe(training_corpus, NUM_MERGES)\n",
    "\n",
    "print(f\"Completed {len(merges)} merges.\")\n",
    "print(f\"Final vocabulary size: {len(set(sym for word in final_vocab for sym in word))}\")\n",
    "print(f\"\\nFirst 30 merges (in order):\")\n",
    "for i, (a, b) in enumerate(merges[:30]):\n",
    "    print(f\"  {i + 1:3d}. '{a}' + '{b}' -> '{a}{b}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Examining the Vocabulary at Different Merge Counts\n",
    "\n",
    "Let's see how the vocabulary evolves as we increase the number of merges.\n",
    "Tokens that appear as whole words in the vocabulary represent concepts the\n",
    "tokenizer has learned to treat as atomic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_vocab(vocab: dict[tuple[str, ...], int]) -> set[str]:\n",
    "    \"\"\"Extract the set of unique symbols from a BPE vocabulary.\"\"\"\n",
    "    return set(sym for word in vocab for sym in word)\n",
    "\n",
    "\n",
    "# Train at multiple merge counts to compare\n",
    "for n_merges in [50, 100, 200, 500]:\n",
    "    m, v = train_bpe(training_corpus, n_merges)\n",
    "    tokens = get_token_vocab(v)\n",
    "    # Show the longest tokens (these are the most \"complete\" words)\n",
    "    longest = sorted(tokens, key=len, reverse=True)[:10]\n",
    "    print(f\"\\n--- {n_merges} merges (vocab size: {len(tokens)}) ---\")\n",
    "    print(f\"Longest tokens: {longest}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "Notice how common legal terms gradually become single tokens as the number of\n",
    "merges increases. Words like \"the\", \"court\", \"that\", and \"of\" are among the\n",
    "first to be merged into whole tokens because they appear so frequently in\n",
    "judicial opinions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "### Vocabulary Size vs Number of Merges\n",
    "\n",
    "Each merge adds one new symbol to the vocabulary. Let's plot the growth and\n",
    "track when specific legal terms become single tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track vocabulary growth during training\n",
    "vocab_for_plot = build_vocab(training_corpus)\n",
    "initial_vocab_size = len(get_token_vocab(vocab_for_plot))\n",
    "\n",
    "merge_counts = [0]\n",
    "vocab_sizes = [initial_vocab_size]\n",
    "\n",
    "# Track when specific terms become single tokens\n",
    "target_words = [\"the</w>\", \"court</w>\", \"that</w>\", \"plaintiff</w>\", \"defendant</w>\"]\n",
    "word_merge_points: dict[str, int | None] = {w: None for w in target_words}\n",
    "\n",
    "vocab_tracking = build_vocab(training_corpus)\n",
    "for i in range(NUM_MERGES):\n",
    "    pairs = get_stats(vocab_tracking)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best_pair = max(pairs, key=pairs.get)\n",
    "    vocab_tracking = merge_vocab(best_pair, vocab_tracking)\n",
    "\n",
    "    current_tokens = get_token_vocab(vocab_tracking)\n",
    "    merge_counts.append(i + 1)\n",
    "    vocab_sizes.append(len(current_tokens))\n",
    "\n",
    "    # Check if any target words just became single tokens\n",
    "    for target in target_words:\n",
    "        if word_merge_points[target] is None and target in current_tokens:\n",
    "            word_merge_points[target] = i + 1\n",
    "\n",
    "print(\"Merge at which each word becomes a single token:\")\n",
    "for word, step in sorted(word_merge_points.items(), key=lambda x: x[1] or 9999):\n",
    "    label = word.replace(\"</w>\", \"\")\n",
    "    if step is not None:\n",
    "        print(f\"  '{label}' -> merge #{step}\")\n",
    "    else:\n",
    "        print(f\"  '{label}' -> not reached in {NUM_MERGES} merges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(merge_counts, vocab_sizes, linewidth=2, color=\"#2563eb\")\n",
    "\n",
    "# Annotate when target words become single tokens\n",
    "for word, step in word_merge_points.items():\n",
    "    if step is not None:\n",
    "        label = word.replace(\"</w>\", \"\")\n",
    "        size_at_step = vocab_sizes[step]\n",
    "        ax.annotate(\n",
    "            f'\"{label}\"',\n",
    "            xy=(step, size_at_step),\n",
    "            xytext=(step + 20, size_at_step + 5),\n",
    "            arrowprops=dict(arrowstyle=\"->\", color=\"#dc2626\"),\n",
    "            fontsize=9,\n",
    "            color=\"#dc2626\",\n",
    "        )\n",
    "        ax.plot(step, size_at_step, \"o\", color=\"#dc2626\", markersize=6)\n",
    "\n",
    "ax.set_xlabel(\"Number of Merges\")\n",
    "ax.set_ylabel(\"Vocabulary Size\")\n",
    "ax.set_title(\"BPE Vocabulary Growth on Legal Corpus\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### How Legal Terms Emerge\n",
    "\n",
    "Let's look at how specific legal terms are tokenized at different merge counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_terms = [\n",
    "    \"court\",\n",
    "    \"plaintiff\",\n",
    "    \"defendant\",\n",
    "    \"summary\",\n",
    "    \"judgment\",\n",
    "    \"injunction\",\n",
    "    \"discrimination\",\n",
    "    \"infringement\",\n",
    "]\n",
    "\n",
    "print(f\"{'Term':<20} {'50 merges':<25} {'200 merges':<25} {'500 merges'}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "for n_merges in [50, 200, 500]:\n",
    "    m, _ = train_bpe(training_corpus, n_merges)\n",
    "    globals()[f\"merges_{n_merges}\"] = m\n",
    "\n",
    "for term in legal_terms:\n",
    "    tok_50 = encode(term, globals()[\"merges_50\"])\n",
    "    tok_200 = encode(term, globals()[\"merges_200\"])\n",
    "    tok_500 = encode(term, globals()[\"merges_500\"])\n",
    "    t50 = \" | \".join(tok_50)\n",
    "    t200 = \" | \".join(tok_200)\n",
    "    t500 = \" | \".join(tok_500)\n",
    "    print(f\"{term:<20} {t50:<25} {t200:<25} {t500}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Round-Trip Test\n",
    "\n",
    "A correct tokenizer must preserve text through an encode-decode round trip.\n",
    "Let's verify this with a legal passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_passage = (\n",
    "    \"The district court granted summary judgment in favor of the defendant \"\n",
    "    \"Meridian Health Systems concluding that the plaintiff failed to establish \"\n",
    "    \"a prima facie case of discrimination under the Americans with Disabilities Act\"\n",
    ")\n",
    "\n",
    "# Encode with the 500-merge model\n",
    "encoded = encode(test_passage, merges)\n",
    "decoded = decode(encoded)\n",
    "\n",
    "print(f\"Original:  {test_passage}\")\n",
    "print(f\"\\nEncoded:   {encoded[:20]}... ({len(encoded)} tokens total)\")\n",
    "print(f\"\\nDecoded:   {decoded}\")\n",
    "print(f\"\\nRound-trip match: {test_passage == decoded}\")\n",
    "assert test_passage == decoded, \"Round-trip failed!\"\n",
    "print(\"\\nRound-trip test PASSED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a more complex passage including citations\n",
    "complex_passage = (\n",
    "    \"We review the district court grant of summary judgment de novo \"\n",
    "    \"construing all facts and drawing all reasonable inferences in \"\n",
    "    \"favor of the nonmoving party\"\n",
    ")\n",
    "\n",
    "encoded_complex = encode(complex_passage, merges)\n",
    "decoded_complex = decode(encoded_complex)\n",
    "\n",
    "print(f\"Original:  {complex_passage}\")\n",
    "print(f\"Decoded:   {decoded_complex}\")\n",
    "print(f\"Match:     {complex_passage == decoded_complex}\")\n",
    "assert complex_passage == decoded_complex, \"Round-trip failed!\"\n",
    "print(f\"Tokens:    {len(encoded_complex)}\")\n",
    "print(f\"\\nToken breakdown:\")\n",
    "for token in encoded_complex:\n",
    "    display = token.replace('</w>', '_')\n",
    "    print(f\"  [{display}]\", end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise (a): Unicode-Aware BPE\n",
    "\n",
    "The implementation above splits on whitespace and treats each character as a\n",
    "symbol. This works for ASCII text but breaks on Unicode characters that legal\n",
    "text commonly uses:\n",
    "\n",
    "- Section symbol: `\\u00a7` (\"\\u00a7\")\n",
    "- Em dash: `\\u2014` (\"\\u2014\")\n",
    "- Paragraph symbol: `\\u00b6` (\"\\u00b6\")\n",
    "- Various quotation marks: `\\u201c` `\\u201d` `\\u2018` `\\u2019`\n",
    "\n",
    "Modify the `build_vocab` function to handle Unicode properly. Two approaches:\n",
    "\n",
    "1. **Byte-level BPE**: Convert text to UTF-8 bytes and run BPE on byte values\n",
    "   (this is what GPT-2 does).\n",
    "2. **Unicode-aware character BPE**: Use Python's built-in Unicode support to\n",
    "   correctly iterate over characters (Python strings are already Unicode, so\n",
    "   the main issue is ensuring your pre-tokenization regex handles punctuation\n",
    "   categories correctly).\n",
    "\n",
    "Test your modified version on: `\"Pursuant to 42 U.S.C. \\u00a7 1983, the plaintiff\\u2014\"`\n",
    "\n",
    "### Exercise (b): Compression Ratio Comparison\n",
    "\n",
    "Compare how well BPE compresses legal text versus general English:\n",
    "\n",
    "1. Take a sample of general English text (e.g., a paragraph from Wikipedia).\n",
    "2. Take a passage of comparable length from the court opinions dataset.\n",
    "3. Train BPE on each corpus separately (with the same number of merges).\n",
    "4. Compute the **compression ratio**: `original_characters / num_tokens`.\n",
    "\n",
    "Which domain achieves better compression? Why? Think about vocabulary\n",
    "diversity, word frequency distributions, and the presence of specialized\n",
    "notation in legal text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
