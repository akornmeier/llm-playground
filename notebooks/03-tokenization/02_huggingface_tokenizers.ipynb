{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 02 - HuggingFace Tokenizers: Comparing Production Tokenizers on Legal Text\n",
    "\n",
    "In the previous notebook we built BPE from scratch. In practice, LLMs use\n",
    "highly optimized tokenizers trained on massive corpora. This notebook explores\n",
    "the tokenizers behind GPT-2 and LLaMA-2 using the HuggingFace `transformers`\n",
    "and `tokenizers` libraries.\n",
    "\n",
    "We will:\n",
    "1. Load pre-trained tokenizers and see how they work\n",
    "2. Tokenize legal text and compare token counts across models\n",
    "3. Visualize token boundaries to see where each tokenizer splits\n",
    "4. Compute **token fertility** -- the average number of tokens per word --\n",
    "   for legal vs general text\n",
    "5. Train a custom tokenizer on a legal corpus and compare it against GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# %pip install transformers tokenizers matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 1. Loading Pre-trained Tokenizers\n",
    "\n",
    "HuggingFace provides tokenizers for every model on the Hub. We load two\n",
    "widely used tokenizers:\n",
    "\n",
    "- **GPT-2** -- BPE with 50,257 tokens, trained on WebText\n",
    "- **LLaMA-2** -- BPE with 32,000 tokens, trained on a larger and more\n",
    "  diverse corpus\n",
    "\n",
    "These two tokenizers were trained on different data with different vocabulary\n",
    "sizes, which leads to measurably different behavior on domain-specific text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizers\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    # If you don't have access to LLaMA-2, you can use an open alternative:\n",
    "    # \"NousResearch/Llama-2-7b-hf\"\n",
    ")\n",
    "\n",
    "print(f\"GPT-2 vocab size:  {gpt2_tokenizer.vocab_size:,}\")\n",
    "print(f\"LLaMA-2 vocab size: {llama_tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic tokenization demonstration\n",
    "sample = \"The court granted summary judgment.\"\n",
    "\n",
    "for name, tokenizer in [(\"GPT-2\", gpt2_tokenizer), (\"LLaMA-2\", llama_tokenizer)]:\n",
    "    token_ids = tokenizer.encode(sample)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    decoded = tokenizer.decode(token_ids)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Token IDs: {token_ids}\")\n",
    "    print(f\"  Tokens:    {tokens}\")\n",
    "    print(f\"  Decoded:   {decoded}\")\n",
    "    print(f\"  Count:     {len(token_ids)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 2. Tokenizing Legal Text\n",
    "\n",
    "Let's load our court opinions dataset and see how these tokenizers handle\n",
    "real legal text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../../datasets/sample/court_opinions.jsonl\")\n",
    "\n",
    "opinions = []\n",
    "with open(DATA_PATH) as f:\n",
    "    for line in f:\n",
    "        opinions.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(opinions)} court opinions\")\n",
    "for op in opinions:\n",
    "    print(f\"  - {op['case_name'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key example: tokenizing a legal citation\n",
    "citation_examples = [\n",
    "    \"42 U.S.C. \\u00a7 1983\",\n",
    "    \"Anderson v. Liberty Lobby, Inc., 477 U.S. 242, 255 (1986)\",\n",
    "    \"35 U.S.C. section 271\",\n",
    "    \"20 U.S.C. sections 1400-1482\",\n",
    "    \"Federal Rule of Civil Procedure 12(b)(6)\",\n",
    "]\n",
    "\n",
    "print(f\"{'Citation':<55} {'GPT-2':>6} {'LLaMA':>6}\")\n",
    "print(\"-\" * 70)\n",
    "for citation in citation_examples:\n",
    "    gpt2_count = len(gpt2_tokenizer.encode(citation))\n",
    "    llama_count = len(llama_tokenizer.encode(citation))\n",
    "    print(f\"{citation:<55} {gpt2_count:>6} {llama_count:>6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed breakdown of \"42 U.S.C. \\u00a7 1983\"\n",
    "example = \"42 U.S.C. \\u00a7 1983\"\n",
    "print(f\"Text: {example}\\n\")\n",
    "\n",
    "for name, tokenizer in [(\"GPT-2\", gpt2_tokenizer), (\"LLaMA-2\", llama_tokenizer)]:\n",
    "    ids = tokenizer.encode(example)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "    print(f\"{name} ({len(ids)} tokens):\")\n",
    "    for token_id, token in zip(ids, tokens):\n",
    "        print(f\"  ID {token_id:>6}  ->  {repr(token)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full opinion token count comparison\n",
    "print(f\"{'Case Name':<55} {'Words':>6} {'GPT-2':>7} {'LLaMA':>7} {'Ratio':>7}\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "for opinion in opinions:\n",
    "    text = opinion[\"text\"]\n",
    "    word_count = len(text.split())\n",
    "    gpt2_count = len(gpt2_tokenizer.encode(text))\n",
    "    llama_count = len(llama_tokenizer.encode(text))\n",
    "    # Ratio: how many more tokens does GPT-2 need vs LLaMA?\n",
    "    ratio = gpt2_count / llama_count if llama_count > 0 else 0\n",
    "    name = opinion[\"case_name\"][:53]\n",
    "    print(f\"{name:<55} {word_count:>6} {gpt2_count:>7} {llama_count:>7} {ratio:>7.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 3. Visualizing Token Boundaries\n",
    "\n",
    "To see *where* each tokenizer splits, we write a function that marks the\n",
    "boundaries between tokens. This makes it easy to spot differences in how\n",
    "tokenizers segment the same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_token_boundaries(text: str, tokenizer, name: str = \"\") -> str:\n",
    "    \"\"\"Show token boundaries using bracket notation.\n",
    "\n",
    "    Each token is wrapped in brackets: [token1][token2][token3]\n",
    "    This makes it easy to see where the tokenizer splits the text.\n",
    "    \"\"\"\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    # Clean up token representations for display\n",
    "    display_tokens = []\n",
    "    for token in tokens:\n",
    "        # GPT-2 uses \\u0120 prefix for space, LLaMA uses \\u2581\n",
    "        cleaned = token.replace(\"\\u0120\", \" \").replace(\"\\u2581\", \" \")\n",
    "        display_tokens.append(cleaned)\n",
    "    bracketed = \"\".join(f\"[{t}]\" for t in display_tokens)\n",
    "    if name:\n",
    "        return f\"{name} ({len(tokens)} tokens):\\n  {bracketed}\"\n",
    "    return bracketed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_passages = [\n",
    "    \"42 U.S.C. \\u00a7 1983\",\n",
    "    \"The district court's grant of summary judgment\",\n",
    "    \"Anderson v. Liberty Lobby, Inc., 477 U.S. 242, 255 (1986)\",\n",
    "    \"prima facie case of discrimination\",\n",
    "    \"National Pollutant Discharge Elimination System\",\n",
    "]\n",
    "\n",
    "for passage in legal_passages:\n",
    "    print(f\"Text: {passage}\")\n",
    "    print(show_token_boundaries(passage, gpt2_tokenizer, \"GPT-2\"))\n",
    "    print(show_token_boundaries(passage, llama_tokenizer, \"LLaMA-2\"))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a longer passage\n",
    "long_passage = opinions[0][\"text\"][:300]\n",
    "print(f\"Passage (first 300 chars of {opinions[0]['case_name']}):\\n\")\n",
    "print(f\"{long_passage}\\n\")\n",
    "print(show_token_boundaries(long_passage, gpt2_tokenizer, \"GPT-2\"))\n",
    "print()\n",
    "print(show_token_boundaries(long_passage, llama_tokenizer, \"LLaMA-2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 4. Token Fertility Analysis\n",
    "\n",
    "**Token fertility** is the average number of tokens produced per whitespace\n",
    "word. A fertility of 1.0 means each word maps to exactly one token. Values\n",
    "above 1.0 mean words are being split into subword pieces.\n",
    "\n",
    "Higher fertility on legal text compared to general text means:\n",
    "- Legal documents consume more of the context window\n",
    "- API costs are proportionally higher\n",
    "- The model must compose meaning from more fragmented representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fertility(text: str, tokenizer) -> float:\n",
    "    \"\"\"Compute token fertility: average tokens per whitespace word.\"\"\"\n",
    "    word_count = len(text.split())\n",
    "    token_count = len(tokenizer.encode(text))\n",
    "    if word_count == 0:\n",
    "        return 0.0\n",
    "    return token_count / word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legal text: from court opinions dataset\n",
    "legal_text = \" \".join(op[\"text\"] for op in opinions)\n",
    "\n",
    "# General English text: a plain-language paragraph for comparison\n",
    "general_text = (\n",
    "    \"The weather was warm and sunny on Saturday morning. Sarah decided to go \"\n",
    "    \"for a walk in the park near her house. She brought her dog Max along for \"\n",
    "    \"the exercise. The park had tall oak trees and a small pond where ducks \"\n",
    "    \"were swimming. Children were playing on the swings and slides while their \"\n",
    "    \"parents watched from nearby benches. After walking for about an hour she \"\n",
    "    \"stopped at the coffee shop on the corner and ordered a latte. The barista \"\n",
    "    \"recognized her and asked about her weekend plans. She mentioned that she \"\n",
    "    \"was thinking about visiting the new bookstore that had just opened downtown. \"\n",
    "    \"It was a pleasant morning and she felt grateful for the simple joy of a \"\n",
    "    \"good walk on a beautiful day. When she got home she made breakfast and \"\n",
    "    \"read the newspaper while Max napped on the couch beside her.\"\n",
    ")\n",
    "\n",
    "print(f\"{'Corpus':<15} {'Words':>7} {'GPT-2 tokens':>13} {'GPT-2 fert.':>12} \"\n",
    "      f\"{'LLaMA tokens':>13} {'LLaMA fert.':>12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for corpus_name, text in [(\"Legal\", legal_text), (\"General\", general_text)]:\n",
    "    word_count = len(text.split())\n",
    "    gpt2_tokens = len(gpt2_tokenizer.encode(text))\n",
    "    llama_tokens = len(llama_tokenizer.encode(text))\n",
    "    gpt2_fert = compute_fertility(text, gpt2_tokenizer)\n",
    "    llama_fert = compute_fertility(text, llama_tokenizer)\n",
    "    print(f\"{corpus_name:<15} {word_count:>7} {gpt2_tokens:>13} {gpt2_fert:>12.3f} \"\n",
    "          f\"{llama_tokens:>13} {llama_fert:>12.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-opinion fertility comparison\n",
    "gpt2_fertilities = []\n",
    "llama_fertilities = []\n",
    "\n",
    "for op in opinions:\n",
    "    gpt2_fertilities.append(compute_fertility(op[\"text\"], gpt2_tokenizer))\n",
    "    llama_fertilities.append(compute_fertility(op[\"text\"], llama_tokenizer))\n",
    "\n",
    "general_gpt2_fert = compute_fertility(general_text, gpt2_tokenizer)\n",
    "general_llama_fert = compute_fertility(general_text, llama_tokenizer)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x_labels = [op[\"case_name\"].split(\" v. \")[0][:15] for op in opinions] + [\"General\\nEnglish\"]\n",
    "x = range(len(x_labels))\n",
    "\n",
    "gpt2_vals = gpt2_fertilities + [general_gpt2_fert]\n",
    "llama_vals = llama_fertilities + [general_llama_fert]\n",
    "\n",
    "width = 0.35\n",
    "bars1 = ax.bar([i - width / 2 for i in x], gpt2_vals, width, label=\"GPT-2\", color=\"#2563eb\")\n",
    "bars2 = ax.bar([i + width / 2 for i in x], llama_vals, width, label=\"LLaMA-2\", color=\"#dc2626\")\n",
    "\n",
    "ax.set_ylabel(\"Token Fertility (tokens per word)\")\n",
    "ax.set_title(\"Token Fertility: Legal Opinions vs General English\")\n",
    "ax.set_xticks(list(x))\n",
    "ax.set_xticklabels(x_labels, rotation=30, ha=\"right\")\n",
    "ax.legend()\n",
    "ax.axhline(y=1.0, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"1:1 ratio\")\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost implications\n",
    "legal_fert_gpt2 = sum(gpt2_fertilities) / len(gpt2_fertilities)\n",
    "overhead = (legal_fert_gpt2 - general_gpt2_fert) / general_gpt2_fert * 100\n",
    "\n",
    "print(f\"Average GPT-2 fertility on legal text:   {legal_fert_gpt2:.3f}\")\n",
    "print(f\"GPT-2 fertility on general English:      {general_gpt2_fert:.3f}\")\n",
    "print(f\"Legal text token overhead:               {overhead:+.1f}%\")\n",
    "print()\n",
    "print(\"Cost implication at $0.01 per 1K tokens:\")\n",
    "print(f\"  A 10,000-word legal document produces ~{int(10000 * legal_fert_gpt2):,} tokens\")\n",
    "print(f\"  A 10,000-word general document produces ~{int(10000 * general_gpt2_fert):,} tokens\")\n",
    "print(f\"  Extra cost per legal document: ${10000 * (legal_fert_gpt2 - general_gpt2_fert) * 0.01 / 1000:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 5. Training a Custom Tokenizer\n",
    "\n",
    "If legal text tokenizes poorly with general-purpose tokenizers, can we do\n",
    "better? The HuggingFace `tokenizers` library provides fast, production-quality\n",
    "BPE training. Let's train a custom tokenizer on our legal corpus and compare\n",
    "it against GPT-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data -- write the legal corpus to a temporary file\n",
    "# (the tokenizers library trains from files for efficiency)\n",
    "import tempfile\n",
    "\n",
    "corpus_path = Path(tempfile.mktemp(suffix=\".txt\"))\n",
    "with open(corpus_path, \"w\") as f:\n",
    "    for opinion in opinions:\n",
    "        f.write(opinion[\"text\"] + \"\\n\")\n",
    "\n",
    "# Also add legislation for more domain coverage\n",
    "legislation_path = Path(\"../../datasets/sample/legislation.jsonl\")\n",
    "if legislation_path.exists():\n",
    "    with open(legislation_path) as f_in, open(corpus_path, \"a\") as f_out:\n",
    "        for line in f_in:\n",
    "            record = json.loads(line)\n",
    "            f_out.write(record[\"text\"] + \"\\n\")\n",
    "\n",
    "print(f\"Training corpus written to: {corpus_path}\")\n",
    "print(f\"Corpus size: {corpus_path.stat().st_size:,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a BPE tokenizer with a vocabulary size comparable to GPT-2\n",
    "legal_tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "legal_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=10000,  # Smaller vocab since our corpus is small\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "legal_tokenizer.train([str(corpus_path)], trainer)\n",
    "print(f\"Trained legal tokenizer with {legal_tokenizer.get_vocab_size()} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare vocabularies: find legal terms in each tokenizer's vocabulary\n",
    "legal_vocab = legal_tokenizer.get_vocab()\n",
    "gpt2_vocab = gpt2_tokenizer.get_vocab()\n",
    "\n",
    "legal_terms_to_check = [\n",
    "    \"court\", \"plaintiff\", \"defendant\", \"judgment\", \"injunction\",\n",
    "    \"statute\", \"amend\", \"jurisdiction\", \"certiorari\", \"appellant\",\n",
    "    \"appellee\", \"arbitration\", \"deposition\", \"affidavit\",\n",
    "    \"subpoena\", \"tort\", \"negligence\", \"liability\",\n",
    "]\n",
    "\n",
    "print(f\"{'Term':<20} {'In Legal Vocab':>15} {'In GPT-2 Vocab':>15}\")\n",
    "print(\"-\" * 52)\n",
    "for term in legal_terms_to_check:\n",
    "    in_legal = term in legal_vocab\n",
    "    in_gpt2 = term in gpt2_vocab or f\"\\u0120{term}\" in gpt2_vocab\n",
    "    print(f\"{term:<20} {'yes':>15} {'yes' if in_gpt2 else 'no':>15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tokenization of legal text\n",
    "comparison_texts = [\n",
    "    \"42 U.S.C. \\u00a7 1983\",\n",
    "    \"summary judgment de novo\",\n",
    "    \"prima facie case of discrimination\",\n",
    "    \"National Pollutant Discharge Elimination System\",\n",
    "    \"per- and polyfluoroalkyl substances\",\n",
    "]\n",
    "\n",
    "print(f\"{'Text':<50} {'GPT-2':>6} {'Legal':>6}\")\n",
    "print(\"-\" * 65)\n",
    "for text in comparison_texts:\n",
    "    gpt2_count = len(gpt2_tokenizer.encode(text))\n",
    "    legal_enc = legal_tokenizer.encode(text)\n",
    "    legal_count = len(legal_enc.ids)\n",
    "    print(f\"{text:<50} {gpt2_count:>6} {legal_count:>6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how the custom tokenizer segments legal text differently\n",
    "example = \"The district court granted summary judgment in favor of the defendant\"\n",
    "\n",
    "gpt2_ids = gpt2_tokenizer.encode(example)\n",
    "gpt2_tokens = gpt2_tokenizer.convert_ids_to_tokens(gpt2_ids)\n",
    "legal_enc = legal_tokenizer.encode(example)\n",
    "legal_tokens = legal_enc.tokens\n",
    "\n",
    "print(f\"Text: {example}\\n\")\n",
    "print(f\"GPT-2 ({len(gpt2_tokens)} tokens):\")\n",
    "for t in gpt2_tokens:\n",
    "    display = t.replace(\"\\u0120\", \" \")\n",
    "    print(f\"  [{display}]\", end=\"\")\n",
    "print()\n",
    "\n",
    "print(f\"\\nLegal ({len(legal_tokens)} tokens):\")\n",
    "for t in legal_tokens:\n",
    "    print(f\"  [{t}]\", end=\"\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary file\n",
    "corpus_path.unlink(missing_ok=True)\n",
    "print(\"Cleaned up temporary training corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise (a): Fertility by Legal Document Type\n",
    "\n",
    "Compute the token fertility for different types of legal text:\n",
    "\n",
    "1. **Statutes** -- Load `datasets/sample/legislation.jsonl` and compute\n",
    "   fertility for each piece of legislation.\n",
    "2. **Court opinions** -- Use the court opinions from this notebook.\n",
    "3. **Contracts** -- Write or find a sample contract clause (e.g., an\n",
    "   indemnification provision) and compute its fertility.\n",
    "\n",
    "Compare across tokenizers. Which type of legal text has the highest fertility?\n",
    "Why might that be? Consider:\n",
    "- Statutory language uses defined terms and cross-references heavily\n",
    "- Court opinions include case citations and procedural terminology\n",
    "- Contracts use boilerplate phrasing and nested conditional clauses\n",
    "\n",
    "### Exercise (b): Cost Estimation\n",
    "\n",
    "A legal technology company processes 1,000 court opinions per day, each\n",
    "averaging 5,000 words. Using an API priced at $0.01 per 1,000 tokens:\n",
    "\n",
    "1. Compute the daily token count and daily cost using GPT-2's tokenizer\n",
    "   fertility.\n",
    "2. Estimate how much could be saved if a custom legal tokenizer reduced\n",
    "   fertility by 10%, 15%, or 20%.\n",
    "3. Over a year (250 working days), what is the total savings at each\n",
    "   reduction level?\n",
    "\n",
    "Present your results in a table. At what fertility reduction does the cost\n",
    "savings justify the engineering effort of training and deploying a custom\n",
    "tokenizer?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
