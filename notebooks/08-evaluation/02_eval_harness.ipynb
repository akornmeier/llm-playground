{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "harness-design",
   "metadata": {},
   "source": [
    "# 02 - Building an Assessment Harness\n",
    "\n",
    "## Harness Design\n",
    "\n",
    "An assessment harness is a systematic way to test model capabilities.\n",
    "Instead of ad-hoc manual testing, a harness provides:\n",
    "\n",
    "- **Reproducibility**: same test cases, same scoring, every time.\n",
    "- **Coverage**: structured test cases that cover different capabilities.\n",
    "- **Automation**: scoring runs without human intervention.\n",
    "- **Tracking**: results are stored and compared across model versions.\n",
    "\n",
    "Components of a harness:\n",
    "1. **Test cases**: structured inputs with expected outputs.\n",
    "2. **Automated scoring**: metrics applied to each test case.\n",
    "3. **Reporting**: aggregated results in tables and charts.\n",
    "\n",
    "This follows the pattern used by projects like EleutherAI's\n",
    "lm-evaluation-harness, adapted for legal domain testing.\n",
    "\n",
    "**CoCounsel context:** For a legal AI product, the harness must test\n",
    "citation accuracy, factual grounding, and appropriate uncertainty --\n",
    "not just fluency or generic helpfulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-cases-heading",
   "metadata": {},
   "source": [
    "## Building Test Cases\n",
    "\n",
    "Each test case has:\n",
    "- `question`: a legal question the model should answer.\n",
    "- `context`: relevant source material (from court opinions).\n",
    "- `ground_truth`: the expected answer or key facts.\n",
    "- `expected_citations`: list of citations the answer should include.\n",
    "\n",
    "We create 18 test cases spanning different legal domains: employment\n",
    "law, environmental law, securities regulation, education law, patent\n",
    "law, constitutional law, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-opinions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load court opinions for context\n",
    "data_path = Path(\"../../datasets/sample/court_opinions.jsonl\")\n",
    "opinions = []\n",
    "with open(data_path) as f:\n",
    "    for line in f:\n",
    "        opinions.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(opinions)} court opinions for test case context\")\n",
    "for op in opinions:\n",
    "    print(f\"  - {op['case_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-cases",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18 legal test cases for the harness\n",
    "test_cases = [\n",
    "    # --- Cases derived from court opinions dataset ---\n",
    "    {\n",
    "        \"id\": \"TC-001\",\n",
    "        \"question\": \"What standard of review applies when an appellate court reviews a grant of summary judgment?\",\n",
    "        \"context\": opinions[0][\"text\"],\n",
    "        \"ground_truth\": (\n",
    "            \"An appellate court reviews a grant of summary judgment de novo, \"\n",
    "            \"construing all facts and drawing all reasonable inferences in \"\n",
    "            \"favor of the nonmoving party.\"\n",
    "        ),\n",
    "        \"expected_citations\": [\"Anderson v. Liberty Lobby, Inc., 477 U.S. 242 (1986)\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC-002\",\n",
    "        \"question\": \"What elements must a plaintiff establish in an ADA employment discrimination case?\",\n",
    "        \"context\": opinions[0][\"text\"],\n",
    "        \"ground_truth\": (\n",
    "            \"Under the ADA, a plaintiff must show: (1) he is disabled within the \"\n",
    "            \"meaning of the Act, (2) he is qualified to perform the essential \"\n",
    "            \"functions of the job with or without reasonable accommodation, and \"\n",
    "            \"(3) he suffered an adverse employment action because of his disability.\"\n",
    "        ),\n",
    "        \"expected_citations\": [\"Hoffman v. Caterpillar, Inc., 256 F.3d 568 (7th Cir. 2001)\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC-003\",\n",
    "        \"question\": \"What factors must a court consider when deciding a motion for preliminary injunction?\",\n",
    "        \"context\": opinions[1][\"text\"],\n",
    "        \"ground_truth\": (\n",
    "            \"A preliminary injunction requires demonstrating: (1) likelihood of \"\n",
    "            \"success on the merits, (2) likelihood of irreparable harm absent \"\n",
    "            \"relief, (3) balance of equities favoring the movant, and (4) an \"\n",
    "            \"injunction serves the public interest.\"\n",
    "        ),\n",
    "        \"expected_citations\": [\"Winter v. Natural Resources Defense Council, Inc., 555 U.S. 7 (2008)\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC-004\",\n",
    "        \"question\": \"Can potential groundwater contamination constitute irreparable harm in environmental cases?\",\n",
    "        \"context\": opinions[1][\"text\"],\n",
    "        \"ground_truth\": (\n",
    "            \"Yes. Courts have recognized that potential contamination of \"\n",
    "            \"groundwater supplies serving as drinking water sources constitutes \"\n",
    "            \"irreparable harm in environmental cases.\"\n",
    "        ),\n",
    "        \"expected_citations\": [\"Amoco Production Co. v. Village of Gambell, 480 U.S. 531 (1987)\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC-005\",\n",
    "        \"question\": \"What standard governs judicial review of SEC sanctions?\",\n",
    "        \"context\": opinions[2][\"text\"],\n",
    "        \"ground_truth\": (\n",
    "            \"Review of SEC sanctions is deferential. Courts uphold findings of \"\n",
    "            \"fact if supported by substantial evidence and defer to the \"\n",
    "            \"Commission's choice of sanction unless unwarranted in law or \"\n",
    "            \"without justification in fact.\"\n",
    "        ),\n",
    "        \"expected_citations\": [\"Steadman v. SEC, 603 F.2d 1126 (5th Cir. 1979)\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC-006\",\n",
    "        \"question\": \"What fiduciary duties do investment advisers owe their clients regarding IPO allocations?\",\n",
    "        \"context\": opinions[2][\"text\"],\n",
    "        \"ground_truth\": (\n",
    "            \"Investment advisers must disclose material conflicts of interest. \"\n",
    "            \"Allocating IPO shares to proprietary accounts before satisfying \"\n",
    "            \"client orders, combined with misrepresentations in Form ADV, \"\n",
    "            \"demonstrates conduct incompatible with fiduciary duties.\"\n",
    "        ),\n",
    "        \"expected_citations\": [\"SEC v. Capital Gains Research Bureau, Inc., 375 U.S. 180 (1963)\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC-007\",\n",
    "        \"question\": \"What standard of review applies to IDEA eligibility decisions?\",\n",
    "        \"context\": opinions[3][\"text\"],\n",
    "        \"ground_truth\": (\n",
    "            \"A reviewing court applies a modified de novo standard, giving \"\n",
    "            \"due weight to the determinations of the administrative hearing \"\n",
    "            \"officer while independently weighing the evidence. The party \"\n",
    "            \"challenging the administrative decision bears the burden of persuasion.\"\n",
    "        ),\n",
    "        \"expected_citations\": [\n",
    "            \"Board of Education v. Rowley, 458 U.S. 176 (1982)\",\n",
    "            \"Schaffer ex rel. Schaffer v. Weast, 546 U.S. 49 (2005)\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC-008\",\n",
    "        \"question\": \"What must a patent infringement complaint allege to survive a motion to dismiss?\",\n",
    "        \"context\": opinions[4][\"text\"],\n",
    "        \"ground_truth\": (\n",
    "            \"A plaintiff must allege facts that plausibly establish ownership \"\n",
    "            \"of a valid patent and infringement by the defendant. Detailed \"\n",
    "            \"claim-by-claim analysis is not required at the pleading stage, but \"\n",
    "            \"sufficient factual content must allow inference of infringement.\"\n",
    "        ),\n",
    "        \"expected_citations\": [\n",
    "            \"Nalco Co. v. Chem-Mod, LLC, 883 F.3d 1337 (Fed. Cir. 2018)\",\n",
    "            \"Ashcroft v. Iqbal, 556 U.S. 662 (2009)\",\n",
    "        ],\n",
    "    },\n",
    "    # --- General legal knowledge test cases (no specific opinion context) ---\n",
    "    {\n",
    "        \"id\": \"TC-009\",\n",
    "        \"question\": \"What is the summary judgment standard under Federal Rule of Civil Procedure 56?\",\n",
    "        \"context\": (\n",
    "            \"Federal Rule of Civil Procedure 56 provides that a court shall \"\n",
    "            \"grant summary judgment if the movant shows that there is no genuine \"\n",
    "            \"dispute as to any material fact and the movant is entitled to \"\n",
    "            \"judgment as a matter of law. The Supreme Court's trilogy of cases \"\n",
    "            \"in 1986 clarified the standard.\"\n",
    "        ),\n",
    "        \"ground_truth\": (\n",
    "            \"Summary judgment is appropriate when there is no genuine dispute \"\n",
    "            \"of material fact and the movant is entitled to judgment as a matter \"\n",
    "            \"of law. The moving party bears the initial burden of demonstrating \"\n",
    "            \"the absence of a genuine issue.\"\n",
    "        ),\n",
    "        \"expected_citations\": [\"Celotex Corp. v. Catrett, 477 U.S. 317 (1986)\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC-010\",\n",
    "        \"question\": \"What constitutional amendment protects against unreasonable searches and seizures?\",\n",
    "        \"context\": (\n",
    "            \"The Bill of Rights contains several amendments protecting individual \"\n",
    "            \"liberties against government intrusion. The Fourth Amendment \"\n",
    "            \"specifically addresses the right of the people to be secure in \"\n",
    "            \"their persons, houses, papers, and effects against unreasonable \"\n",
    "            \"searches and seizures.\"\n",
    "        ),\n",
    "        \"ground_truth\": (\n",
    "            \"The Fourth Amendment protects against unreasonable searches and \"\n",
    "            \"seizures. Under Katz v. United States, a search occurs when the \"\n",
    "            \"government violates a reasonable expectation of privacy.\"\n",
    "        ),\n",
    "        \"expected_citations\": [\"Katz v. United States, 389 U.S. 347 (1967)\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC-011\",\n",
    "        \"question\": \"What is the burden-shifting framework for employment discrimination cases?\",\n",
    "        \"context\": (\n",
    "            \"Employment discrimination claims under Title VII of the Civil Rights \"\n",
    "            \"Act of 1964 often rely on circumstantial evidence. The Supreme Court \"\n",
    "            \"established a framework for analyzing such claims that allocates \"\n",
    "            \"burdens of production and proof between the parties.\"\n",
    "        ),\n",
    "        \"ground_truth\": (\n",
    "            \"The McDonnell Douglas framework requires the plaintiff to establish \"\n",
    "            \"a prima facie case, then shifts the burden to the employer to \"\n",
    "            \"articulate a legitimate nondiscriminatory reason, and finally \"\n",
    "            \"allows the plaintiff to show pretext.\"\n",
    "        ),\n",
    "        \"expected_citations\": [\"McDonnell Douglas Corp. v. Green, 411 U.S. 792 (1973)\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC-012\",\n",
    "        \"question\": \"What is the plausibility standard for federal complaints?\",\n",
    "        \"context\": (\n",
    "            \"Federal pleading standards require that a complaint contain a short \"\n",
    "            \"and plain statement of the claim showing the pleader is entitled to \"\n",
    "            \"relief. The Supreme Court has interpreted this requirement in recent \"\n",
    "            \"landmark decisions that moved away from the old notice pleading standard.\"\n",
    "        ),\n",
    "        \"ground_truth\": (\n",
    "            \"A complaint must contain factual content that allows the court to \"\n",
    "            \"draw a reasonable inference that the defendant is liable. Threadbare \"\n",
    "            \"recitals of elements supported by conclusory statements do not suffice.\"\n",
    "        ),\n",
    "        \"expected_citations\": [\n",
    "            \"Ashcroft v. Iqbal, 556 U.S. 662 (2009)\",\n",
    "            \"Bell Atlantic Corp. v. Twombly, 550 U.S. 544 (2007)\",\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC-013\",\n",
    "        \"question\": \"When does the statute of limitations begin to run for latent injuries?\",\n",
    "        \"context\": (\n",
    "            \"Statutes of limitations set deadlines for filing lawsuits. For most \"\n",
    "            \"torts, the clock begins at the time of injury. However, some injuries \"\n",
    "            \"are not immediately discoverable, creating a tension between the \"\n",
    "            \"policy of repose and the right to seek redress.\"\n",
    "        ),\n",
    "        \"ground_truth\": (\n",
    "            \"Under the discovery rule, the statute of limitations begins to run \"\n",
    "            \"when the plaintiff knew or should have known of the injury and its \"\n",
    "            \"cause. This tolls the limitations period for latent injuries that \"\n",
    "            \"are not immediately apparent.\"\n",
    "        ),\n",
    "        \"expected_citations\": [],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC-014\",\n",
    "        \"question\": \"What is qualified immunity and how does it protect government officials?\",\n",
    "        \"context\": (\n",
    "            \"Section 1983 of Title 42 allows individuals to sue state officials \"\n",
    "            \"for constitutional violations. However, officials may assert \"\n",
    "            \"affirmative defenses based on their official status. The doctrine \"\n",
    "            \"of immunity has evolved significantly through Supreme Court decisions.\"\n",
    "        ),\n",
    "        \"ground_truth\": (\n",
    "            \"Qualified immunity shields government officials from civil liability \"\n",
    "            \"unless their conduct violates clearly established statutory or \"\n",
    "            \"constitutional rights of which a reasonable person would have known.\"\n",
    "        ),\n",
    "        \"expected_citations\": [\"Harlow v. Fitzgerald, 457 U.S. 800 (1982)\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC-015\",\n",
    "        \"question\": \"What does the Clean Water Act require regarding discharge of pollutants?\",\n",
    "        \"context\": opinions[1][\"text\"],\n",
    "        \"ground_truth\": (\n",
    "            \"The Clean Water Act prohibits the discharge of pollutants into \"\n",
    "            \"waters of the United States without the required National Pollutant \"\n",
    "            \"Discharge Elimination System (NPDES) permits.\"\n",
    "        ),\n",
    "        \"expected_citations\": [],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC-016\",\n",
    "        \"question\": \"What must a school district do with independent educational evaluations under IDEA?\",\n",
    "        \"context\": opinions[3][\"text\"],\n",
    "        \"ground_truth\": (\n",
    "            \"Under 34 C.F.R. section 300.502(c), a school district's evaluation \"\n",
    "            \"team must consider independent evaluations submitted by parents. \"\n",
    "            \"Failure to do so constitutes a procedural violation of the IDEA.\"\n",
    "        ),\n",
    "        \"expected_citations\": [\"Board of Education v. Rowley, 458 U.S. 176 (1982)\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC-017\",\n",
    "        \"question\": \"Can the SEC revoke an investment adviser's registration for conflicts of interest?\",\n",
    "        \"context\": opinions[2][\"text\"],\n",
    "        \"ground_truth\": (\n",
    "            \"Yes. The SEC has broad discretion in selecting sanctions to protect \"\n",
    "            \"the investing public. Systematic prioritization of proprietary \"\n",
    "            \"trading over client interests, combined with misrepresentations, \"\n",
    "            \"can justify revocation even without actual client financial loss.\"\n",
    "        ),\n",
    "        \"expected_citations\": [\"Steadman v. SEC, 603 F.2d 1126 (5th Cir. 1979)\"],\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC-018\",\n",
    "        \"question\": \"What is the difference between essential and marginal job functions under the ADA?\",\n",
    "        \"context\": opinions[0][\"text\"],\n",
    "        \"ground_truth\": (\n",
    "            \"Essential functions are fundamental duties of a position. Courts \"\n",
    "            \"examine whether other employees in the role perform the same \"\n",
    "            \"function, the employer's judgment, and the consequences of not \"\n",
    "            \"requiring the function. A disputed characterization of a function \"\n",
    "            \"as essential can defeat summary judgment.\"\n",
    "        ),\n",
    "        \"expected_citations\": [\"Hoffman v. Caterpillar, Inc., 256 F.3d 568 (7th Cir. 2001)\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Created {len(test_cases)} legal test cases\")\n",
    "print()\n",
    "for tc in test_cases:\n",
    "    n_cites = len(tc[\"expected_citations\"])\n",
    "    print(f\"  {tc['id']}: {tc['question'][:65]}... [{n_cites} citations]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scoring-heading",
   "metadata": {},
   "source": [
    "## Automated Scoring\n",
    "\n",
    "We build a scoring pipeline that measures each test case on multiple\n",
    "dimensions. For this notebook, we simulate model outputs rather than\n",
    "requiring a running model -- the focus is on the harness design, not\n",
    "the model.\n",
    "\n",
    "Scoring dimensions:\n",
    "1. **Citation accuracy**: fraction of citations that are real.\n",
    "2. **ROUGE-L**: overlap with ground truth answer.\n",
    "3. **Hallucination rate**: fraction of claims not grounded in context.\n",
    "4. **Overall**: weighted combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics-from-notebook-01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-implement the metrics from notebook 01 so this notebook is self-contained.\n",
    "\n",
    "import evaluate\n",
    "\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "# -- Citation corpus --\n",
    "citation_corpus = set()\n",
    "for opinion in opinions:\n",
    "    for cite in opinion.get(\"citations\", []):\n",
    "        citation_corpus.add(cite)\n",
    "\n",
    "citation_corpus.update([\n",
    "    \"Marbury v. Madison, 5 U.S. 137 (1803)\",\n",
    "    \"Brown v. Board of Education, 347 U.S. 483 (1954)\",\n",
    "    \"Miranda v. Arizona, 384 U.S. 436 (1966)\",\n",
    "    \"Chevron U.S.A., Inc. v. NRDC, 467 U.S. 837 (1984)\",\n",
    "    \"Celotex Corp. v. Catrett, 477 U.S. 317 (1986)\",\n",
    "    \"Katz v. United States, 389 U.S. 347 (1967)\",\n",
    "    \"Bell Atlantic Corp. v. Twombly, 550 U.S. 544 (2007)\",\n",
    "    \"Ashcroft v. Iqbal, 556 U.S. 662 (2009)\",\n",
    "    \"McDonnell Douglas Corp. v. Green, 411 U.S. 792 (1973)\",\n",
    "    \"Harlow v. Fitzgerald, 457 U.S. 800 (1982)\",\n",
    "])\n",
    "\n",
    "\n",
    "# -- Citation extraction --\n",
    "def extract_citations(text):\n",
    "    \"\"\"Extract legal case citations from text using regex.\"\"\"\n",
    "    pattern = (\n",
    "        r\"([A-Z][A-Za-z.'\\-\\s]+\"\n",
    "        r\"v\\.\"\n",
    "        r\"\\s+[A-Z][A-Za-z.'\\-\\s,]+\"\n",
    "        r\"\\d+\\s+\"\n",
    "        r\"(?:U\\.S\\.|S\\.\\s*Ct\\.|F\\.\\d+[a-z]*|F\\.\\s*(?:Supp|App)[.'\\s]*\\d*[a-z]*)\"\n",
    "        r\"\\s+\\d+\"\n",
    "        r\"\\s*\\([^)]+\\))\"\n",
    "    )\n",
    "    matches = re.findall(pattern, text)\n",
    "    return [\" \".join(m.split()) for m in matches]\n",
    "\n",
    "\n",
    "def citation_accuracy(generated_text, corpus):\n",
    "    \"\"\"Compute citation accuracy: verified / total citations.\"\"\"\n",
    "    citations = extract_citations(generated_text)\n",
    "    if not citations:\n",
    "        return 1.0, {\"total\": 0, \"verified\": [], \"unverified\": []}\n",
    "\n",
    "    verified, unverified = [], []\n",
    "    for cite in citations:\n",
    "        name_match = re.match(\n",
    "            r\"([A-Z][A-Za-z.'\\-\\s]+v\\.\\s+[A-Z][A-Za-z.'\\-\\s]+?),?\\s*\\d\", cite\n",
    "        )\n",
    "        if name_match:\n",
    "            case_name = name_match.group(1).strip().rstrip(\",\")\n",
    "            if any(case_name in known for known in corpus):\n",
    "                verified.append(cite)\n",
    "            else:\n",
    "                unverified.append(cite)\n",
    "        else:\n",
    "            unverified.append(cite)\n",
    "\n",
    "    return len(verified) / len(citations), {\n",
    "        \"total\": len(citations), \"verified\": verified, \"unverified\": unverified,\n",
    "    }\n",
    "\n",
    "\n",
    "# -- Hallucination detection --\n",
    "def extract_claims(text):\n",
    "    \"\"\"Extract factual claims from text (sentences with entities/numbers/legal terms).\"\"\"\n",
    "    sentences = re.split(r\"(?<=[.!?])\\s+\", text.strip())\n",
    "    claims = []\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        if not sent:\n",
    "            continue\n",
    "        has_indicator = (\n",
    "            bool(re.search(r\"[A-Z][a-z]+\\s+[A-Z]\", sent))\n",
    "            or bool(re.search(r\"\\d+\", sent))\n",
    "            or bool(re.search(\n",
    "                r\"\\b(held|ruled|found|granted|denied|reversed|affirmed|court|statute|amendment)\\b\",\n",
    "                sent, re.IGNORECASE,\n",
    "            ))\n",
    "        )\n",
    "        if has_indicator:\n",
    "            claims.append(sent)\n",
    "    return claims\n",
    "\n",
    "\n",
    "def hallucination_rate(generated_text, source_context, threshold=0.5):\n",
    "    \"\"\"Compute hallucination rate: ungrounded claims / total claims.\"\"\"\n",
    "    stopwords = {\n",
    "        \"the\", \"a\", \"an\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\",\n",
    "        \"being\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"will\",\n",
    "        \"would\", \"could\", \"should\", \"may\", \"might\", \"shall\", \"can\",\n",
    "        \"to\", \"of\", \"in\", \"for\", \"on\", \"with\", \"at\", \"by\", \"from\",\n",
    "        \"as\", \"into\", \"through\", \"during\", \"before\", \"after\",\n",
    "        \"and\", \"but\", \"or\", \"nor\", \"not\", \"so\", \"yet\",\n",
    "        \"than\", \"too\", \"very\", \"just\", \"because\", \"if\", \"when\",\n",
    "        \"where\", \"how\", \"what\", \"which\", \"who\", \"whom\", \"this\",\n",
    "        \"that\", \"these\", \"those\", \"it\", \"its\",\n",
    "    }\n",
    "\n",
    "    def content_words(text):\n",
    "        words = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "        return [w for w in words if w not in stopwords and len(w) > 2]\n",
    "\n",
    "    claims = extract_claims(generated_text)\n",
    "    if not claims:\n",
    "        return 0.0, {\"total\": 0, \"grounded\": [], \"ungrounded\": []}\n",
    "\n",
    "    source_words = set(content_words(source_context))\n",
    "    grounded, ungrounded = [], []\n",
    "\n",
    "    for claim in claims:\n",
    "        claim_words = content_words(claim)\n",
    "        if not claim_words:\n",
    "            grounded.append(claim)\n",
    "            continue\n",
    "        overlap = sum(1 for w in claim_words if w in source_words) / len(claim_words)\n",
    "        if overlap >= threshold:\n",
    "            grounded.append(claim)\n",
    "        else:\n",
    "            ungrounded.append(claim)\n",
    "\n",
    "    return len(ungrounded) / len(claims), {\n",
    "        \"total\": len(claims), \"grounded\": grounded, \"ungrounded\": ungrounded,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Metrics loaded: citation_accuracy, hallucination_rate, ROUGE\")\n",
    "print(f\"Citation corpus: {len(citation_corpus)} known cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simulated-outputs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated model outputs for each test case.\n",
    "# In production, these would come from model.generate().\n",
    "# We simulate a mix of good and bad outputs to demonstrate the harness.\n",
    "\n",
    "simulated_outputs = {\n",
    "    \"TC-001\": (\n",
    "        \"A grant of summary judgment is reviewed de novo on appeal. The \"\n",
    "        \"appellate court construes all facts and draws all reasonable \"\n",
    "        \"inferences in favor of the nonmoving party. Anderson v. Liberty \"\n",
    "        \"Lobby, Inc., 477 U.S. 242 (1986).\"\n",
    "    ),\n",
    "    \"TC-002\": (\n",
    "        \"Under the ADA, the plaintiff must show three elements: (1) disability \"\n",
    "        \"within the meaning of the Act, (2) qualification to perform essential \"\n",
    "        \"job functions with or without reasonable accommodation, and (3) adverse \"\n",
    "        \"employment action because of the disability. Hoffman v. Caterpillar, \"\n",
    "        \"Inc., 256 F.3d 568 (7th Cir. 2001).\"\n",
    "    ),\n",
    "    \"TC-003\": (\n",
    "        \"A preliminary injunction requires showing: (1) likelihood of success, \"\n",
    "        \"(2) irreparable harm, (3) balance of equities, and (4) public interest. \"\n",
    "        \"Winter v. Natural Resources Defense Council, Inc., 555 U.S. 7 (2008).\"\n",
    "    ),\n",
    "    \"TC-004\": (\n",
    "        \"Yes, courts have recognized groundwater contamination as irreparable \"\n",
    "        \"harm. Amoco Production Co. v. Village of Gambell, 480 U.S. 531 (1987). \"\n",
    "        \"The potential for drinking water contamination is sufficient.\"\n",
    "    ),\n",
    "    # TC-005: Good answer but fabricated citation\n",
    "    \"TC-005\": (\n",
    "        \"SEC sanctions are reviewed under a deferential standard. Findings of \"\n",
    "        \"fact must be supported by substantial evidence. The Commission has \"\n",
    "        \"broad discretion in selecting sanctions. Morrison v. Securities \"\n",
    "        \"Regulatory Board, 588 U.S. 201 (2015).\"\n",
    "    ),\n",
    "    \"TC-006\": (\n",
    "        \"Investment advisers owe fiduciary duties to clients. Allocating IPO \"\n",
    "        \"shares to proprietary accounts before satisfying client orders violates \"\n",
    "        \"these duties. SEC v. Capital Gains Research Bureau, Inc., 375 U.S. 180 \"\n",
    "        \"(1963).\"\n",
    "    ),\n",
    "    # TC-007: Partially correct, missing one citation\n",
    "    \"TC-007\": (\n",
    "        \"IDEA eligibility decisions are reviewed under a modified de novo standard. \"\n",
    "        \"The court gives due weight to administrative determinations while \"\n",
    "        \"independently reviewing the evidence. Board of Education v. Rowley, \"\n",
    "        \"458 U.S. 176 (1982).\"\n",
    "    ),\n",
    "    \"TC-008\": (\n",
    "        \"A patent infringement complaint must allege facts plausibly establishing \"\n",
    "        \"ownership of a valid patent and infringement. Detailed claim-by-claim \"\n",
    "        \"analysis is not required at the pleading stage. Ashcroft v. Iqbal, \"\n",
    "        \"556 U.S. 662 (2009).\"\n",
    "    ),\n",
    "    # TC-009: Hallucinated facts mixed with correct info\n",
    "    \"TC-009\": (\n",
    "        \"Summary judgment requires no genuine dispute of material fact. The \"\n",
    "        \"Supreme Court's 1986 trilogy established the modern standard. The \"\n",
    "        \"movant must file within 30 days of discovery closing. Celotex Corp. \"\n",
    "        \"v. Catrett, 477 U.S. 317 (1986). Courts grant summary judgment in \"\n",
    "        \"approximately 40% of federal cases.\"\n",
    "    ),\n",
    "    \"TC-010\": (\n",
    "        \"The Fourth Amendment protects against unreasonable searches and seizures. \"\n",
    "        \"Katz v. United States, 389 U.S. 347 (1967) established the reasonable \"\n",
    "        \"expectation of privacy test.\"\n",
    "    ),\n",
    "    # TC-011: Overconfident and partially wrong\n",
    "    \"TC-011\": (\n",
    "        \"The McDonnell Douglas framework always results in employer liability if \"\n",
    "        \"the plaintiff shows any evidence of discrimination. The employer can never \"\n",
    "        \"overcome the presumption once established. McDonnell Douglas Corp. v. \"\n",
    "        \"Green, 411 U.S. 792 (1973).\"\n",
    "    ),\n",
    "    \"TC-012\": (\n",
    "        \"Under Iqbal and Twombly, a complaint must contain factual content \"\n",
    "        \"allowing the court to draw a reasonable inference of liability. \"\n",
    "        \"Conclusory statements do not suffice. Ashcroft v. Iqbal, 556 U.S. 662 \"\n",
    "        \"(2009). Bell Atlantic Corp. v. Twombly, 550 U.S. 544 (2007).\"\n",
    "    ),\n",
    "    \"TC-013\": (\n",
    "        \"Under the discovery rule, the statute of limitations begins when the \"\n",
    "        \"plaintiff knew or should have known of the injury. This tolls the \"\n",
    "        \"limitations period for latent injuries not immediately apparent.\"\n",
    "    ),\n",
    "    # TC-014: All fabricated citations\n",
    "    \"TC-014\": (\n",
    "        \"Qualified immunity protects officials from liability unless they \"\n",
    "        \"violated clearly established rights. Thompson v. Federal Bureau of \"\n",
    "        \"Investigations, 534 U.S. 289 (2003). The standard requires a \"\n",
    "        \"reasonable person to have known the conduct was unlawful. Richardson \"\n",
    "        \"v. State Law Enforcement Agency, 521 U.S. 445 (1998).\"\n",
    "    ),\n",
    "    \"TC-015\": (\n",
    "        \"The Clean Water Act prohibits discharge of pollutants into waters \"\n",
    "        \"of the United States without NPDES permits. Violations can result \"\n",
    "        \"in injunctive relief and civil penalties.\"\n",
    "    ),\n",
    "    \"TC-016\": (\n",
    "        \"Under IDEA regulations, school districts must consider independent \"\n",
    "        \"evaluations submitted by parents. Failure to do so is a procedural \"\n",
    "        \"violation. Board of Education v. Rowley, 458 U.S. 176 (1982).\"\n",
    "    ),\n",
    "    # TC-017: Good content, one real and one fabricated citation\n",
    "    \"TC-017\": (\n",
    "        \"The SEC can revoke registration even without actual client losses. \"\n",
    "        \"Systematic conflicts of interest and misrepresentations justify \"\n",
    "        \"severe sanctions. Steadman v. SEC, 603 F.2d 1126 (5th Cir. 1979). \"\n",
    "        \"See also Porter v. Investment Advisory Commission, 478 U.S. 331 (1988).\"\n",
    "    ),\n",
    "    \"TC-018\": (\n",
    "        \"Essential functions are fundamental job duties. Courts examine whether \"\n",
    "        \"other employees perform the function and the employer's judgment. A \"\n",
    "        \"disputed characterization can create genuine issues of material fact \"\n",
    "        \"defeating summary judgment.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "print(f\"Simulated outputs for {len(simulated_outputs)} test cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-scoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score every test case on all dimensions\n",
    "\n",
    "results = []\n",
    "\n",
    "for tc in test_cases:\n",
    "    tc_id = tc[\"id\"]\n",
    "    output = simulated_outputs[tc_id]\n",
    "\n",
    "    # 1. Citation accuracy\n",
    "    cite_score, cite_details = citation_accuracy(output, citation_corpus)\n",
    "\n",
    "    # 2. ROUGE-L against ground truth\n",
    "    rouge_result = rouge_metric.compute(\n",
    "        predictions=[output],\n",
    "        references=[tc[\"ground_truth\"]],\n",
    "    )\n",
    "    rouge_l = rouge_result[\"rougeL\"]\n",
    "\n",
    "    # 3. Hallucination rate against context\n",
    "    hall_rate, hall_details = hallucination_rate(output, tc[\"context\"])\n",
    "\n",
    "    # 4. Overall score (weighted average)\n",
    "    # Citation accuracy and grounding weighted higher for legal domain\n",
    "    overall = (\n",
    "        0.35 * cite_score\n",
    "        + 0.25 * rouge_l\n",
    "        + 0.40 * (1.0 - hall_rate)  # invert: lower hallucination = better\n",
    "    )\n",
    "\n",
    "    results.append({\n",
    "        \"id\": tc_id,\n",
    "        \"question\": tc[\"question\"][:50] + \"...\",\n",
    "        \"citation_acc\": cite_score,\n",
    "        \"rouge_l\": rouge_l,\n",
    "        \"hallucination\": hall_rate,\n",
    "        \"overall\": overall,\n",
    "        \"n_citations\": cite_details[\"total\"],\n",
    "        \"n_unverified\": len(cite_details[\"unverified\"]),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "print(\"Assessment Results:\")\n",
    "print(\"=\" * 95)\n",
    "print(\n",
    "    df[[\"id\", \"citation_acc\", \"rouge_l\", \"hallucination\", \"overall\"]]\n",
    "    .to_string(index=False, float_format=\"{:.3f}\".format)\n",
    ")\n",
    "print(\"=\" * 95)\n",
    "print()\n",
    "print(\"Summary statistics:\")\n",
    "for col in [\"citation_acc\", \"rouge_l\", \"hallucination\", \"overall\"]:\n",
    "    print(f\"  {col:>15}: mean={df[col].mean():.3f}, min={df[col].min():.3f}, max={df[col].max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flag-failures",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag test cases that fail quality thresholds\n",
    "\n",
    "THRESHOLDS = {\n",
    "    \"citation_acc\": 0.8,   # At least 80% of citations must be verified\n",
    "    \"hallucination\": 0.3,  # At most 30% hallucination rate\n",
    "    \"overall\": 0.6,        # At least 0.6 overall score\n",
    "}\n",
    "\n",
    "print(\"Flagged test cases (below quality thresholds):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "flagged = []\n",
    "for _, row in df.iterrows():\n",
    "    reasons = []\n",
    "    if row[\"citation_acc\"] < THRESHOLDS[\"citation_acc\"] and row[\"n_citations\"] > 0:\n",
    "        reasons.append(f\"citation_acc={row['citation_acc']:.2f} < {THRESHOLDS['citation_acc']}\")\n",
    "    if row[\"hallucination\"] > THRESHOLDS[\"hallucination\"]:\n",
    "        reasons.append(f\"hallucination={row['hallucination']:.2f} > {THRESHOLDS['hallucination']}\")\n",
    "    if row[\"overall\"] < THRESHOLDS[\"overall\"]:\n",
    "        reasons.append(f\"overall={row['overall']:.2f} < {THRESHOLDS['overall']}\")\n",
    "\n",
    "    if reasons:\n",
    "        flagged.append(row[\"id\"])\n",
    "        print(f\"\\n  {row['id']}: {row['question']}\")\n",
    "        for reason in reasons:\n",
    "            print(f\"    - {reason}\")\n",
    "\n",
    "print(f\"\\n{len(flagged)}/{len(df)} test cases flagged\")\n",
    "print(f\"Pass rate: {(len(df) - len(flagged)) / len(df):.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "llm-judge-heading",
   "metadata": {},
   "source": [
    "## LLM-as-Judge\n",
    "\n",
    "Automated metrics like ROUGE and citation accuracy capture specific\n",
    "dimensions of quality. But some aspects -- helpfulness, coherence,\n",
    "tone -- are hard to measure with rules or word overlap.\n",
    "\n",
    "**LLM-as-judge** uses a larger, more capable model to score a smaller\n",
    "model's outputs. The judge model receives the question, the response,\n",
    "and a rubric, then produces structured ratings.\n",
    "\n",
    "Advantages:\n",
    "- Captures subjective quality dimensions.\n",
    "- Scales better than human annotation.\n",
    "- Can be customized with domain-specific rubrics.\n",
    "\n",
    "Limitations:\n",
    "- Judge bias (models prefer their own style).\n",
    "- Cost (requires API calls for each judgment).\n",
    "- Not a substitute for ground-truth metrics like citation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judge-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judge prompt template for legal response quality\n",
    "\n",
    "JUDGE_PROMPT = \"\"\"Rate the following legal response on:\n",
    "1. Accuracy (1-5): Are the legal citations and facts correct?\n",
    "2. Helpfulness (1-5): Does it answer the question?\n",
    "3. Citation quality (1-5): Are sources properly cited?\n",
    "\n",
    "Question: {question}\n",
    "Response: {response}\n",
    "\n",
    "Provide ratings as JSON: {{\"accuracy\": N, \"helpfulness\": N, \"citation_quality\": N}}\"\"\"\n",
    "\n",
    "\n",
    "def format_judge_prompt(question, response):\n",
    "    \"\"\"Format the judge prompt with a specific question and response.\"\"\"\n",
    "    return JUDGE_PROMPT.format(question=question, response=response)\n",
    "\n",
    "\n",
    "def call_llm_judge(question, response, api_key=None):\n",
    "    \"\"\"Call an LLM API to judge a response.\n",
    "\n",
    "    Requires an API key for the LLM provider (e.g., OpenAI, Anthropic).\n",
    "    Returns parsed JSON ratings.\n",
    "\n",
    "    NOTE: This function requires an API key. For offline testing,\n",
    "    use mock_judge() below.\n",
    "    \"\"\"\n",
    "    prompt = format_judge_prompt(question, response)\n",
    "\n",
    "    if api_key is None:\n",
    "        print(\"No API key provided. Use mock_judge() for offline testing.\")\n",
    "        return None\n",
    "\n",
    "    # Example with OpenAI (uncomment and configure as needed):\n",
    "    # import openai\n",
    "    # client = openai.OpenAI(api_key=api_key)\n",
    "    # completion = client.chat.completions.create(\n",
    "    #     model=\"gpt-4\",\n",
    "    #     messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    #     temperature=0.0,\n",
    "    # )\n",
    "    # return json.loads(completion.choices[0].message.content)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def mock_judge(question, response, citation_corpus=citation_corpus):\n",
    "    \"\"\"A deterministic mock judge for offline testing.\n",
    "\n",
    "    Scores based on heuristics:\n",
    "    - Accuracy: based on citation verification.\n",
    "    - Helpfulness: based on response length and question-word overlap.\n",
    "    - Citation quality: based on number of properly formatted citations.\n",
    "    \"\"\"\n",
    "    # Accuracy: citation-based\n",
    "    cite_score, cite_details = citation_accuracy(response, citation_corpus)\n",
    "    if cite_details[\"total\"] == 0:\n",
    "        accuracy = 3  # neutral if no citations\n",
    "    else:\n",
    "        accuracy = max(1, min(5, round(cite_score * 5)))\n",
    "\n",
    "    # Helpfulness: does the response address the question?\n",
    "    q_words = set(re.findall(r\"\\b\\w+\\b\", question.lower()))\n",
    "    r_words = set(re.findall(r\"\\b\\w+\\b\", response.lower()))\n",
    "    overlap = len(q_words & r_words) / max(len(q_words), 1)\n",
    "    length_factor = min(1.0, len(response) / 200)  # reward adequate length\n",
    "    helpfulness = max(1, min(5, round((overlap * 0.5 + length_factor * 0.5) * 5)))\n",
    "\n",
    "    # Citation quality: format and count\n",
    "    citations = extract_citations(response)\n",
    "    if len(citations) == 0:\n",
    "        citation_quality = 2\n",
    "    elif len(citations) >= 2:\n",
    "        citation_quality = min(5, 3 + len(citations))\n",
    "    else:\n",
    "        citation_quality = 3\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"helpfulness\": helpfulness,\n",
    "        \"citation_quality\": citation_quality,\n",
    "    }\n",
    "\n",
    "\n",
    "# Demonstrate the judge on a few test cases\n",
    "print(\"LLM-as-Judge (mock) results:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'ID':>7}  {'Accuracy':>8}  {'Helpful':>8}  {'Citations':>9}  {'Avg':>6}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "judge_results = []\n",
    "for tc in test_cases:\n",
    "    output = simulated_outputs[tc[\"id\"]]\n",
    "    scores = mock_judge(tc[\"question\"], output)\n",
    "    avg = np.mean(list(scores.values()))\n",
    "    judge_results.append({\"id\": tc[\"id\"], **scores, \"avg\": avg})\n",
    "    print(\n",
    "        f\"{tc['id']:>7}  {scores['accuracy']:>8}  \"\n",
    "        f\"{scores['helpfulness']:>8}  {scores['citation_quality']:>9}  {avg:>6.1f}\"\n",
    "    )\n",
    "\n",
    "print()\n",
    "print(\"Note: The mock judge uses heuristics. A real LLM judge would\")\n",
    "print(\"provide more nuanced assessments of response quality.\")\n",
    "print()\n",
    "\n",
    "# Show what the prompt looks like for one example\n",
    "example_prompt = format_judge_prompt(\n",
    "    test_cases[0][\"question\"], simulated_outputs[\"TC-001\"]\n",
    ")\n",
    "print(\"Example judge prompt:\")\n",
    "print(\"-\" * 70)\n",
    "print(example_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-gaming-heading",
   "metadata": {},
   "source": [
    "## Benchmark Gaming\n",
    "\n",
    "**Goodhart's Law**: \"When a measure becomes a target, it ceases to be\n",
    "a good measure.\"\n",
    "\n",
    "This applies directly to LLM benchmarks. Common gaming strategies:\n",
    "\n",
    "### 1. Training on Benchmark Data\n",
    "If a model is trained (or fine-tuned) on the exact questions from a\n",
    "benchmark, it memorizes the answers rather than learning the skill.\n",
    "Scores go up, but real-world performance does not.\n",
    "\n",
    "### 2. Prompt Engineering for Benchmarks\n",
    "Models can be optimized for the specific format of benchmark questions\n",
    "(e.g., multiple choice with options A-D) without improving general\n",
    "capability. A model that scores well on MMLU-style questions may\n",
    "still fail when asked the same question in free-form.\n",
    "\n",
    "### 3. Leaderboard Contamination\n",
    "Public benchmarks inevitably end up in training data. As web-scraped\n",
    "datasets grow, the chance that benchmark questions appeared in training\n",
    "increases. This inflates scores across the board.\n",
    "\n",
    "### Why This Matters for Product Teams\n",
    "\n",
    "For a legal AI product like CoCounsel, the implications are:\n",
    "\n",
    "- **Internal test sets must stay private.** If your assessment questions\n",
    "  are public, models will eventually train on them.\n",
    "- **Rotate test sets periodically.** Even private sets lose value if\n",
    "  the same questions are used for too many iterations.\n",
    "- **Use held-out test cases.** Never optimize directly against your\n",
    "  assessment set -- use a separate validation set for model selection.\n",
    "- **Combine automated and human review.** No single metric captures\n",
    "  everything. Human lawyers reviewing model outputs remain essential.\n",
    "- **Measure what matters in production.** Citation accuracy on real\n",
    "  user queries is more informative than any benchmark score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-heading",
   "metadata": {},
   "source": [
    "## Results Visualization\n",
    "\n",
    "Good assessment reporting makes results actionable. We create:\n",
    "1. A radar chart showing average scores across dimensions.\n",
    "2. A per-question breakdown.\n",
    "3. A comparison table summarizing pass/fail status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radar-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart of average metric scores\n",
    "\n",
    "categories = [\"Citation\\nAccuracy\", \"ROUGE-L\", \"Grounding\\n(1 - Halluc.)\", \"Overall\"]\n",
    "values = [\n",
    "    df[\"citation_acc\"].mean(),\n",
    "    df[\"rouge_l\"].mean(),\n",
    "    1.0 - df[\"hallucination\"].mean(),\n",
    "    df[\"overall\"].mean(),\n",
    "]\n",
    "\n",
    "# Close the radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "values_closed = values + [values[0]]\n",
    "angles_closed = angles + [angles[0]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "ax.fill(angles_closed, values_closed, color=\"steelblue\", alpha=0.25)\n",
    "ax.plot(angles_closed, values_closed, color=\"steelblue\", linewidth=2, marker=\"o\", markersize=8)\n",
    "\n",
    "ax.set_xticks(angles)\n",
    "ax.set_xticklabels(categories, size=12)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels([\"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"], size=9, color=\"gray\")\n",
    "ax.set_title(\"Model Quality Radar Chart\", size=14, fontweight=\"bold\", pad=20)\n",
    "\n",
    "# Annotate values\n",
    "for angle, value, label in zip(angles, values, categories):\n",
    "    ax.annotate(\n",
    "        f\"{value:.2f}\",\n",
    "        xy=(angle, value),\n",
    "        xytext=(5, 10),\n",
    "        textcoords=\"offset points\",\n",
    "        fontsize=10,\n",
    "        fontweight=\"bold\",\n",
    "        color=\"darkblue\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The radar chart shows average scores across all test cases.\")\n",
    "print(\"Weak dimensions indicate areas for model improvement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "per-question-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-question breakdown: horizontal bar chart\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 8))\n",
    "\n",
    "metrics_to_plot = [\n",
    "    (\"citation_acc\", \"Citation Accuracy\", \"steelblue\"),\n",
    "    (\"rouge_l\", \"ROUGE-L\", \"#2ecc71\"),\n",
    "    (\"hallucination\", \"Hallucination Rate\", \"#e74c3c\"),\n",
    "]\n",
    "\n",
    "for ax, (col, title, color) in zip(axes, metrics_to_plot):\n",
    "    y_pos = range(len(df))\n",
    "    ax.barh(y_pos, df[col], color=color, alpha=0.7)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(df[\"id\"], fontsize=9)\n",
    "    ax.set_xlabel(title)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Add threshold line\n",
    "    if col == \"hallucination\":\n",
    "        ax.axvline(x=0.3, color=\"black\", linestyle=\"--\", alpha=0.5, label=\"Threshold\")\n",
    "    elif col == \"citation_acc\":\n",
    "        ax.axvline(x=0.8, color=\"black\", linestyle=\"--\", alpha=0.5, label=\"Threshold\")\n",
    "\n",
    "    ax.legend(loc=\"lower right\", fontsize=9)\n",
    "    ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Per-Question Score Breakdown\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each row is one test case. Dashed lines show quality thresholds.\")\n",
    "print(\"For hallucination rate, LOWER is better (below the threshold line).\")\n",
    "print(\"For citation accuracy, HIGHER is better (above the threshold line).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table combining automated metrics and judge scores\n",
    "\n",
    "judge_df = pd.DataFrame(judge_results)\n",
    "combined = df.merge(judge_df[[\"id\", \"accuracy\", \"helpfulness\", \"citation_quality\"]], on=\"id\")\n",
    "\n",
    "# Add pass/fail column\n",
    "def assess_status(row):\n",
    "    if row[\"citation_acc\"] < 0.8 and row[\"n_citations\"] > 0:\n",
    "        return \"FAIL\"\n",
    "    if row[\"hallucination\"] > 0.3:\n",
    "        return \"WARN\"\n",
    "    if row[\"overall\"] < 0.6:\n",
    "        return \"WARN\"\n",
    "    return \"PASS\"\n",
    "\n",
    "combined[\"status\"] = combined.apply(assess_status, axis=1)\n",
    "\n",
    "print(\"Combined Results: Automated Metrics + Judge Scores\")\n",
    "print(\"=\" * 100)\n",
    "display_cols = [\n",
    "    \"id\", \"citation_acc\", \"rouge_l\", \"hallucination\",\n",
    "    \"accuracy\", \"helpfulness\", \"citation_quality\", \"status\",\n",
    "]\n",
    "print(\n",
    "    combined[display_cols].to_string(\n",
    "        index=False,\n",
    "        float_format=\"{:.2f}\".format,\n",
    "    )\n",
    ")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "status_counts = combined[\"status\"].value_counts()\n",
    "print(f\"\\nStatus breakdown:\")\n",
    "for status in [\"PASS\", \"WARN\", \"FAIL\"]:\n",
    "    count = status_counts.get(status, 0)\n",
    "    print(f\"  {status}: {count}/{len(combined)}\")\n",
    "\n",
    "print(f\"\\nOverall pass rate: {status_counts.get('PASS', 0) / len(combined):.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises-heading",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise (a): Add a Conciseness Dimension\n",
    "\n",
    "Add a new scoring dimension: \"conciseness\". A good legal response\n",
    "is thorough but not unnecessarily verbose.\n",
    "\n",
    "1. Modify the judge prompt to include a conciseness rating (1-5).\n",
    "2. Implement a heuristic conciseness metric: penalize responses that\n",
    "   are more than 2x the length of the ground truth.\n",
    "3. Add it to the scoring pipeline and results table.\n",
    "\n",
    "```python\n",
    "JUDGE_PROMPT_V2 = \"\"\"Rate the following legal response on:\n",
    "1. Accuracy (1-5): Are the legal citations and facts correct?\n",
    "2. Helpfulness (1-5): Does it answer the question?\n",
    "3. Citation quality (1-5): Are sources properly cited?\n",
    "4. Conciseness (1-5): Is the response appropriately concise?\n",
    "\n",
    "Question: {question}\n",
    "Response: {response}\n",
    "\n",
    "Provide ratings as JSON:\n",
    "{\"accuracy\": N, \"helpfulness\": N, \"citation_quality\": N, \"conciseness\": N}\"\"\"\n",
    "\n",
    "def conciseness_score(response, ground_truth):\n",
    "    \"\"\"Score conciseness: penalize verbosity beyond 2x ground truth length.\"\"\"\n",
    "    ratio = len(response.split()) / max(len(ground_truth.split()), 1)\n",
    "    if ratio <= 1.5:\n",
    "        return 1.0\n",
    "    elif ratio <= 2.0:\n",
    "        return 0.8\n",
    "    elif ratio <= 3.0:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 0.2\n",
    "```\n",
    "\n",
    "### Exercise (b): Preventing Benchmark Contamination\n",
    "\n",
    "Discuss: If the test cases in this notebook were made public (e.g.,\n",
    "published in a paper or open-source repository), how would you\n",
    "prevent benchmark contamination? Consider:\n",
    "\n",
    "1. **Detection**: How would you check if a model has seen these\n",
    "   test cases during training? (Hint: canary strings, perplexity\n",
    "   analysis on exact test case text.)\n",
    "\n",
    "2. **Prevention**: What operational practices would you implement?\n",
    "   (Hint: private test sets, periodic rotation, held-out splits.)\n",
    "\n",
    "3. **Mitigation**: If contamination is suspected, how would you\n",
    "   adjust scores? (Hint: compare performance on contaminated vs.\n",
    "   fresh test cases, decontamination benchmarks.)\n",
    "\n",
    "4. **Design**: How would you design test cases that are harder to\n",
    "   contaminate? (Hint: procedurally generated, parameterized\n",
    "   questions, adversarial variations.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
