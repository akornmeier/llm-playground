{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "context",
   "metadata": {},
   "source": [
    "# 01 - Metrics for LLM Assessment\n",
    "\n",
    "## Context\n",
    "\n",
    "Assessment is how you know if your model actually works. Without\n",
    "rigorous measurement, you are guessing -- and in legal AI, guessing\n",
    "has consequences.\n",
    "\n",
    "**CoCounsel context:** For legal AI, \"works\" means factually accurate,\n",
    "properly cited, and appropriately uncertain. A model that generates\n",
    "fluent, well-structured legal text is useless if it fabricates\n",
    "citations, misstates holdings, or expresses false confidence.\n",
    "Standard NLP metrics measure surface-level text overlap -- they\n",
    "tell you whether the generated text *looks like* the reference,\n",
    "not whether it is *correct*.\n",
    "\n",
    "This notebook covers:\n",
    "1. **Perplexity** -- how well the model predicts the next token.\n",
    "2. **BLEU and ROUGE** -- standard overlap-based metrics.\n",
    "3. **Where metrics break down** -- concrete examples of misleading scores.\n",
    "4. **Citation accuracy** -- a legal-specific metric.\n",
    "5. **Hallucination detection** -- measuring ungrounded claims."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perplexity-heading",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "\n",
    "Perplexity measures how well a language model predicts the next token\n",
    "in a sequence. It is the exponential of the average cross-entropy loss:\n",
    "\n",
    "$$\\text{PPL} = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log p(x_i \\mid x_{<i})\\right) = \\exp(\\text{loss})$$\n",
    "\n",
    "**Lower perplexity = better language modeling.** A perplexity of 1\n",
    "means the model perfectly predicts every token. A perplexity of 10\n",
    "means the model is, on average, as uncertain as choosing uniformly\n",
    "among 10 equally likely tokens.\n",
    "\n",
    "Perplexity is useful for:\n",
    "- Comparing model quality on held-out data (domain adaptation).\n",
    "- Detecting distribution shift (high perplexity = unfamiliar text).\n",
    "- Sanity-checking training (perplexity should decrease over training).\n",
    "\n",
    "Perplexity is **not** useful for:\n",
    "- Measuring factual accuracy.\n",
    "- Comparing models of different sizes (larger models always win).\n",
    "- Judging generation quality (a model can be fluent but wrong)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model-and-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small model for perplexity computation\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Load held-out legal text from court opinions\n",
    "data_path = Path(\"../../datasets/sample/court_opinions.jsonl\")\n",
    "opinions = []\n",
    "with open(data_path) as f:\n",
    "    for line in f:\n",
    "        opinions.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(opinions)} court opinions\")\n",
    "print(f\"First opinion: {opinions[0]['case_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-perplexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(text, model, tokenizer, max_length=512):\n",
    "    \"\"\"Compute perplexity of a model on a text string.\n",
    "\n",
    "    Perplexity = exp(average cross-entropy loss).\n",
    "\n",
    "    Args:\n",
    "        text: The input text to measure.\n",
    "        model: A HuggingFace causal language model.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        max_length: Maximum number of tokens.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (perplexity, loss).\n",
    "    \"\"\"\n",
    "    encodings = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    input_ids = encodings[\"input_ids\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss.item()\n",
    "\n",
    "    perplexity = math.exp(loss)\n",
    "    return perplexity, loss\n",
    "\n",
    "\n",
    "# Compute perplexity on each court opinion\n",
    "print(\"Perplexity on held-out legal text:\")\n",
    "print(\"=\" * 65)\n",
    "print(f\"{'Case':>45}  {'PPL':>8}  {'Loss':>6}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "perplexities = []\n",
    "for opinion in opinions:\n",
    "    ppl, loss = compute_perplexity(opinion[\"text\"], model, tokenizer)\n",
    "    perplexities.append(ppl)\n",
    "    name = opinion[\"case_name\"][:45]\n",
    "    print(f\"{name:>45}  {ppl:>8.1f}  {loss:>6.3f}\")\n",
    "\n",
    "print(\"-\" * 65)\n",
    "avg_ppl = np.mean(perplexities)\n",
    "avg_loss = np.mean([math.log(p) for p in perplexities])\n",
    "print(f\"{'Mean':>45}  {avg_ppl:>8.1f}  {avg_loss:>6.3f}\")\n",
    "print()\n",
    "print(\"Lower perplexity means the model finds the text more predictable.\")\n",
    "print(\"Legal text tends to have moderate perplexity: formal but domain-specific.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perplexity-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare perplexity across different text types\n",
    "comparison_texts = {\n",
    "    \"Legal (court opinion)\": opinions[0][\"text\"][:500],\n",
    "    \"Casual English\": (\n",
    "        \"Hey, so I was thinking about going to the store later today. \"\n",
    "        \"Do you want me to pick up anything? I heard they have a sale \"\n",
    "        \"on those chips you like. Also, did you see the game last night? \"\n",
    "        \"It was pretty wild. The team really came through in the fourth \"\n",
    "        \"quarter. Anyway, let me know if you need anything.\"\n",
    "    ),\n",
    "    \"Python code\": (\n",
    "        \"def fibonacci(n):\\n\"\n",
    "        \"    if n <= 1:\\n\"\n",
    "        \"        return n\\n\"\n",
    "        \"    a, b = 0, 1\\n\"\n",
    "        \"    for _ in range(2, n + 1):\\n\"\n",
    "        \"        a, b = b, a + b\\n\"\n",
    "        \"    return b\\n\\n\"\n",
    "        \"for i in range(10):\\n\"\n",
    "        \"    print(f'fib({i}) = {fibonacci(i)}')\"\n",
    "    ),\n",
    "    \"Random tokens\": \"glorp xyzzy fleem quux bazinga wibble fnord plugh\",\n",
    "}\n",
    "\n",
    "print(\"Perplexity across text types:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Text type':<25}  {'PPL':>8}  {'Loss':>6}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for label, text in comparison_texts.items():\n",
    "    ppl, loss = compute_perplexity(text, model, tokenizer)\n",
    "    print(f\"{label:<25}  {ppl:>8.1f}  {loss:>6.3f}\")\n",
    "\n",
    "print()\n",
    "print(\"Random tokens have very high perplexity -- the model has no idea\")\n",
    "print(\"what comes next. Domain-specific text (legal, code) will vary based\")\n",
    "print(\"on how much of that domain appeared in the training data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bleu-rouge-heading",
   "metadata": {},
   "source": [
    "## BLEU and ROUGE\n",
    "\n",
    "**BLEU** (Bilingual Understudy) measures the **precision**\n",
    "of n-gram overlap between generated text and a reference. Originally\n",
    "designed for machine translation, it asks: \"Of the n-grams in the\n",
    "generated text, how many also appear in the reference?\"\n",
    "\n",
    "**ROUGE** (Recall-Oriented Understudy for Gisting) measures\n",
    "the **recall** of n-gram overlap. Designed for summarization, it asks:\n",
    "\"Of the n-grams in the reference, how many also appear in the generated\n",
    "text?\"\n",
    "\n",
    "Key variants:\n",
    "- **ROUGE-1**: unigram overlap (individual words)\n",
    "- **ROUGE-2**: bigram overlap (word pairs)\n",
    "- **ROUGE-L**: longest common subsequence\n",
    "\n",
    "Both metrics produce scores between 0 and 1, where 1 means perfect\n",
    "overlap with the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bleu-rouge-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load BLEU and ROUGE metrics\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# Legal summarization examples: reference summaries and generated summaries\n",
    "summarization_examples = [\n",
    "    {\n",
    "        \"reference\": (\n",
    "            \"The Seventh Circuit reversed the grant of summary judgment \"\n",
    "            \"in an ADA employment discrimination case, finding genuine \"\n",
    "            \"issues of material fact regarding whether the plaintiff \"\n",
    "            \"could perform essential job functions with a modified \"\n",
    "            \"travel schedule as a reasonable accommodation.\"\n",
    "        ),\n",
    "        \"generated\": (\n",
    "            \"The appellate court reversed summary judgment in the ADA \"\n",
    "            \"discrimination case. The court found disputed facts about \"\n",
    "            \"whether the employee could perform essential functions with \"\n",
    "            \"a reasonable accommodation of a modified travel schedule.\"\n",
    "        ),\n",
    "        \"label\": \"Good summary (accurate, covers key facts)\",\n",
    "    },\n",
    "    {\n",
    "        \"reference\": (\n",
    "            \"The court granted a preliminary injunction against hydraulic \"\n",
    "            \"fracturing operations under the Clean Water Act, finding \"\n",
    "            \"likelihood of success on the merits and irreparable harm \"\n",
    "            \"from potential groundwater contamination.\"\n",
    "        ),\n",
    "        \"generated\": (\n",
    "            \"The court issued an injunction stopping fracking operations \"\n",
    "            \"because the company was discharging pollutants without \"\n",
    "            \"proper permits under the Clean Water Act, and contamination \"\n",
    "            \"of drinking water supplies constituted irreparable harm.\"\n",
    "        ),\n",
    "        \"label\": \"Good summary (accurate, different wording)\",\n",
    "    },\n",
    "    {\n",
    "        \"reference\": (\n",
    "            \"The DC Circuit denied review of SEC sanctions against an \"\n",
    "            \"investment adviser for failing to disclose conflicts of \"\n",
    "            \"interest in IPO share allocation and lacking adequate \"\n",
    "            \"compliance procedures.\"\n",
    "        ),\n",
    "        \"generated\": (\n",
    "            \"The appellate court upheld SEC penalties against Westbrook \"\n",
    "            \"Capital for prioritizing proprietary trading over client \"\n",
    "            \"interests and making misrepresentations in regulatory \"\n",
    "            \"filings, finding the sanctions proportionate.\"\n",
    "        ),\n",
    "        \"label\": \"Good summary (accurate, emphasizes different details)\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Compute BLEU and ROUGE for each example\n",
    "print(\"BLEU and ROUGE scores on legal summarization:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, example in enumerate(summarization_examples):\n",
    "    bleu_result = bleu_metric.compute(\n",
    "        predictions=[example[\"generated\"]],\n",
    "        references=[[example[\"reference\"]]],\n",
    "    )\n",
    "    rouge_result = rouge_metric.compute(\n",
    "        predictions=[example[\"generated\"]],\n",
    "        references=[example[\"reference\"]],\n",
    "    )\n",
    "\n",
    "    print(f\"\\nExample {i + 1}: {example['label']}\")\n",
    "    print(f\"  BLEU:    {bleu_result['bleu']:.4f}\")\n",
    "    print(f\"  ROUGE-1: {rouge_result['rouge1']:.4f}\")\n",
    "    print(f\"  ROUGE-2: {rouge_result['rouge2']:.4f}\")\n",
    "    print(f\"  ROUGE-L: {rouge_result['rougeL']:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"These scores reflect word overlap with the reference.\")\n",
    "print(\"Scores below 0.5 are common -- legal text uses varied phrasing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics-break-down-heading",
   "metadata": {},
   "source": [
    "## Where Metrics Break Down\n",
    "\n",
    "This is the critical section. Traditional metrics can be **actively\n",
    "misleading** for legal AI. A model that scores well on BLEU and ROUGE\n",
    "may still be dangerous to use.\n",
    "\n",
    "We demonstrate two failure modes:\n",
    "1. A summary with **fabricated citations** that scores high on ROUGE.\n",
    "2. A **factually wrong** answer with good word overlap that scores well on BLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failure-mode-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure mode 1: Fabricated citations score high on ROUGE\n",
    "#\n",
    "# The generated summary matches the reference structure and most of the\n",
    "# key terms, but cites a completely fabricated case. ROUGE does not know\n",
    "# whether citations are real -- it only counts word overlap.\n",
    "\n",
    "reference_summary = (\n",
    "    \"The Seventh Circuit reversed the grant of summary judgment, \"\n",
    "    \"holding that genuine issues of material fact exist regarding \"\n",
    "    \"whether the plaintiff could perform essential job functions with \"\n",
    "    \"reasonable accommodation. The court applied the de novo standard \"\n",
    "    \"of review and cited Anderson v. Liberty Lobby, Inc., 477 U.S. 242 \"\n",
    "    \"(1986) for the summary judgment framework.\"\n",
    ")\n",
    "\n",
    "# This summary is WRONG -- it cites a fabricated case -- but structurally\n",
    "# matches the reference very well.\n",
    "fabricated_citation_summary = (\n",
    "    \"The Seventh Circuit reversed the grant of summary judgment, \"\n",
    "    \"holding that genuine issues of material fact exist regarding \"\n",
    "    \"whether the plaintiff could perform essential job functions with \"\n",
    "    \"reasonable accommodation. The court applied the de novo standard \"\n",
    "    \"of review and cited Richardson v. National Labor Board, 485 U.S. 117 \"\n",
    "    \"(1988) for the summary judgment framework.\"\n",
    ")\n",
    "\n",
    "# This summary is CORRECT but uses very different wording.\n",
    "correct_but_different = (\n",
    "    \"An appeals court overturned the lower court's decision dismissing \"\n",
    "    \"an ADA disability discrimination claim. The employer failed to show \"\n",
    "    \"that modified travel requirements could not serve as an accommodation. \"\n",
    "    \"The panel reviewed the record independently, following the precedent \"\n",
    "    \"set in Anderson v. Liberty Lobby.\"\n",
    ")\n",
    "\n",
    "print(\"FAILURE MODE 1: Fabricated citations score high on ROUGE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for label, generated in [\n",
    "    (\"Fabricated citation (WRONG)\", fabricated_citation_summary),\n",
    "    (\"Correct but different wording\", correct_but_different),\n",
    "]:\n",
    "    rouge_result = rouge_metric.compute(\n",
    "        predictions=[generated],\n",
    "        references=[reference_summary],\n",
    "    )\n",
    "    print(f\"\\n  {label}:\")\n",
    "    print(f\"    ROUGE-1: {rouge_result['rouge1']:.4f}\")\n",
    "    print(f\"    ROUGE-2: {rouge_result['rouge2']:.4f}\")\n",
    "    print(f\"    ROUGE-L: {rouge_result['rougeL']:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"The fabricated-citation summary scores HIGHER because it copies the\")\n",
    "print(\"reference structure almost verbatim. The correct summary scores LOWER\")\n",
    "print(\"because it paraphrases. ROUGE rewards copying, not correctness.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failure-mode-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure mode 2: Factually wrong answer with high BLEU\n",
    "#\n",
    "# The generated answer reuses many of the same words and phrases as the\n",
    "# reference, but gets the critical legal conclusion wrong.\n",
    "\n",
    "reference_answer = (\n",
    "    \"The court granted the preliminary injunction because the state \"\n",
    "    \"demonstrated a likelihood of success on the merits of its Clean \"\n",
    "    \"Water Act claims and showed that potential contamination of drinking \"\n",
    "    \"water constitutes irreparable harm that outweighs economic burden \"\n",
    "    \"on the defendant.\"\n",
    ")\n",
    "\n",
    "# WRONG: gets the outcome backwards (denied vs granted)\n",
    "wrong_outcome = (\n",
    "    \"The court denied the preliminary injunction because the state \"\n",
    "    \"failed to demonstrate a likelihood of success on the merits of its \"\n",
    "    \"Clean Water Act claims. The court found that potential contamination \"\n",
    "    \"of drinking water does not constitute irreparable harm sufficient \"\n",
    "    \"to outweigh the economic burden on the defendant.\"\n",
    ")\n",
    "\n",
    "# CORRECT: right outcome, different words\n",
    "correct_different_words = (\n",
    "    \"The judge issued an emergency order halting fracking operations. \"\n",
    "    \"California proved it would likely win its pollution case and that \"\n",
    "    \"groundwater poisoning posed an irreversible threat exceeding any \"\n",
    "    \"financial harm to Pacific Coast Energy.\"\n",
    ")\n",
    "\n",
    "print(\"FAILURE MODE 2: Factually wrong answer with high BLEU\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for label, generated in [\n",
    "    (\"Wrong outcome (INCORRECT)\", wrong_outcome),\n",
    "    (\"Correct but different wording\", correct_different_words),\n",
    "]:\n",
    "    bleu_result = bleu_metric.compute(\n",
    "        predictions=[generated],\n",
    "        references=[[reference_answer]],\n",
    "    )\n",
    "    rouge_result = rouge_metric.compute(\n",
    "        predictions=[generated],\n",
    "        references=[reference_answer],\n",
    "    )\n",
    "    print(f\"\\n  {label}:\")\n",
    "    print(f\"    BLEU:    {bleu_result['bleu']:.4f}\")\n",
    "    print(f\"    ROUGE-1: {rouge_result['rouge1']:.4f}\")\n",
    "    print(f\"    ROUGE-L: {rouge_result['rougeL']:.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"The factually WRONG answer scores higher on both BLEU and ROUGE.\")\n",
    "print(\"It reuses the same sentence structure and vocabulary but reverses\")\n",
    "print(\"the legal outcome. Standard metrics cannot detect this -- they\")\n",
    "print(\"measure surface overlap, not semantic correctness.\")\n",
    "print()\n",
    "print(\"This is why legal AI needs domain-specific metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "citation-accuracy-heading",
   "metadata": {},
   "source": [
    "## Citation Accuracy\n",
    "\n",
    "Legal text makes verifiable factual claims in the form of case\n",
    "citations. Unlike general text, where \"correctness\" is hard to define\n",
    "programmatically, we can check whether a cited case actually exists.\n",
    "\n",
    "The metric:\n",
    "1. Extract all case citations from the generated text using regex.\n",
    "2. Check each citation against a known corpus of real cases.\n",
    "3. Compute: `citation_accuracy = verified / total`.\n",
    "\n",
    "A score of 1.0 means every citation is real. A score of 0.0 means\n",
    "every citation is fabricated. This is a **verifiable** metric -- it\n",
    "measures ground truth, not surface similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a citation corpus from our court opinions dataset.\n",
    "# In production, this would be a comprehensive legal database.\n",
    "\n",
    "citation_corpus = set()\n",
    "for opinion in opinions:\n",
    "    for cite in opinion.get(\"citations\", []):\n",
    "        citation_corpus.add(cite)\n",
    "\n",
    "# Add some well-known cases for a richer corpus\n",
    "additional_citations = [\n",
    "    \"Marbury v. Madison, 5 U.S. 137 (1803)\",\n",
    "    \"Brown v. Board of Education, 347 U.S. 483 (1954)\",\n",
    "    \"Miranda v. Arizona, 384 U.S. 436 (1966)\",\n",
    "    \"Roe v. Wade, 410 U.S. 113 (1973)\",\n",
    "    \"Chevron U.S.A., Inc. v. NRDC, 467 U.S. 837 (1984)\",\n",
    "    \"Celotex Corp. v. Catrett, 477 U.S. 317 (1986)\",\n",
    "    \"Daubert v. Merrell Dow Pharmaceuticals, 509 U.S. 579 (1993)\",\n",
    "    \"Katz v. United States, 389 U.S. 347 (1967)\",\n",
    "    \"Terry v. Ohio, 392 U.S. 1 (1968)\",\n",
    "    \"Gideon v. Wainwright, 372 U.S. 335 (1963)\",\n",
    "    \"Bell Atlantic Corp. v. Twombly, 550 U.S. 544 (2007)\",\n",
    "    \"Ashcroft v. Iqbal, 556 U.S. 662 (2009)\",\n",
    "    \"McDonnell Douglas Corp. v. Green, 411 U.S. 792 (1973)\",\n",
    "]\n",
    "citation_corpus.update(additional_citations)\n",
    "\n",
    "print(f\"Citation corpus: {len(citation_corpus)} known cases\")\n",
    "print()\n",
    "for cite in sorted(citation_corpus)[:10]:\n",
    "    print(f\"  {cite}\")\n",
    "print(f\"  ... and {len(citation_corpus) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "citation-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_citations(text):\n",
    "    \"\"\"Extract legal case citations from text using regex.\n",
    "\n",
    "    Matches standard US case citation formats:\n",
    "    - Case Name v. Case Name, VOL REPORTER PAGE (COURT YEAR)\n",
    "    - Case Name v. Case Name, VOL REPORTER PAGE (YEAR)\n",
    "\n",
    "    Supported reporters: U.S., S.Ct., F.2d, F.3d, F.4th,\n",
    "    F.Supp., F.Supp.2d, F.Supp.3d, F.App'x.\n",
    "\n",
    "    Args:\n",
    "        text: Input text to search for citations.\n",
    "\n",
    "    Returns:\n",
    "        List of extracted citation strings.\n",
    "    \"\"\"\n",
    "    # Pattern for standard case citations:\n",
    "    # Name v. Name, ### Reporter ### (Court/Year)\n",
    "    pattern = (\n",
    "        r\"([A-Z][A-Za-z.'\\-\\s]+\"\n",
    "        r\"v\\.\"\n",
    "        r\"\\s+[A-Z][A-Za-z.'\\-\\s,]+\"\n",
    "        r\"\\d+\\s+\"\n",
    "        r\"(?:U\\.S\\.|S\\.\\s*Ct\\.|F\\.\\d+[a-z]*|F\\.\\s*(?:Supp|App)[.'\\s]*\\d*[a-z]*)\"\n",
    "        r\"\\s+\\d+\"\n",
    "        r\"\\s*\\([^)]+\\))\"\n",
    "    )\n",
    "    matches = re.findall(pattern, text)\n",
    "    # Clean up whitespace in each match\n",
    "    cleaned = [\" \".join(m.split()) for m in matches]\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# Test extraction on court opinions\n",
    "print(\"Citation extraction tests:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for opinion in opinions[:3]:\n",
    "    extracted = extract_citations(opinion[\"text\"])\n",
    "    print(f\"\\n  {opinion['case_name']}\")\n",
    "    print(f\"  Known:     {opinion['citations']}\")\n",
    "    print(f\"  Extracted: {extracted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "citation-accuracy-metric",
   "metadata": {},
   "outputs": [],
   "source": [
    "def citation_accuracy(generated_text, corpus):\n",
    "    \"\"\"Compute citation accuracy: fraction of citations that are verified.\n",
    "\n",
    "    Extracts all case citations from the generated text, then checks each\n",
    "    against the known corpus using fuzzy matching on the case name.\n",
    "\n",
    "    Args:\n",
    "        generated_text: The model output text to check.\n",
    "        corpus: A set of known real citation strings.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (accuracy_score, details_dict).\n",
    "        accuracy_score is in [0, 1]. Returns 1.0 if no citations found.\n",
    "    \"\"\"\n",
    "    citations = extract_citations(generated_text)\n",
    "\n",
    "    if not citations:\n",
    "        return 1.0, {\"total\": 0, \"verified\": [], \"unverified\": []}\n",
    "\n",
    "    verified = []\n",
    "    unverified = []\n",
    "\n",
    "    for cite in citations:\n",
    "        # Extract the case name portion (everything before the volume number)\n",
    "        name_match = re.match(\n",
    "            r\"([A-Z][A-Za-z.'\\-\\s]+v\\.\\s+[A-Z][A-Za-z.'\\-\\s]+?),?\\s*\\d\",\n",
    "            cite,\n",
    "        )\n",
    "        if name_match:\n",
    "            case_name = name_match.group(1).strip().rstrip(\",\")\n",
    "            # Check if any citation in the corpus contains this case name\n",
    "            found = any(case_name in known for known in corpus)\n",
    "            if found:\n",
    "                verified.append(cite)\n",
    "            else:\n",
    "                unverified.append(cite)\n",
    "        else:\n",
    "            unverified.append(cite)\n",
    "\n",
    "    accuracy = len(verified) / len(citations) if citations else 1.0\n",
    "\n",
    "    return accuracy, {\n",
    "        \"total\": len(citations),\n",
    "        \"verified\": verified,\n",
    "        \"unverified\": unverified,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test on text with real citations\n",
    "real_citation_text = (\n",
    "    \"The summary judgment standard requires demonstrating that no genuine \"\n",
    "    \"dispute of material fact exists. Anderson v. Liberty Lobby, Inc., \"\n",
    "    \"477 U.S. 242 (1986). The moving party bears the initial burden. \"\n",
    "    \"Celotex Corp. v. Catrett, 477 U.S. 317 (1986). In the Fourth \"\n",
    "    \"Amendment context, Katz v. United States, 389 U.S. 347 (1967) \"\n",
    "    \"established the reasonable expectation of privacy test.\"\n",
    ")\n",
    "\n",
    "# Test on text with fabricated citations\n",
    "fabricated_citation_text = (\n",
    "    \"Employment discrimination claims must satisfy the burden-shifting \"\n",
    "    \"framework. Williams v. National Employment Board, 523 U.S. 891 \"\n",
    "    \"(2001). The employer must then articulate a legitimate reason. \"\n",
    "    \"Roberts v. Federal Labor Commission, 498 U.S. 332 (1995). \"\n",
    "    \"Failure to do so results in automatic liability per Davidson v. \"\n",
    "    \"Interstate Commerce Authority, 512 U.S. 445 (1999).\"\n",
    ")\n",
    "\n",
    "# Test on text with a mix\n",
    "mixed_citation_text = (\n",
    "    \"The Fourth Amendment protects against unreasonable searches. \"\n",
    "    \"Katz v. United States, 389 U.S. 347 (1967). The exclusionary \"\n",
    "    \"rule was applied to the states in Mapp v. Ohio, 367 U.S. 643 \"\n",
    "    \"(1961). Additionally, the good-faith exception was established \"\n",
    "    \"in Patterson v. State Police Authority, 501 U.S. 223 (1990).\"\n",
    ")\n",
    "\n",
    "print(\"Citation Accuracy Metric:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for label, text in [\n",
    "    (\"All real citations\", real_citation_text),\n",
    "    (\"All fabricated citations\", fabricated_citation_text),\n",
    "    (\"Mixed (2 real, 1 fabricated)\", mixed_citation_text),\n",
    "]:\n",
    "    score, details = citation_accuracy(text, citation_corpus)\n",
    "    print(f\"\\n  {label}:\")\n",
    "    print(f\"    Score: {score:.2f} ({details['total']} citations found)\")\n",
    "    if details[\"verified\"]:\n",
    "        print(f\"    Verified: {details['verified']}\")\n",
    "    if details[\"unverified\"]:\n",
    "        print(f\"    Unverified: {details['unverified']}\")\n",
    "\n",
    "print()\n",
    "print(\"Unlike ROUGE, this metric catches fabricated citations directly.\")\n",
    "print(\"A model that invents plausible-sounding cases scores 0.0, not high.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hallucination-heading",
   "metadata": {},
   "source": [
    "## Hallucination Detection\n",
    "\n",
    "Hallucination in legal AI means generating claims that are not grounded\n",
    "in the source material. This is distinct from citation fabrication --\n",
    "a model can hallucinate facts even without citing cases.\n",
    "\n",
    "Our approach:\n",
    "1. Extract factual claims from the generated text (sentences that make\n",
    "   assertions about specific entities, dates, or legal conclusions).\n",
    "2. Check each claim against the source context using token overlap.\n",
    "3. Compute: `hallucination_rate = ungrounded / total_claims`.\n",
    "\n",
    "This is a simple, heuristic approach. Production systems use NLI models\n",
    "or LLM-based verification for more accurate grounding checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hallucination-metric",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_claims(text):\n",
    "    \"\"\"Extract factual claims from text.\n",
    "\n",
    "    A simple heuristic: split into sentences, keep those that contain\n",
    "    specific entities (proper nouns, dates, numbers, legal terms).\n",
    "\n",
    "    Args:\n",
    "        text: Input text.\n",
    "\n",
    "    Returns:\n",
    "        List of claim strings.\n",
    "    \"\"\"\n",
    "    # Split into sentences (simple heuristic)\n",
    "    sentences = re.split(r\"(?<=[.!?])\\s+\", text.strip())\n",
    "\n",
    "    claims = []\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        if not sent:\n",
    "            continue\n",
    "        # Keep sentences that contain specific factual indicators:\n",
    "        # proper nouns, numbers, dates, legal keywords\n",
    "        has_proper_noun = bool(re.search(r\"[A-Z][a-z]+\\s+[A-Z]\", sent))\n",
    "        has_number = bool(re.search(r\"\\d+\", sent))\n",
    "        has_legal_term = bool(\n",
    "            re.search(\n",
    "                r\"\\b(held|ruled|found|granted|denied|reversed|affirmed|court|statute|amendment)\\b\",\n",
    "                sent,\n",
    "                re.IGNORECASE,\n",
    "            )\n",
    "        )\n",
    "        if has_proper_noun or has_number or has_legal_term:\n",
    "            claims.append(sent)\n",
    "\n",
    "    return claims\n",
    "\n",
    "\n",
    "def check_grounding(claim, source, threshold=0.5):\n",
    "    \"\"\"Check if a claim is grounded in source text using token overlap.\n",
    "\n",
    "    Computes the fraction of content words in the claim that appear\n",
    "    somewhere in the source text. A claim is grounded if the overlap\n",
    "    exceeds the threshold.\n",
    "\n",
    "    Args:\n",
    "        claim: A factual claim string.\n",
    "        source: The source/context text.\n",
    "        threshold: Minimum overlap ratio to consider grounded.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (is_grounded, overlap_ratio).\n",
    "    \"\"\"\n",
    "    # Tokenize into lowercase words, removing stopwords\n",
    "    stopwords = {\n",
    "        \"the\", \"a\", \"an\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\",\n",
    "        \"being\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"will\",\n",
    "        \"would\", \"could\", \"should\", \"may\", \"might\", \"shall\", \"can\",\n",
    "        \"to\", \"of\", \"in\", \"for\", \"on\", \"with\", \"at\", \"by\", \"from\",\n",
    "        \"as\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\",\n",
    "        \"below\", \"and\", \"but\", \"or\", \"nor\", \"not\", \"so\", \"yet\",\n",
    "        \"both\", \"either\", \"neither\", \"each\", \"every\", \"all\", \"any\",\n",
    "        \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\",\n",
    "        \"than\", \"too\", \"very\", \"just\", \"because\", \"if\", \"when\",\n",
    "        \"where\", \"how\", \"what\", \"which\", \"who\", \"whom\", \"this\",\n",
    "        \"that\", \"these\", \"those\", \"it\", \"its\", \"he\", \"she\", \"they\",\n",
    "        \"them\", \"their\", \"we\", \"our\", \"your\", \"my\",\n",
    "    }\n",
    "\n",
    "    def content_words(text):\n",
    "        words = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "        return [w for w in words if w not in stopwords and len(w) > 2]\n",
    "\n",
    "    claim_words = content_words(claim)\n",
    "    source_words = set(content_words(source))\n",
    "\n",
    "    if not claim_words:\n",
    "        return True, 1.0\n",
    "\n",
    "    overlap = sum(1 for w in claim_words if w in source_words)\n",
    "    ratio = overlap / len(claim_words)\n",
    "\n",
    "    return ratio >= threshold, ratio\n",
    "\n",
    "\n",
    "def hallucination_rate(generated_text, source_context, threshold=0.5):\n",
    "    \"\"\"Compute the hallucination rate of generated text.\n",
    "\n",
    "    Hallucination rate = (ungrounded claims) / (total claims).\n",
    "    Lower is better: 0.0 means all claims are grounded in the source.\n",
    "\n",
    "    Args:\n",
    "        generated_text: The model output to check.\n",
    "        source_context: The source material the model should draw from.\n",
    "        threshold: Overlap threshold for grounding check.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (hallucination_rate, details_dict).\n",
    "    \"\"\"\n",
    "    claims = extract_claims(generated_text)\n",
    "\n",
    "    if not claims:\n",
    "        return 0.0, {\"total\": 0, \"grounded\": [], \"ungrounded\": []}\n",
    "\n",
    "    grounded = []\n",
    "    ungrounded = []\n",
    "\n",
    "    for claim in claims:\n",
    "        is_grounded, overlap = check_grounding(claim, source_context, threshold)\n",
    "        if is_grounded:\n",
    "            grounded.append((claim, overlap))\n",
    "        else:\n",
    "            ungrounded.append((claim, overlap))\n",
    "\n",
    "    rate = len(ungrounded) / len(claims)\n",
    "\n",
    "    return rate, {\n",
    "        \"total\": len(claims),\n",
    "        \"grounded\": grounded,\n",
    "        \"ungrounded\": ungrounded,\n",
    "    }\n",
    "\n",
    "\n",
    "# Demonstrate with a court opinion as source context\n",
    "source = opinions[0][\"text\"]\n",
    "\n",
    "# Grounded response: draws facts from the source\n",
    "grounded_response = (\n",
    "    \"The Seventh Circuit reversed the district court's grant of summary \"\n",
    "    \"judgment in Henderson v. Meridian Health Systems. Henderson alleged \"\n",
    "    \"that Meridian violated the ADA by terminating his employment after \"\n",
    "    \"he disclosed his diagnosis of multiple sclerosis. The court found \"\n",
    "    \"genuine issues of material fact regarding whether Henderson could \"\n",
    "    \"perform essential functions with a modified travel schedule.\"\n",
    ")\n",
    "\n",
    "# Hallucinated response: invents facts not in the source\n",
    "hallucinated_response = (\n",
    "    \"The Seventh Circuit reversed the district court's grant of summary \"\n",
    "    \"judgment in Henderson v. Meridian Health Systems. The court awarded \"\n",
    "    \"$2.5 million in compensatory damages to Henderson. The jury found \"\n",
    "    \"that Meridian's CEO personally ordered the termination after learning \"\n",
    "    \"of the diagnosis. The court also imposed punitive damages of $10 million \"\n",
    "    \"and ordered the company to implement a new disability accommodation \"\n",
    "    \"program within 90 days.\"\n",
    ")\n",
    "\n",
    "print(\"Hallucination Detection:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for label, response in [\n",
    "    (\"Grounded response\", grounded_response),\n",
    "    (\"Hallucinated response\", hallucinated_response),\n",
    "]:\n",
    "    rate, details = hallucination_rate(response, source)\n",
    "    print(f\"\\n  {label}:\")\n",
    "    print(f\"    Hallucination rate: {rate:.2f} ({details['total']} claims)\")\n",
    "    print(f\"    Grounded claims: {len(details['grounded'])}\")\n",
    "    print(f\"    Ungrounded claims: {len(details['ungrounded'])}\")\n",
    "    if details[\"ungrounded\"]:\n",
    "        print(\"    Examples of ungrounded claims:\")\n",
    "        for claim, overlap in details[\"ungrounded\"][:3]:\n",
    "            truncated = claim[:80] + \"...\" if len(claim) > 80 else claim\n",
    "            print(f\"      - [{overlap:.2f}] {truncated}\")\n",
    "\n",
    "print()\n",
    "print(\"The grounded response draws from the source text and has low\")\n",
    "print(\"hallucination rate. The hallucinated response invents specific\")\n",
    "print(\"facts (dollar amounts, jury findings, timelines) not in the source.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises-heading",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise (a): Appropriate Uncertainty Metric\n",
    "\n",
    "Design a metric for \"appropriate uncertainty\" -- does the model say\n",
    "\"I'm not sure\" when it should?\n",
    "\n",
    "Consider:\n",
    "- Create a test set of questions where the correct answer involves\n",
    "  uncertainty (e.g., \"What will the Supreme Court rule on X?\", \"Is\n",
    "  this contract enforceable in all states?\").\n",
    "- Define hedging indicators: phrases like \"may\", \"could\", \"it depends\",\n",
    "  \"in some jurisdictions\", \"consult an attorney\".\n",
    "- Define overconfidence indicators: \"always\", \"never\", \"certainly\",\n",
    "  \"guaranteed\", \"will definitely\".\n",
    "- Compute: `uncertainty_score = hedging_count / (hedging_count + overconfidence_count)`.\n",
    "\n",
    "```python\n",
    "HEDGING_PHRASES = [\n",
    "    \"may\", \"could\", \"might\", \"it depends\", \"varies by jurisdiction\",\n",
    "    \"consult an attorney\", \"generally\", \"typically\", \"in most cases\",\n",
    "]\n",
    "OVERCONFIDENCE_PHRASES = [\n",
    "    \"always\", \"never\", \"certainly\", \"guaranteed\", \"will definitely\",\n",
    "    \"absolutely\", \"without exception\", \"in all cases\",\n",
    "]\n",
    "\n",
    "def uncertainty_score(text):\n",
    "    \"\"\"Compute appropriate uncertainty score.\"\"\"\n",
    "    # Your implementation here\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Exercise (b): Cross-Model Metric Comparison\n",
    "\n",
    "If you have trained models from Modules 06 and 07 (base, SFT, DPO),\n",
    "generate responses from each model for the same set of legal questions,\n",
    "then compare:\n",
    "\n",
    "1. Perplexity on held-out legal text.\n",
    "2. Citation accuracy on generated responses.\n",
    "3. Hallucination rate when given source context.\n",
    "4. ROUGE scores against reference answers.\n",
    "\n",
    "Create a comparison table:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "results = {\n",
    "    \"Model\": [\"Base\", \"SFT\", \"DPO\"],\n",
    "    \"Perplexity\": [...],\n",
    "    \"Citation Accuracy\": [...],\n",
    "    \"Hallucination Rate\": [...],\n",
    "    \"ROUGE-L\": [...],\n",
    "}\n",
    "df = pd.DataFrame(results)\n",
    "print(df.to_string(index=False))\n",
    "```\n",
    "\n",
    "Which model improves most on which metric? Does alignment (DPO)\n",
    "help with citation accuracy? Does SFT reduce hallucination rate?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
