{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 01 - Decoding Strategies for Text Generation\n",
    "\n",
    "## Context\n",
    "\n",
    "The generation strategy you choose directly affects the quality of an LLM's\n",
    "output. The same model can produce wildly different text depending on whether\n",
    "you use greedy decoding, beam search, or nucleus sampling.\n",
    "\n",
    "**CoCounsel context:** Legal responses need deterministic, accurate text -- not\n",
    "creative sampling. A contract clause that invents a nonexistent statute is worse\n",
    "than useless; it is dangerous. Understanding how each decoding strategy works\n",
    "lets you pick the right configuration for each legal task: deterministic greedy\n",
    "decoding for citation extraction, conservative sampling for drafting, and so on.\n",
    "\n",
    "### Autoregressive Generation\n",
    "\n",
    "Modern LLMs generate text **one token at a time**. At each step:\n",
    "\n",
    "1. The model takes the full sequence so far (prompt + previously generated tokens).\n",
    "2. It runs a forward pass and produces **logits** -- a raw score for every token\n",
    "   in the vocabulary (e.g., 50,257 scores for GPT-2).\n",
    "3. A **decoding strategy** selects the next token from those logits.\n",
    "4. The selected token is appended to the sequence, and we repeat.\n",
    "\n",
    "The decoding strategy is the decision-making layer between the model's raw\n",
    "predictions and the final text. In this notebook, we implement every major\n",
    "strategy from scratch -- no `model.generate()` -- so you understand exactly\n",
    "what happens at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We load GPT-2 small (124M parameters) from HuggingFace. This model is small\n",
    "enough to run on a CPU but large enough to produce coherent text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load GPT-2 small\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our legal prompt and get initial logits\n",
    "legal_prompt = \"The court held that the defendant\"\n",
    "\n",
    "input_ids = tokenizer.encode(legal_prompt, return_tensors=\"pt\")\n",
    "print(f\"Prompt: {legal_prompt!r}\")\n",
    "print(f\"Token IDs: {input_ids[0].tolist()}\")\n",
    "print(f\"Tokens: {[tokenizer.decode(tid) for tid in input_ids[0]]}\")\n",
    "print(f\"Number of tokens: {input_ids.shape[1]}\")\n",
    "\n",
    "# Get logits from the model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    # logits shape: (batch, seq_len, vocab_size)\n",
    "    # We want the logits for the NEXT token, so we take the last position\n",
    "    next_token_logits = outputs.logits[0, -1, :]\n",
    "\n",
    "print(f\"\\nLogits shape (full): {outputs.logits.shape}\")\n",
    "print(f\"Next-token logits shape: {next_token_logits.shape}\")\n",
    "print(f\"Logits range: [{next_token_logits.min():.2f}, {next_token_logits.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Understanding Logits\n",
    "\n",
    "The model outputs **logits** -- raw, unnormalized scores for every token in the\n",
    "vocabulary. These are not probabilities yet. To convert logits to probabilities,\n",
    "we apply the **softmax** function:\n",
    "\n",
    "$$P(\\text{token}_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$\n",
    "\n",
    "where $z_i$ is the logit for token $i$. Softmax has two key properties:\n",
    "\n",
    "1. All outputs are positive (due to the exponential).\n",
    "2. All outputs sum to 1 (due to the normalization).\n",
    "\n",
    "This gives us a valid probability distribution over the entire vocabulary.\n",
    "The decoding strategy then decides **how to select a token** from this\n",
    "distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert logits to probabilities\n",
    "probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "print(f\"Probabilities sum: {probs.sum():.6f}\")\n",
    "print(f\"Number of tokens with prob > 0.01: {(probs > 0.01).sum().item()}\")\n",
    "print(f\"Number of tokens with prob > 0.001: {(probs > 0.001).sum().item()}\")\n",
    "print(f\"Number of tokens with prob > 0.0001: {(probs > 0.0001).sum().item()}\")\n",
    "print(f\"\\nMost probability mass is concentrated in a small number of tokens.\")\n",
    "print(f\"Top 10 tokens account for {probs.topk(10).values.sum():.1%} of total probability.\")\n",
    "print(f\"Top 100 tokens account for {probs.topk(100).values.sum():.1%} of total probability.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top-10 most likely next tokens\n",
    "top_k_probs, top_k_indices = probs.topk(10)\n",
    "\n",
    "print(f\"Prompt: {legal_prompt!r}\")\n",
    "print(f\"\\nTop 10 next-token predictions:\")\n",
    "print(f\"{'Rank':<6} {'Token':<20} {'Probability':<15} {'Logit':<10}\")\n",
    "print(\"-\" * 51)\n",
    "for rank, (prob, idx) in enumerate(zip(top_k_probs, top_k_indices), 1):\n",
    "    token_str = tokenizer.decode(idx.item())\n",
    "    logit_val = next_token_logits[idx].item()\n",
    "    print(f\"{rank:<6} {token_str!r:<20} {prob.item():<15.4f} {logit_val:<10.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the probability distribution over the vocabulary\n",
    "top_n = 30\n",
    "top_probs, top_indices = probs.topk(top_n)\n",
    "top_tokens = [tokenizer.decode(idx.item()).strip() for idx in top_indices]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: bar chart of top-30 tokens\n",
    "ax = axes[0]\n",
    "bars = ax.barh(range(top_n - 1, -1, -1), top_probs.numpy(), color=\"steelblue\")\n",
    "ax.set_yticks(range(top_n - 1, -1, -1))\n",
    "ax.set_yticklabels(top_tokens, fontsize=8)\n",
    "ax.set_xlabel(\"Probability\")\n",
    "ax.set_title(f\"Top {top_n} Next-Token Probabilities\\nPrompt: {legal_prompt!r}\")\n",
    "ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "# Right: full distribution (sorted, log scale)\n",
    "ax = axes[1]\n",
    "sorted_probs, _ = probs.sort(descending=True)\n",
    "ax.plot(sorted_probs.numpy(), color=\"steelblue\", linewidth=1.5)\n",
    "ax.set_xlabel(\"Token rank (sorted by probability)\")\n",
    "ax.set_ylabel(\"Probability (log scale)\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(\"Full Vocabulary Probability Distribution\")\n",
    "ax.axvline(x=10, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Top 10\")\n",
    "ax.axvline(x=100, color=\"orange\", linestyle=\"--\", alpha=0.5, label=\"Top 100\")\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The distribution is extremely skewed -- a handful of tokens hold most\")\n",
    "print(\"of the probability mass, while thousands of tokens are near zero.\")\n",
    "print(\"This is why raw logits need processing before selecting a token:\")\n",
    "print(\"different strategies handle this long tail differently.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Greedy Decoding\n",
    "\n",
    "The simplest decoding strategy: at each step, pick the token with the\n",
    "**highest probability** (i.e., take the argmax of the logits).\n",
    "\n",
    "**Advantages:**\n",
    "- Deterministic -- same input always produces same output.\n",
    "- Fast -- no sampling, no maintaining multiple candidates.\n",
    "- Often produces grammatically correct text.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Tends to produce **repetitive text** -- the model gets stuck in loops.\n",
    "- Misses higher-probability sequences that require a locally suboptimal\n",
    "  choice early on (greedy is not globally optimal).\n",
    "- Output lacks diversity -- no variation between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, tokenizer, prompt, max_tokens=50):\n",
    "    \"\"\"Generate text using greedy decoding (always pick the argmax token).\n",
    "\n",
    "    Args:\n",
    "        model: A HuggingFace causal LM.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        prompt: The input text string.\n",
    "        max_tokens: Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        The full generated text (prompt + generated tokens).\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            # Get logits for the last token position\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "\n",
    "        # Greedy: pick the token with the highest logit\n",
    "        next_token_id = torch.argmax(logits).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        # Append to the sequence\n",
    "        input_ids = torch.cat([input_ids, next_token_id], dim=1)\n",
    "\n",
    "        # Stop if we generate the EOS token\n",
    "        if next_token_id.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Run greedy decoding on the legal prompt\n",
    "greedy_output = greedy_decode(model, tokenizer, legal_prompt, max_tokens=80)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GREEDY DECODING\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Prompt: {legal_prompt!r}\")\n",
    "print(f\"\\nGenerated text:\\n{greedy_output}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Notice: greedy decoding often produces repetitive loops.\")\n",
    "print(\"The model gets stuck repeating the same phrases because the\")\n",
    "print(\"highest-probability next token keeps leading back to the same state.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Beam Search\n",
    "\n",
    "Beam search addresses greedy decoding's key weakness: greedy decoding makes\n",
    "locally optimal choices that may not be globally optimal. A sequence might\n",
    "require picking a lower-probability token early on to reach a much\n",
    "higher-probability sequence overall.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. Start with the prompt as the single initial beam.\n",
    "2. At each step, expand each beam by considering every possible next token.\n",
    "3. Score each expanded sequence by summing the log-probabilities.\n",
    "4. Keep only the top-k sequences (where k = `num_beams`).\n",
    "5. Repeat until `max_tokens` or all beams hit EOS.\n",
    "6. Return the highest-scoring beam.\n",
    "\n",
    "Beam search explores more of the search space than greedy decoding while\n",
    "remaining tractable (we never explore the full exponential tree).\n",
    "\n",
    "**Note:** Beam search is still deterministic -- it always produces the same\n",
    "output for the same input. It generally finds higher-probability sequences\n",
    "than greedy decoding but is slower (k forward passes per step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model, tokenizer, prompt, num_beams=5, max_tokens=50):\n",
    "    \"\"\"Generate text using beam search.\n",
    "\n",
    "    Maintains `num_beams` candidate sequences at each step, expanding all\n",
    "    candidates and keeping the top-scoring ones.\n",
    "\n",
    "    Args:\n",
    "        model: A HuggingFace causal LM.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        prompt: The input text string.\n",
    "        num_beams: Number of beams (candidate sequences) to maintain.\n",
    "        max_tokens: Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        The highest-scoring generated text.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Each beam is a tuple of (token_ids_tensor, cumulative_log_prob)\n",
    "    beams = [(input_ids, 0.0)]\n",
    "\n",
    "    for step in range(max_tokens):\n",
    "        all_candidates = []\n",
    "\n",
    "        for seq, score in beams:\n",
    "            # If this beam already ended with EOS, keep it as-is\n",
    "            if seq[0, -1].item() == tokenizer.eos_token_id:\n",
    "                all_candidates.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(seq)\n",
    "                logits = outputs.logits[0, -1, :]\n",
    "\n",
    "            # Convert to log-probabilities\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "            # Get top-k candidates for this beam\n",
    "            top_log_probs, top_indices = log_probs.topk(num_beams)\n",
    "\n",
    "            for i in range(num_beams):\n",
    "                next_token_id = top_indices[i].unsqueeze(0).unsqueeze(0)\n",
    "                new_seq = torch.cat([seq, next_token_id], dim=1)\n",
    "                new_score = score + top_log_probs[i].item()\n",
    "                all_candidates.append((new_seq, new_score))\n",
    "\n",
    "        # Keep only the top num_beams candidates\n",
    "        all_candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        beams = all_candidates[:num_beams]\n",
    "\n",
    "        # Early stop if all beams ended with EOS\n",
    "        if all(\n",
    "            seq[0, -1].item() == tokenizer.eos_token_id\n",
    "            for seq, _ in beams\n",
    "        ):\n",
    "            break\n",
    "\n",
    "    # Return the highest-scoring beam\n",
    "    best_seq, best_score = beams[0]\n",
    "    return tokenizer.decode(best_seq[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Run beam search on the legal prompt\n",
    "beam_output = beam_search(model, tokenizer, legal_prompt, num_beams=5, max_tokens=50)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BEAM SEARCH (num_beams=5)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Prompt: {legal_prompt!r}\")\n",
    "print(f\"\\nGenerated text:\\n{beam_output}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nComparison with greedy:\")\n",
    "print(f\"  Greedy: {greedy_output[:120]}...\")\n",
    "print(f\"  Beam:   {beam_output[:120]}...\")\n",
    "print(\"\\nBeam search finds higher-probability sequences by exploring multiple\")\n",
    "print(\"paths simultaneously, but it is still deterministic and can still repeat.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Temperature Scaling\n",
    "\n",
    "Temperature scaling adjusts the **sharpness** of the probability distribution\n",
    "before sampling. The idea is simple: divide the logits by a temperature value\n",
    "$T$ before applying softmax.\n",
    "\n",
    "$$P(\\text{token}_i) = \\frac{e^{z_i / T}}{\\sum_j e^{z_j / T}}$$\n",
    "\n",
    "The effect:\n",
    "\n",
    "- **T < 1** (low temperature): Makes the distribution **sharper**. The\n",
    "  highest-probability token gets even more probability mass. As T approaches 0,\n",
    "  this approaches greedy decoding.\n",
    "- **T = 1**: No change (the default).\n",
    "- **T > 1** (high temperature): Makes the distribution **flatter**. Probability\n",
    "  mass spreads more evenly across tokens. As T approaches infinity, this\n",
    "  approaches uniform random sampling.\n",
    "\n",
    "**For legal text:** Lower temperatures (0.1 - 0.5) are generally preferred\n",
    "because they keep the model focused on high-probability, factually grounded\n",
    "continuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_temperature(logits, temperature):\n",
    "    \"\"\"Scale logits by temperature before softmax.\n",
    "\n",
    "    Args:\n",
    "        logits: Raw model logits, shape (vocab_size,).\n",
    "        temperature: Scaling factor. Lower = sharper, higher = flatter.\n",
    "\n",
    "    Returns:\n",
    "        Scaled logits.\n",
    "    \"\"\"\n",
    "    return logits / temperature\n",
    "\n",
    "\n",
    "# Demonstrate temperature scaling on the next-token logits from our legal prompt\n",
    "temperatures = [0.1, 0.5, 1.0, 2.0]\n",
    "\n",
    "print(f\"Prompt: {legal_prompt!r}\")\n",
    "print(f\"\\nEffect of temperature on top-5 token probabilities:\\n\")\n",
    "\n",
    "for temp in temperatures:\n",
    "    scaled_logits = apply_temperature(next_token_logits, temp)\n",
    "    scaled_probs = F.softmax(scaled_logits, dim=-1)\n",
    "    top5_probs, top5_indices = scaled_probs.topk(5)\n",
    "\n",
    "    tokens_str = \", \".join(\n",
    "        f\"{tokenizer.decode(idx.item()).strip()!r}: {p:.4f}\"\n",
    "        for p, idx in zip(top5_probs, top5_indices)\n",
    "    )\n",
    "    print(f\"  T={temp:<4} -> {tokens_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize probability distributions at different temperatures\n",
    "top_n_vis = 20\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, temp in zip(axes, temperatures):\n",
    "    scaled_logits = apply_temperature(next_token_logits, temp)\n",
    "    scaled_probs = F.softmax(scaled_logits, dim=-1)\n",
    "    top_probs_t, top_indices_t = scaled_probs.topk(top_n_vis)\n",
    "    top_tokens_t = [tokenizer.decode(idx.item()).strip() for idx in top_indices_t]\n",
    "\n",
    "    colors = [\"#d62728\" if i == 0 else \"steelblue\" for i in range(top_n_vis)]\n",
    "    ax.barh(\n",
    "        range(top_n_vis - 1, -1, -1), top_probs_t.numpy(), color=colors\n",
    "    )\n",
    "    ax.set_yticks(range(top_n_vis - 1, -1, -1))\n",
    "    ax.set_yticklabels(top_tokens_t, fontsize=8)\n",
    "    ax.set_xlabel(\"Probability\")\n",
    "\n",
    "    # Show entropy as a measure of distribution spread\n",
    "    entropy = -(scaled_probs * torch.log(scaled_probs + 1e-10)).sum().item()\n",
    "    ax.set_title(f\"Temperature = {temp}\\nEntropy = {entropy:.2f} nats\")\n",
    "    ax.grid(axis=\"x\", alpha=0.3)\n",
    "\n",
    "fig.suptitle(\n",
    "    f\"Next-Token Probability Distribution at Different Temperatures\\n\"\n",
    "    f\"Prompt: {legal_prompt!r}\",\n",
    "    fontsize=13,\n",
    "    y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Low temperature (0.1): Almost all mass on the top token -- nearly greedy.\")\n",
    "print(\"High temperature (2.0): Mass is spread across many tokens -- high randomness.\")\n",
    "print(\"For legal text, low temperatures keep the model focused and factual.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Top-k Sampling\n",
    "\n",
    "Top-k sampling restricts the candidate pool to the **k most likely tokens**,\n",
    "then renormalizes the probabilities and samples from this reduced distribution.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. Compute probabilities from logits (with optional temperature scaling).\n",
    "2. Keep only the top-k tokens; set all other probabilities to zero.\n",
    "3. Renormalize the remaining probabilities so they sum to 1.\n",
    "4. Sample from this distribution.\n",
    "\n",
    "**Advantages:**\n",
    "- Prevents sampling very unlikely tokens (which cause incoherent output).\n",
    "- Introduces controlled randomness -- output varies between runs.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Fixed k does not adapt to the distribution. When the model is confident\n",
    "  (one token has 95% probability), k=50 still considers 49 unlikely tokens.\n",
    "  When the model is uncertain, k=50 might cut off reasonable options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_sample(logits, k=50, temperature=1.0):\n",
    "    \"\"\"Sample a token using top-k sampling.\n",
    "\n",
    "    Keeps only the top-k highest-probability tokens, zeroes out the rest,\n",
    "    renormalizes, and samples.\n",
    "\n",
    "    Args:\n",
    "        logits: Raw model logits, shape (vocab_size,).\n",
    "        k: Number of top tokens to keep.\n",
    "        temperature: Temperature scaling factor.\n",
    "\n",
    "    Returns:\n",
    "        Sampled token index (int).\n",
    "    \"\"\"\n",
    "    # Apply temperature\n",
    "    scaled_logits = apply_temperature(logits, temperature)\n",
    "\n",
    "    # Find the top-k logits and their indices\n",
    "    top_k_logits, top_k_indices = scaled_logits.topk(k)\n",
    "\n",
    "    # Create a new logits tensor filled with -inf (zeroed after softmax)\n",
    "    filtered_logits = torch.full_like(scaled_logits, float(\"-inf\"))\n",
    "    filtered_logits.scatter_(0, top_k_indices, top_k_logits)\n",
    "\n",
    "    # Softmax to get renormalized probabilities\n",
    "    token_probs = F.softmax(filtered_logits, dim=-1)\n",
    "\n",
    "    # Sample from the distribution\n",
    "    return torch.multinomial(token_probs, num_samples=1).item()\n",
    "\n",
    "\n",
    "def generate_top_k(model, tokenizer, prompt, k=50, temperature=1.0, max_tokens=50):\n",
    "    \"\"\"Generate text using top-k sampling.\n",
    "\n",
    "    Args:\n",
    "        model: A HuggingFace causal LM.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        prompt: The input text string.\n",
    "        k: Number of top tokens to consider at each step.\n",
    "        temperature: Temperature scaling factor.\n",
    "        max_tokens: Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        The full generated text.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "\n",
    "        next_token_id = top_k_sample(logits, k=k, temperature=temperature)\n",
    "        next_token_tensor = torch.tensor([[next_token_id]])\n",
    "        input_ids = torch.cat([input_ids, next_token_tensor], dim=1)\n",
    "\n",
    "        if next_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Run top-k sampling with different k values\n",
    "print(f\"Prompt: {legal_prompt!r}\")\n",
    "print()\n",
    "\n",
    "k_values = [5, 20, 50, 100]\n",
    "for k in k_values:\n",
    "    torch.manual_seed(42)  # Reproducibility\n",
    "    output = generate_top_k(\n",
    "        model, tokenizer, legal_prompt, k=k, temperature=0.8, max_tokens=50\n",
    "    )\n",
    "    print(f\"k={k:<4} -> {output}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Top-p (Nucleus) Sampling\n",
    "\n",
    "Top-p sampling (also called **nucleus sampling**, introduced by Holtzman et al.,\n",
    "2019) fixes the key limitation of top-k: instead of using a fixed number of\n",
    "tokens, it dynamically selects tokens until their **cumulative probability**\n",
    "reaches a threshold $p$.\n",
    "\n",
    "**How it works:**\n",
    "\n",
    "1. Compute probabilities from logits (with optional temperature scaling).\n",
    "2. Sort tokens by probability (highest first).\n",
    "3. Compute the cumulative sum of probabilities.\n",
    "4. Keep tokens until the cumulative probability reaches $p$.\n",
    "5. Zero out all other tokens, renormalize, and sample.\n",
    "\n",
    "**Why this is better than top-k:**\n",
    "\n",
    "- When the model is **confident** (one token has 90% probability), top-p=0.9\n",
    "  might only keep 1-2 tokens -- staying focused.\n",
    "- When the model is **uncertain** (probability is spread across many tokens),\n",
    "  top-p=0.9 might keep 50+ tokens -- allowing diversity.\n",
    "\n",
    "Top-p **adapts to the shape of the distribution**, which top-k cannot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_sample(logits, p=0.9, temperature=1.0):\n",
    "    \"\"\"Sample a token using top-p (nucleus) sampling.\n",
    "\n",
    "    Includes the smallest set of tokens whose cumulative probability exceeds p,\n",
    "    then renormalizes and samples.\n",
    "\n",
    "    Args:\n",
    "        logits: Raw model logits, shape (vocab_size,).\n",
    "        p: Cumulative probability threshold.\n",
    "        temperature: Temperature scaling factor.\n",
    "\n",
    "    Returns:\n",
    "        Sampled token index (int).\n",
    "    \"\"\"\n",
    "    # Apply temperature\n",
    "    scaled_logits = apply_temperature(logits, temperature)\n",
    "\n",
    "    # Convert to probabilities\n",
    "    token_probs = F.softmax(scaled_logits, dim=-1)\n",
    "\n",
    "    # Sort by probability (descending)\n",
    "    sorted_probs, sorted_indices = torch.sort(token_probs, descending=True)\n",
    "\n",
    "    # Compute cumulative sum\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "    # Find tokens to remove: those where cumulative prob exceeds p\n",
    "    # We shift the cumulative sum right by 1 so the first token that\n",
    "    # pushes us over p is still included.\n",
    "    sorted_mask = cumulative_probs - sorted_probs >= p\n",
    "\n",
    "    # Zero out tokens beyond the threshold\n",
    "    sorted_probs[sorted_mask] = 0.0\n",
    "\n",
    "    # Renormalize\n",
    "    sorted_probs = sorted_probs / sorted_probs.sum()\n",
    "\n",
    "    # Sample from the filtered distribution\n",
    "    sampled_index = torch.multinomial(sorted_probs, num_samples=1).item()\n",
    "\n",
    "    # Map back to the original vocabulary index\n",
    "    return sorted_indices[sampled_index].item()\n",
    "\n",
    "\n",
    "def generate_top_p(model, tokenizer, prompt, p=0.9, temperature=1.0, max_tokens=50):\n",
    "    \"\"\"Generate text using top-p (nucleus) sampling.\n",
    "\n",
    "    Args:\n",
    "        model: A HuggingFace causal LM.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        prompt: The input text string.\n",
    "        p: Cumulative probability threshold.\n",
    "        temperature: Temperature scaling factor.\n",
    "        max_tokens: Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        The full generated text.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "\n",
    "        next_token_id = top_p_sample(logits, p=p, temperature=temperature)\n",
    "        next_token_tensor = torch.tensor([[next_token_id]])\n",
    "        input_ids = torch.cat([input_ids, next_token_tensor], dim=1)\n",
    "\n",
    "        if next_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Run top-p sampling with different p values\n",
    "print(f\"Prompt: {legal_prompt!r}\")\n",
    "print()\n",
    "\n",
    "p_values = [0.5, 0.8, 0.9, 0.95]\n",
    "for p_val in p_values:\n",
    "    torch.manual_seed(42)\n",
    "    output = generate_top_p(\n",
    "        model, tokenizer, legal_prompt, p=p_val, temperature=0.8, max_tokens=50\n",
    "    )\n",
    "    print(f\"p={p_val:<5} -> {output}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare top-p vs top-k: how many tokens does each include?\n",
    "# This demonstrates why top-p adapts better to different distributions.\n",
    "\n",
    "# Get logits at two different positions to show distribution variation\n",
    "comparison_prompts = [\n",
    "    \"The court held that the defendant\",\n",
    "    \"The contract shall be governed by the laws of\",\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, prompt_text in zip(axes, comparison_prompts):\n",
    "    ids = tokenizer.encode(prompt_text, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        out = model(ids)\n",
    "        logits_i = out.logits[0, -1, :]\n",
    "\n",
    "    p_dist = F.softmax(logits_i, dim=-1)\n",
    "    sorted_p, _ = p_dist.sort(descending=True)\n",
    "    cumsum = torch.cumsum(sorted_p, dim=-1)\n",
    "\n",
    "    # How many tokens needed to reach p=0.9?\n",
    "    n_tokens_p90 = (cumsum < 0.9).sum().item() + 1\n",
    "\n",
    "    ax.plot(sorted_p[:100].numpy(), label=\"Token probability\", color=\"steelblue\")\n",
    "    ax.axvline(\n",
    "        x=n_tokens_p90, color=\"red\", linestyle=\"--\",\n",
    "        label=f\"top-p=0.9 boundary ({n_tokens_p90} tokens)\",\n",
    "    )\n",
    "    ax.axvline(\n",
    "        x=50, color=\"orange\", linestyle=\"--\",\n",
    "        label=\"top-k=50 boundary\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Token rank\")\n",
    "    ax.set_ylabel(\"Probability\")\n",
    "    ax.set_title(f\"Prompt: {prompt_text!r}\\n(top-p=0.9 uses {n_tokens_p90} tokens)\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Top-p adapts to distribution shape; top-k uses a fixed cutoff\",\n",
    "    fontsize=12, y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"When the distribution is peaked (model is confident), top-p includes fewer tokens.\")\n",
    "print(\"When the distribution is flat (model is uncertain), top-p includes more tokens.\")\n",
    "print(\"Top-k always includes exactly k tokens regardless of the distribution shape.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Side-by-Side Comparison\n",
    "\n",
    "Let's run all strategies on the same legal prompt and compare the outputs\n",
    "directly. For sampling methods, we use `torch.manual_seed()` for\n",
    "reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate with all strategies\n",
    "max_gen_tokens = 60\n",
    "\n",
    "results = {}\n",
    "\n",
    "# 1. Greedy\n",
    "results[\"Greedy\"] = greedy_decode(\n",
    "    model, tokenizer, legal_prompt, max_tokens=max_gen_tokens\n",
    ")\n",
    "\n",
    "# 2. Beam search\n",
    "results[\"Beam Search (k=5)\"] = beam_search(\n",
    "    model, tokenizer, legal_prompt, num_beams=5, max_tokens=max_gen_tokens\n",
    ")\n",
    "\n",
    "# 3. Top-k sampling\n",
    "torch.manual_seed(42)\n",
    "results[\"Top-k (k=50, T=0.7)\"] = generate_top_k(\n",
    "    model, tokenizer, legal_prompt, k=50, temperature=0.7, max_tokens=max_gen_tokens\n",
    ")\n",
    "\n",
    "# 4. Top-p sampling\n",
    "torch.manual_seed(42)\n",
    "results[\"Top-p (p=0.9, T=0.7)\"] = generate_top_p(\n",
    "    model, tokenizer, legal_prompt, p=0.9, temperature=0.7, max_tokens=max_gen_tokens\n",
    ")\n",
    "\n",
    "# 5. Low temperature top-p (conservative, good for legal)\n",
    "torch.manual_seed(42)\n",
    "results[\"Top-p (p=0.9, T=0.3)\"] = generate_top_p(\n",
    "    model, tokenizer, legal_prompt, p=0.9, temperature=0.3, max_tokens=max_gen_tokens\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"=\" * 80)\n",
    "print(\"SIDE-BY-SIDE COMPARISON\")\n",
    "print(f\"Prompt: {legal_prompt!r}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for strategy, output in results.items():\n",
    "    print(f\"\\n--- {strategy} ---\")\n",
    "    # Show only the generated part (after the prompt)\n",
    "    generated = output[len(legal_prompt):]\n",
    "    print(f\"{legal_prompt}[{generated}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"- Greedy and beam search are deterministic but may be repetitive.\")\n",
    "print(\"- Sampling methods (top-k, top-p) produce more diverse output.\")\n",
    "print(\"- Lower temperature + top-p gives conservative, legal-appropriate text.\")\n",
    "print(\"- Higher temperature adds creativity but risks factual errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Probability Distribution Visualization\n",
    "\n",
    "For a single generation step, let's visualize the full probability distribution\n",
    "and highlight which tokens each strategy would select. This makes the\n",
    "differences concrete and visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get fresh logits for the visualization\n",
    "with torch.no_grad():\n",
    "    vis_outputs = model(tokenizer.encode(legal_prompt, return_tensors=\"pt\"))\n",
    "    vis_logits = vis_outputs.logits[0, -1, :]\n",
    "\n",
    "vis_probs = F.softmax(vis_logits, dim=-1)\n",
    "\n",
    "# Get top-30 tokens for display\n",
    "top30_probs, top30_indices = vis_probs.topk(30)\n",
    "top30_tokens = [tokenizer.decode(idx.item()).strip() for idx in top30_indices]\n",
    "\n",
    "# Determine which tokens each strategy would consider\n",
    "# Greedy: just the top-1 token\n",
    "greedy_selection = {0}  # index into top-30\n",
    "\n",
    "# Top-k (k=10): top 10 tokens\n",
    "topk_selection = set(range(min(10, 30)))\n",
    "\n",
    "# Top-p (p=0.9): tokens until cumulative prob >= 0.9\n",
    "cumulative = torch.cumsum(top30_probs, dim=-1)\n",
    "topp_selection = set()\n",
    "for i in range(30):\n",
    "    topp_selection.add(i)\n",
    "    if cumulative[i].item() >= 0.9:\n",
    "        break\n",
    "\n",
    "# Create the visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "x = np.arange(30)\n",
    "bar_colors = []\n",
    "for i in range(30):\n",
    "    if i in greedy_selection:\n",
    "        bar_colors.append(\"#d62728\")   # red for greedy\n",
    "    elif i in topp_selection:\n",
    "        bar_colors.append(\"#2ca02c\")   # green for top-p\n",
    "    elif i in topk_selection:\n",
    "        bar_colors.append(\"#1f77b4\")   # blue for top-k only\n",
    "    else:\n",
    "        bar_colors.append(\"#cccccc\")   # gray for excluded\n",
    "\n",
    "bars = ax.bar(x, top30_probs.numpy(), color=bar_colors, edgecolor=\"white\", linewidth=0.5)\n",
    "\n",
    "# Add token labels\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(top30_tokens, rotation=60, ha=\"right\", fontsize=8)\n",
    "ax.set_ylabel(\"Probability\", fontsize=11)\n",
    "ax.set_title(\n",
    "    f\"Next-Token Probabilities with Strategy Selections Highlighted\\n\"\n",
    "    f\"Prompt: {legal_prompt!r}\",\n",
    "    fontsize=12,\n",
    ")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"#d62728\", label=\"Greedy (top-1)\"),\n",
    "    Patch(facecolor=\"#2ca02c\", label=f\"Top-p=0.9 ({len(topp_selection)} tokens)\"),\n",
    "    Patch(facecolor=\"#1f77b4\", label=\"Top-k=10 (10 tokens)\"),\n",
    "    Patch(facecolor=\"#cccccc\", label=\"Excluded by all\"),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc=\"upper right\", fontsize=10)\n",
    "\n",
    "# Add cumulative probability line on secondary y-axis\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(\n",
    "    x, cumulative.numpy(), color=\"black\", linewidth=1.5, linestyle=\"--\",\n",
    "    alpha=0.6, marker=\".\", markersize=3,\n",
    ")\n",
    "ax2.axhline(y=0.9, color=\"black\", linewidth=0.8, linestyle=\":\", alpha=0.4)\n",
    "ax2.set_ylabel(\"Cumulative probability\", fontsize=11)\n",
    "ax2.set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Greedy selects: {top30_tokens[0]!r} (probability: {top30_probs[0]:.4f})\")\n",
    "print(f\"Top-k=10 considers: {top30_tokens[:10]}\")\n",
    "print(f\"Top-p=0.9 considers: {top30_tokens[:len(topp_selection)]}\")\n",
    "print(f\"\")\n",
    "print(f\"Top-p adapts: it includes {len(topp_selection)} tokens to reach 90% cumulative probability.\")\n",
    "print(f\"Top-k always includes exactly 10 tokens regardless of the distribution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise (a): Combine Top-k + Top-p + Temperature\n",
    "\n",
    "In practice, production systems often combine multiple strategies. Implement\n",
    "a generation function that applies all three: temperature scaling, then top-k\n",
    "filtering, then top-p filtering, then sampling.\n",
    "\n",
    "Experiment with different combinations and find settings that produce good\n",
    "legal text -- coherent, non-repetitive, and factually conservative.\n",
    "\n",
    "```python\n",
    "def combined_sample(logits, k=50, p=0.9, temperature=0.7):\n",
    "    \"\"\"Sample using temperature + top-k + top-p combined.\"\"\"\n",
    "    # Step 1: Apply temperature\n",
    "    scaled_logits = logits / temperature\n",
    "\n",
    "    # Step 2: Top-k filtering -- keep only top-k logits\n",
    "    top_k_logits, top_k_indices = scaled_logits.topk(k)\n",
    "    filtered = torch.full_like(scaled_logits, float(\"-inf\"))\n",
    "    filtered.scatter_(0, top_k_indices, top_k_logits)\n",
    "\n",
    "    # Step 3: Top-p filtering on the remaining tokens\n",
    "    probs = F.softmax(filtered, dim=-1)\n",
    "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "    cumulative = torch.cumsum(sorted_probs, dim=-1)\n",
    "    mask = (cumulative - sorted_probs) >= p\n",
    "    sorted_probs[mask] = 0.0\n",
    "    sorted_probs = sorted_probs / sorted_probs.sum()\n",
    "\n",
    "    # Step 4: Sample\n",
    "    sampled_idx = torch.multinomial(sorted_probs, num_samples=1).item()\n",
    "    return sorted_indices[sampled_idx].item()\n",
    "\n",
    "\n",
    "# Try these combinations on the legal prompt:\n",
    "configs = [\n",
    "    {\"k\": 50, \"p\": 0.9, \"temperature\": 0.3},   # Conservative\n",
    "    {\"k\": 50, \"p\": 0.9, \"temperature\": 0.7},   # Balanced\n",
    "    {\"k\": 100, \"p\": 0.95, \"temperature\": 1.0},  # Creative\n",
    "]\n",
    "```\n",
    "\n",
    "### Exercise (b): Implement Repetition Penalty\n",
    "\n",
    "Repetition is a common problem in text generation, especially with greedy\n",
    "decoding. Implement a **repetition penalty** that divides the logits of\n",
    "previously generated tokens by a penalty factor, making them less likely\n",
    "to be selected again.\n",
    "\n",
    "```python\n",
    "def apply_repetition_penalty(logits, generated_ids, penalty=1.2):\n",
    "    \"\"\"Penalize tokens that have already been generated.\n",
    "\n",
    "    For each token that appears in generated_ids:\n",
    "    - If its logit is positive, divide by penalty (makes it smaller).\n",
    "    - If its logit is negative, multiply by penalty (makes it more negative).\n",
    "\n",
    "    Args:\n",
    "        logits: Raw model logits, shape (vocab_size,).\n",
    "        generated_ids: List of previously generated token IDs.\n",
    "        penalty: Penalty factor (> 1.0 to penalize repetition).\n",
    "\n",
    "    Returns:\n",
    "        Modified logits with repetition penalty applied.\n",
    "    \"\"\"\n",
    "    penalized_logits = logits.clone()\n",
    "    for token_id in set(generated_ids):\n",
    "        if penalized_logits[token_id] > 0:\n",
    "            penalized_logits[token_id] /= penalty\n",
    "        else:\n",
    "            penalized_logits[token_id] *= penalty\n",
    "    return penalized_logits\n",
    "\n",
    "\n",
    "# Modify greedy_decode to use repetition penalty:\n",
    "def greedy_decode_with_penalty(model, tokenizer, prompt, max_tokens=50, penalty=1.2):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    generated_ids = input_ids[0].tolist()\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            next_logits = outputs.logits[0, -1, :]\n",
    "\n",
    "        # Apply repetition penalty\n",
    "        next_logits = apply_repetition_penalty(next_logits, generated_ids, penalty)\n",
    "\n",
    "        next_token_id = torch.argmax(next_logits).item()\n",
    "        generated_ids.append(next_token_id)\n",
    "        next_token_tensor = torch.tensor([[next_token_id]])\n",
    "        input_ids = torch.cat([input_ids, next_token_tensor], dim=1)\n",
    "\n",
    "        if next_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Compare greedy with and without repetition penalty:\n",
    "# greedy_output = greedy_decode(model, tokenizer, legal_prompt, max_tokens=80)\n",
    "# penalized_output = greedy_decode_with_penalty(\n",
    "#     model, tokenizer, legal_prompt, max_tokens=80, penalty=1.2\n",
    "# )\n",
    "#\n",
    "# Try different penalty values: 1.0 (no penalty), 1.2, 1.5, 2.0\n",
    "# Observe how repetition decreases but coherence may also change.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
