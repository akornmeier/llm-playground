{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "theory",
   "metadata": {},
   "source": [
    "# 02 - Direct Preference Optimization (DPO)\n",
    "\n",
    "## Theory\n",
    "\n",
    "In the previous notebook, we trained a reward model and discussed PPO\n",
    "as the mechanism for using that reward model to align a language model.\n",
    "PPO works, but it is complex: you need a reward model, a value function,\n",
    "a reference policy, and a carefully tuned RL training loop.\n",
    "\n",
    "**DPO (Direct Preference Optimization)** is a simpler alternative.\n",
    "The key insight: you can skip the reward model entirely and directly\n",
    "optimize the policy using preference data. The DPO loss implicitly\n",
    "learns the reward.\n",
    "\n",
    "### The DPO Loss Function\n",
    "\n",
    "Given a prompt $x$, a chosen response $y_w$, and a rejected response\n",
    "$y_l$, the DPO loss is:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{DPO}} = -\\log \\sigma \\left( \\beta \\left[ \\log \\frac{\\pi_\\theta(y_w | x)}{\\pi_{\\text{ref}}(y_w | x)} - \\log \\frac{\\pi_\\theta(y_l | x)}{\\pi_{\\text{ref}}(y_l | x)} \\right] \\right)$$\n",
    "\n",
    "Where:\n",
    "- $\\pi_\\theta$ is the policy (the model being trained)\n",
    "- $\\pi_{\\text{ref}}$ is the reference policy (a frozen copy of the SFT model)\n",
    "- $\\beta$ is a temperature parameter controlling alignment strength\n",
    "- $\\sigma$ is the sigmoid function\n",
    "\n",
    "### Intuition\n",
    "\n",
    "The log-ratio $\\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)}$\n",
    "measures how much the policy has changed from the reference for a given\n",
    "response. DPO pushes the policy to:\n",
    "\n",
    "- **Increase** the probability of the chosen response (relative to the\n",
    "  reference)\n",
    "- **Decrease** the probability of the rejected response (relative to\n",
    "  the reference)\n",
    "\n",
    "The $\\beta$ parameter acts as a KL constraint (like in PPO): higher\n",
    "$\\beta$ means weaker alignment (stays closer to the reference), lower\n",
    "$\\beta$ means stronger alignment (deviates more from the reference).\n",
    "\n",
    "### Connection to Reward Modeling\n",
    "\n",
    "Rafailov et al. (2023) showed that DPO is mathematically equivalent to\n",
    "first training a reward model and then running PPO -- but collapsed into\n",
    "a single training step. The implicit reward learned by DPO is:\n",
    "\n",
    "$$r(x, y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{\\text{ref}}(y|x)} + \\beta \\log Z(x)$$\n",
    "\n",
    "This means DPO learns a reward model \"for free\" as a byproduct of\n",
    "training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-dpo",
   "metadata": {},
   "source": [
    "## Why DPO is Practical\n",
    "\n",
    "DPO has several advantages over the full RLHF pipeline:\n",
    "\n",
    "| Aspect | RLHF (PPO) | DPO |\n",
    "|--------|-----------|-----|\n",
    "| Reward model | Required (separate training) | Not needed |\n",
    "| Value function | Required (critic network) | Not needed |\n",
    "| Training loop | Complex RL loop | Standard supervised loss |\n",
    "| Hyperparameters | Many (clip range, GAE lambda, etc.) | Few (mainly beta) |\n",
    "| Compute | High (multiple forward passes) | Moderate (two forward passes) |\n",
    "| Stability | Can be unstable | Generally stable |\n",
    "| Small-scale | Difficult | Works well |\n",
    "\n",
    "DPO requires only:\n",
    "1. Preference data (prompt, chosen, rejected)\n",
    "2. A model to train (the policy)\n",
    "3. A frozen copy of that model (the reference)\n",
    "4. A single training loop with the DPO loss\n",
    "\n",
    "This makes it particularly suitable for our setting: a small model\n",
    "(SmolLM-135M) on CPU with limited preference data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-heading",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset as HFDataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import DPOConfig, DPOTrainer\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load a fresh model for DPO training\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# The reference model is a frozen copy of the same model.\n",
    "# In practice, this would be the SFT model from Module 06.\n",
    "# Here we use the base model as both policy and reference for simplicity.\n",
    "ref_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Parameters: {num_params:,}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token!r} (id={tokenizer.pad_token_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-heading",
   "metadata": {},
   "source": [
    "## Preference Dataset\n",
    "\n",
    "We use the same preference dataset from notebook 01. DPO expects data\n",
    "in a specific format: each example needs a `prompt`, a `chosen` response,\n",
    "and a `rejected` response. The `trl` DPOTrainer handles tokenization\n",
    "and formatting internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preference-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same preference dataset from notebook 01\n",
    "preference_data = [\n",
    "    {\n",
    "        \"prompt\": \"What is the standard for summary judgment in federal court?\",\n",
    "        \"chosen\": (\n",
    "            \"Under Federal Rule of Civil Procedure 56, summary judgment is \"\n",
    "            \"appropriate when there is no genuine dispute as to any material \"\n",
    "            \"fact and the movant is entitled to judgment as a matter of law. \"\n",
    "            \"The Supreme Court clarified this standard in Celotex Corp. v. \"\n",
    "            \"Catrett, 477 U.S. 317 (1986), holding that the moving party \"\n",
    "            \"bears the initial burden of demonstrating the absence of a \"\n",
    "            \"genuine issue of material fact.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"Summary judgment is granted when there are no facts in dispute. \"\n",
    "            \"The landmark case Henderson v. United States Department of \"\n",
    "            \"Justice, 589 U.S. 42 (2019) established the modern three-part \"\n",
    "            \"test for summary judgment: (1) materiality, (2) genuineness, \"\n",
    "            \"and (3) sufficiency of evidence. This test is universally \"\n",
    "            \"applied in all federal courts.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Can an employer fire an employee for filing a workers' compensation claim?\",\n",
    "        \"chosen\": (\n",
    "            \"In most jurisdictions, retaliatory termination for filing a \"\n",
    "            \"workers' compensation claim is prohibited. However, the specific \"\n",
    "            \"protections and remedies vary by state. Many states have enacted \"\n",
    "            \"statutes explicitly prohibiting such retaliation, while others \"\n",
    "            \"recognize a common-law tort of retaliatory discharge. An \"\n",
    "            \"employment attorney licensed in the relevant jurisdiction should \"\n",
    "            \"be consulted for case-specific advice.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"No, an employer absolutely cannot fire an employee for filing a \"\n",
    "            \"workers' compensation claim. This is illegal in all 50 states \"\n",
    "            \"and will always result in a successful wrongful termination \"\n",
    "            \"lawsuit. The employee will be awarded damages and the employer \"\n",
    "            \"will face criminal penalties.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What did the Supreme Court hold in Miranda v. Arizona?\",\n",
    "        \"chosen\": (\n",
    "            \"In Miranda v. Arizona, 384 U.S. 436 (1966), the Supreme Court \"\n",
    "            \"held that the Fifth Amendment's protection against self-\"\n",
    "            \"incrimination requires law enforcement to inform suspects of \"\n",
    "            \"their rights before custodial interrogation. These rights \"\n",
    "            \"include the right to remain silent, the warning that statements \"\n",
    "            \"may be used against them, and the right to an attorney.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"In Miranda v. Arizona (1966), the Supreme Court held that all \"\n",
    "            \"criminal suspects must be read their rights at the time of \"\n",
    "            \"arrest, regardless of whether interrogation occurs. Failure to \"\n",
    "            \"read Miranda rights automatically results in dismissal of all \"\n",
    "            \"charges and the suspect must be immediately released from \"\n",
    "            \"custody.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is the doctrine of qualified immunity?\",\n",
    "        \"chosen\": (\n",
    "            \"Qualified immunity shields government officials from civil \"\n",
    "            \"liability unless their conduct violates clearly established \"\n",
    "            \"statutory or constitutional rights of which a reasonable person \"\n",
    "            \"would have known. Harlow v. Fitzgerald, 457 U.S. 800 (1982). \"\n",
    "            \"The doctrine balances the need to hold officials accountable \"\n",
    "            \"with the need to protect them from undue interference with \"\n",
    "            \"their duties.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"Qualified immunity means government employees can never be \"\n",
    "            \"sued for actions taken during their official duties. This was \"\n",
    "            \"established in Roberts v. Federal Government Agency, 12 F.Supp. \"\n",
    "            \"4th 892 (D.D.C. 2021). It is an absolute protection that \"\n",
    "            \"cannot be overcome regardless of the severity of the \"\n",
    "            \"constitutional violation.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Is a verbal agreement legally binding?\",\n",
    "        \"chosen\": (\n",
    "            \"A verbal agreement can be legally binding if it meets the \"\n",
    "            \"general requirements of contract formation: offer, acceptance, \"\n",
    "            \"consideration, and mutual assent. However, certain types of \"\n",
    "            \"contracts must be in writing under the Statute of Frauds, \"\n",
    "            \"including contracts for the sale of land, contracts that cannot \"\n",
    "            \"be performed within one year, and contracts for the sale of \"\n",
    "            \"goods over $500 under UCC 2-201. Enforceability also depends \"\n",
    "            \"on the ability to prove the agreement's terms.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"Verbal agreements are never legally binding. You always need a \"\n",
    "            \"written contract signed by both parties for any agreement to \"\n",
    "            \"have legal force. Without a written document, no court will \"\n",
    "            \"enforce the agreement.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Does federal or state law govern a slip-and-fall in a grocery store?\",\n",
    "        \"chosen\": (\n",
    "            \"Slip-and-fall cases in grocery stores are typically governed by \"\n",
    "            \"state negligence law. The specific elements and standards vary \"\n",
    "            \"by jurisdiction. In most states, the plaintiff must show the \"\n",
    "            \"store owed a duty of care, breached that duty, and the breach \"\n",
    "            \"caused the plaintiff's injuries. Some states apply comparative \"\n",
    "            \"negligence while others apply contributory negligence. Federal \"\n",
    "            \"courts may hear such cases under diversity jurisdiction if the \"\n",
    "            \"parties are from different states and the amount in controversy \"\n",
    "            \"exceeds $75,000, but state substantive law still applies.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"Slip-and-fall cases are governed by the Federal Premises \"\n",
    "            \"Liability Act, 42 U.S.C. 1983-B. This federal statute \"\n",
    "            \"establishes a strict liability standard for all commercial \"\n",
    "            \"property owners, meaning the grocery store is automatically \"\n",
    "            \"liable for any injury that occurs on its premises.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is the difference between a motion to dismiss and a motion for summary judgment?\",\n",
    "        \"chosen\": (\n",
    "            \"A motion to dismiss under FRCP 12(b)(6) tests the legal \"\n",
    "            \"sufficiency of the complaint, accepting all factual allegations \"\n",
    "            \"as true. It asks whether the complaint states a plausible claim \"\n",
    "            \"for relief. Ashcroft v. Iqbal, 556 U.S. 662 (2009). A motion \"\n",
    "            \"for summary judgment under FRCP 56 comes later, after \"\n",
    "            \"discovery, and tests whether genuine disputes of material fact \"\n",
    "            \"exist. It considers evidence beyond the pleadings.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"A motion to dismiss and a motion for summary judgment are \"\n",
    "            \"essentially the same thing filed at different times. Both ask \"\n",
    "            \"the judge to end the case because the other side has no \"\n",
    "            \"evidence. The only difference is that a motion to dismiss is \"\n",
    "            \"filed before trial and summary judgment is filed during trial.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What constitutional protection applies to unreasonable searches?\",\n",
    "        \"chosen\": (\n",
    "            \"The Fourth Amendment protects against unreasonable searches and \"\n",
    "            \"seizures by the government. Under Katz v. United States, 389 \"\n",
    "            \"U.S. 347 (1967), a search occurs when the government violates \"\n",
    "            \"a person's reasonable expectation of privacy. The exclusionary \"\n",
    "            \"rule, established in Mapp v. Ohio, 367 U.S. 643 (1961), \"\n",
    "            \"generally bars the use of illegally obtained evidence at trial, \"\n",
    "            \"though several exceptions exist.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"The Sixth Amendment protects against unreasonable searches. It \"\n",
    "            \"states that no search can ever be conducted without a warrant \"\n",
    "            \"signed by a federal judge. Any evidence found without a warrant \"\n",
    "            \"is automatically inadmissible in any court proceeding, with no \"\n",
    "            \"exceptions.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Will I win my discrimination lawsuit against my employer?\",\n",
    "        \"chosen\": (\n",
    "            \"I cannot predict the outcome of a specific case. Employment \"\n",
    "            \"discrimination claims under Title VII require showing that the \"\n",
    "            \"employer took an adverse action because of a protected \"\n",
    "            \"characteristic. The McDonnell Douglas burden-shifting framework \"\n",
    "            \"applies to circumstantial evidence cases. Success depends on \"\n",
    "            \"the specific facts, available evidence, jurisdiction, and many \"\n",
    "            \"other factors. Consulting with an employment attorney who can \"\n",
    "            \"review the details of your situation is strongly recommended.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"Based on what you've described, you will definitely win your \"\n",
    "            \"case. Discrimination lawsuits are almost always successful when \"\n",
    "            \"the employee has been treated unfairly. You should expect to \"\n",
    "            \"receive at least $500,000 in damages plus attorney's fees. I \"\n",
    "            \"recommend filing immediately.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What standard of review applies to a district court's factual findings on appeal?\",\n",
    "        \"chosen\": (\n",
    "            \"Appellate courts review a district court's findings of fact \"\n",
    "            \"under the clearly erroneous standard, as required by Federal \"\n",
    "            \"Rule of Civil Procedure 52(a)(6). A finding is clearly \"\n",
    "            \"erroneous when, although there is evidence to support it, the \"\n",
    "            \"reviewing court is left with the definite and firm conviction \"\n",
    "            \"that a mistake has been made. Anderson v. City of Bessemer \"\n",
    "            \"City, 470 U.S. 564 (1985). Legal conclusions are reviewed \"\n",
    "            \"de novo.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"All district court decisions are reviewed de novo on appeal, \"\n",
    "            \"meaning the appellate court starts completely fresh and gives \"\n",
    "            \"no deference to the lower court. The appellate court re-weighs \"\n",
    "            \"all evidence and can substitute its own factual findings for \"\n",
    "            \"those of the district court.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is the statute of limitations for personal injury in most states?\",\n",
    "        \"chosen\": (\n",
    "            \"The statute of limitations for personal injury varies by state \"\n",
    "            \"but is commonly two to three years from the date of injury. \"\n",
    "            \"Some states, like Kentucky and Louisiana, set it at one year, \"\n",
    "            \"while others, like Maine, allow up to six years. The discovery \"\n",
    "            \"rule may toll the limitations period in cases where the injury \"\n",
    "            \"was not immediately apparent. It is critical to check the \"\n",
    "            \"specific statute in the applicable jurisdiction.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"The statute of limitations for personal injury is exactly five \"\n",
    "            \"years in every state under the Uniform Personal Injury \"\n",
    "            \"Limitations Act. There are no exceptions or tolling provisions. \"\n",
    "            \"If you miss this deadline, your case is permanently barred.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Can a landlord evict a tenant without notice?\",\n",
    "        \"chosen\": (\n",
    "            \"In nearly all jurisdictions, landlords must provide written \"\n",
    "            \"notice before initiating eviction proceedings. The required \"\n",
    "            \"notice period varies: commonly 3 days for nonpayment of rent, \"\n",
    "            \"30 days for month-to-month tenancies, and longer in some rent-\"\n",
    "            \"controlled jurisdictions. Self-help evictions -- such as \"\n",
    "            \"changing locks or shutting off utilities -- are generally \"\n",
    "            \"illegal. The landlord must follow the judicial eviction process \"\n",
    "            \"established by state law.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"A landlord can evict a tenant whenever they want because it is \"\n",
    "            \"the landlord's property. Property rights mean the owner has \"\n",
    "            \"complete control over who lives in the property. The tenant \"\n",
    "            \"has no rights once the landlord decides they should leave.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What damages can I recover in a breach of contract case?\",\n",
    "        \"chosen\": (\n",
    "            \"In a breach of contract case, the non-breaching party may \"\n",
    "            \"recover expectation damages -- the amount needed to put them \"\n",
    "            \"in the position they would have been in had the contract been \"\n",
    "            \"performed. This can include direct damages and consequential \"\n",
    "            \"damages that were foreseeable at the time of contracting, per \"\n",
    "            \"Hadley v. Baxendale (1854). Punitive damages are generally not \"\n",
    "            \"available in contract cases. The non-breaching party also has \"\n",
    "            \"a duty to mitigate damages.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"In a breach of contract case, you can recover unlimited \"\n",
    "            \"damages including punitive damages, emotional distress, and \"\n",
    "            \"pain and suffering. Courts typically award triple damages as a \"\n",
    "            \"punishment to the breaching party. There is no limit on what \"\n",
    "            \"you can claim.\"\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Preference dataset: {len(preference_data)} pairs\")\n",
    "for i, pair in enumerate(preference_data):\n",
    "    print(f\"  [{i:>2}] {pair['prompt'][:65]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dpo-training-heading",
   "metadata": {},
   "source": [
    "## DPO Training with trl\n",
    "\n",
    "The `trl` library provides `DPOTrainer`, which handles the DPO loss\n",
    "computation, reference model management, and training loop. We configure\n",
    "it with:\n",
    "\n",
    "- **beta**: The KL constraint strength (default 0.1)\n",
    "- The preference dataset in HuggingFace Dataset format\n",
    "- The policy model and tokenizer\n",
    "\n",
    "The trainer internally:\n",
    "1. Tokenizes prompts, chosen, and rejected responses\n",
    "2. Computes log-probabilities under the policy and reference\n",
    "3. Applies the DPO loss\n",
    "4. Updates the policy while keeping the reference frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert preference data to HuggingFace Dataset format.\n",
    "# DPOTrainer expects columns: \"prompt\", \"chosen\", \"rejected\"\n",
    "dpo_dataset = HFDataset.from_dict({\n",
    "    \"prompt\": [p[\"prompt\"] for p in preference_data],\n",
    "    \"chosen\": [p[\"chosen\"] for p in preference_data],\n",
    "    \"rejected\": [p[\"rejected\"] for p in preference_data],\n",
    "})\n",
    "\n",
    "print(f\"DPO dataset: {dpo_dataset}\")\n",
    "print(f\"\\nSample entry:\")\n",
    "print(f\"  Prompt:   {dpo_dataset[0]['prompt'][:60]}...\")\n",
    "print(f\"  Chosen:   {dpo_dataset[0]['chosen'][:60]}...\")\n",
    "print(f\"  Rejected: {dpo_dataset[0]['rejected'][:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dpo-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure DPO training\n",
    "dpo_config = DPOConfig(\n",
    "    output_dir=\"./dpo_output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    learning_rate=5e-5,\n",
    "    beta=0.1,                  # KL constraint strength\n",
    "    max_length=512,            # Max total sequence length\n",
    "    max_prompt_length=256,     # Max prompt length\n",
    "    logging_steps=2,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",          # Disable wandb / tensorboard\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(f\"DPO Configuration:\")\n",
    "print(f\"  Beta: {dpo_config.beta}\")\n",
    "print(f\"  Learning rate: {dpo_config.learning_rate}\")\n",
    "print(f\"  Epochs: {dpo_config.num_train_epochs}\")\n",
    "print(f\"  Batch size: {dpo_config.per_device_train_batch_size}\")\n",
    "print(f\"  Max length: {dpo_config.max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DPOTrainer\n",
    "# The trainer takes the policy model, reference model, and dataset.\n",
    "# It handles all the DPO loss computation internally.\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    args=dpo_config,\n",
    "    train_dataset=dpo_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"DPOTrainer created.\")\n",
    "print(f\"Training examples: {len(dpo_dataset)}\")\n",
    "print(f\"Steps per epoch: {len(trainer.get_train_dataloader())}\")\n",
    "print(f\"Total training steps: {len(trainer.get_train_dataloader()) * int(dpo_config.num_train_epochs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DPO training\n",
    "print(\"Starting DPO training...\")\n",
    "print(\"(This will take a few minutes on CPU.)\")\n",
    "print()\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(f\"\\nTraining complete.\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Total steps: {train_result.global_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-training-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and plot training metrics from the trainer log history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract loss values from log entries that have 'loss' key\n",
    "train_losses = [\n",
    "    entry[\"loss\"] for entry in log_history if \"loss\" in entry\n",
    "]\n",
    "train_steps = [\n",
    "    entry[\"step\"] for entry in log_history if \"loss\" in entry\n",
    "]\n",
    "\n",
    "if train_losses:\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(train_steps, train_losses, marker=\"o\", color=\"steelblue\", linewidth=2)\n",
    "    ax.set_xlabel(\"Training Step\")\n",
    "    ax.set_ylabel(\"DPO Loss\")\n",
    "    ax.set_title(\"DPO Training Loss\")\n",
    "    ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Initial loss: {train_losses[0]:.4f}\")\n",
    "    print(f\"Final loss: {train_losses[-1]:.4f}\")\n",
    "else:\n",
    "    print(\"No loss values found in training log.\")\n",
    "    print(\"Log entries:\", [list(e.keys()) for e in log_history[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparing-heading",
   "metadata": {},
   "source": [
    "## Comparing Outputs\n",
    "\n",
    "Now we compare the DPO-aligned model against the base model on legal\n",
    "prompts where alignment matters. We focus on prompts that could lead to:\n",
    "\n",
    "- Hallucinated citations\n",
    "- Overconfident legal advice\n",
    "- Incorrect statements of law\n",
    "\n",
    "The DPO model should show some preference shift toward the patterns\n",
    "in the \"chosen\" responses: hedged language, accurate framing, and\n",
    "appropriate caveats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(gen_model, gen_tokenizer, prompt, max_new_tokens=100):\n",
    "    \"\"\"Generate text from a prompt using greedy decoding.\n",
    "\n",
    "    Args:\n",
    "        gen_model: A HuggingFace causal LM.\n",
    "        gen_tokenizer: The corresponding tokenizer.\n",
    "        prompt: Input text.\n",
    "        max_new_tokens: Maximum tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        The generated text (only the completion, not the prompt).\n",
    "    \"\"\"\n",
    "    gen_model.eval()\n",
    "    inputs = gen_tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = gen_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=gen_tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode only the generated tokens (not the prompt)\n",
    "    generated_ids = output_ids[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return gen_tokenizer.decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-outputs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tricky legal prompts where alignment matters\n",
    "test_prompts = [\n",
    "    # Could lead to hallucinated citations\n",
    "    \"What is the legal standard for establishing negligence?\",\n",
    "    # Could lead to overconfident advice\n",
    "    \"Will I win my personal injury case?\",\n",
    "    # Could lead to incorrect law\n",
    "    \"Can the police search my car without a warrant?\",\n",
    "    # Could lead to oversimplification\n",
    "    \"Is my non-compete agreement enforceable?\",\n",
    "    # Could lead to false certainty\n",
    "    \"How much will I get in my divorce settlement?\",\n",
    "]\n",
    "\n",
    "# Load a fresh base model for comparison (the ref_model was not trained)\n",
    "print(\"COMPARISON: Base Model vs DPO-Aligned Model\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    base_output = generate_text(ref_model, tokenizer, prompt, max_new_tokens=80)\n",
    "    dpo_output = generate_text(model, tokenizer, prompt, max_new_tokens=80)\n",
    "\n",
    "    print(f\"PROMPT: {prompt}\")\n",
    "    print(f\"  BASE MODEL:  {base_output[:200]}\")\n",
    "    print(f\"  DPO ALIGNED: {dpo_output[:200]}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Notes on interpreting results:\")\n",
    "print(\"- SmolLM-135M is a tiny model with limited language ability.\")\n",
    "print(\"- With only 13 preference pairs and 3 epochs, alignment effects\")\n",
    "print(\"  will be subtle. In production, you would use thousands of pairs.\")\n",
    "print(\"- The key question is not whether the output is perfect, but whether\")\n",
    "print(\"  the DPO model shows any shift toward the preferred response style:\")\n",
    "print(\"  hedging, caveats, and measured language.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beta-heading",
   "metadata": {},
   "source": [
    "## Beta Parameter\n",
    "\n",
    "The beta parameter in DPO controls the strength of alignment:\n",
    "\n",
    "- **Low beta** (e.g., 0.05): Stronger alignment. The model deviates more\n",
    "  from the reference to match preferences. Risk: the model may become\n",
    "  too constrained or lose general capabilities.\n",
    "\n",
    "- **High beta** (e.g., 0.5): Weaker alignment. The model stays closer\n",
    "  to the reference. Risk: the alignment signal may be too weak to\n",
    "  change behavior meaningfully.\n",
    "\n",
    "- **Default beta** (0.1): A common starting point that balances alignment\n",
    "  strength with stability.\n",
    "\n",
    "Below we train with different beta values to observe the effect.\n",
    "Each configuration starts from a fresh copy of the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beta-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_values = [0.05, 0.1, 0.5]\n",
    "beta_models = {}\n",
    "beta_losses = {}\n",
    "\n",
    "for beta in beta_values:\n",
    "    print(f\"\\nTraining with beta={beta}...\")\n",
    "\n",
    "    # Fresh model for each beta\n",
    "    beta_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    beta_ref = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    beta_config = DPOConfig(\n",
    "        output_dir=f\"./dpo_beta_{beta}\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=2,\n",
    "        learning_rate=5e-5,\n",
    "        beta=beta,\n",
    "        max_length=512,\n",
    "        max_prompt_length=256,\n",
    "        logging_steps=2,\n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    beta_trainer = DPOTrainer(\n",
    "        model=beta_model,\n",
    "        ref_model=beta_ref,\n",
    "        args=beta_config,\n",
    "        train_dataset=dpo_dataset,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "\n",
    "    result = beta_trainer.train()\n",
    "    print(f\"  beta={beta}: loss={result.training_loss:.4f}\")\n",
    "\n",
    "    # Store model and losses\n",
    "    beta_models[beta] = beta_model\n",
    "    beta_losses[beta] = [\n",
    "        entry[\"loss\"]\n",
    "        for entry in beta_trainer.state.log_history\n",
    "        if \"loss\" in entry\n",
    "    ]\n",
    "\n",
    "print(\"\\nAll beta experiments complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-beta-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves for different beta values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax = axes[0]\n",
    "colors = [\"#e74c3c\", \"#2ecc71\", \"#3498db\"]\n",
    "for i, (beta, losses) in enumerate(beta_losses.items()):\n",
    "    if losses:\n",
    "        ax.plot(losses, marker=\"o\", color=colors[i], label=f\"beta={beta}\", linewidth=2)\n",
    "ax.set_xlabel(\"Logging Step\")\n",
    "ax.set_ylabel(\"DPO Loss\")\n",
    "ax.set_title(\"DPO Loss by Beta Value\")\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Final losses comparison\n",
    "ax = axes[1]\n",
    "final_losses = [beta_losses[b][-1] if beta_losses[b] else 0 for b in beta_values]\n",
    "ax.bar(\n",
    "    [f\"beta={b}\" for b in beta_values],\n",
    "    final_losses,\n",
    "    color=colors,\n",
    "    edgecolor=\"white\",\n",
    ")\n",
    "ax.set_ylabel(\"Final DPO Loss\")\n",
    "ax.set_title(\"Final Loss by Beta Value\")\n",
    "ax.grid(alpha=0.3, axis=\"y\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Lower beta = stronger alignment signal = potentially lower loss.\")\n",
    "print(\"Higher beta = weaker alignment signal = loss may not decrease as much.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-beta-outputs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare outputs across beta values\n",
    "comparison_prompts = [\n",
    "    \"Can the police search my phone without a warrant?\",\n",
    "    \"Will I win my breach of contract lawsuit?\",\n",
    "]\n",
    "\n",
    "print(\"OUTPUT COMPARISON ACROSS BETA VALUES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt in comparison_prompts:\n",
    "    print(f\"\\nPROMPT: {prompt}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    # Base model\n",
    "    base_out = generate_text(ref_model, tokenizer, prompt, max_new_tokens=80)\n",
    "    print(f\"  BASE:       {base_out[:180]}\")\n",
    "\n",
    "    # Each beta value\n",
    "    for beta in beta_values:\n",
    "        beta_out = generate_text(beta_models[beta], tokenizer, prompt, max_new_tokens=80)\n",
    "        print(f\"  beta={beta}: {beta_out[:180]}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Observations:\")\n",
    "print(\"- Lower beta (0.05) should show the most deviation from the base model.\")\n",
    "print(\"- Higher beta (0.5) should produce outputs closest to the base model.\")\n",
    "print(\"- With a tiny model and small dataset, differences may be subtle.\")\n",
    "print(\"- In production, beta tuning is done by measuring win rates on held-out\")\n",
    "print(\"  preference data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises-heading",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise (a): Vary the DPO Beta Parameter\n",
    "\n",
    "The beta experiment above used 3 epochs. Explore more systematically:\n",
    "\n",
    "1. Train with beta values of 0.01, 0.05, 0.1, 0.2, 0.5, and 1.0.\n",
    "2. For each, measure the final loss and generate outputs for the same\n",
    "   set of test prompts.\n",
    "3. Plot final loss vs beta. What is the relationship?\n",
    "4. At what beta value does the model's output become indistinguishable\n",
    "   from the base model?\n",
    "\n",
    "```python\n",
    "# Starter code\n",
    "beta_sweep = [0.01, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
    "results = {}\n",
    "\n",
    "for beta in beta_sweep:\n",
    "    sweep_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    sweep_ref = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "    config = DPOConfig(\n",
    "        output_dir=f\"./dpo_sweep_{beta}\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=2,\n",
    "        learning_rate=5e-5,\n",
    "        beta=beta,\n",
    "        max_length=512,\n",
    "        max_prompt_length=256,\n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    t = DPOTrainer(\n",
    "        model=sweep_model,\n",
    "        ref_model=sweep_ref,\n",
    "        args=config,\n",
    "        train_dataset=dpo_dataset,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "    res = t.train()\n",
    "    results[beta] = {\n",
    "        \"loss\": res.training_loss,\n",
    "        \"model\": sweep_model,\n",
    "    }\n",
    "\n",
    "# Plot loss vs beta\n",
    "plt.plot(\n",
    "    list(results.keys()),\n",
    "    [r[\"loss\"] for r in results.values()],\n",
    "    marker=\"o\",\n",
    ")\n",
    "plt.xlabel(\"Beta\")\n",
    "plt.ylabel(\"Final Loss\")\n",
    "plt.title(\"DPO Loss vs Beta\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Exercise (b): Adversarial Prompts\n",
    "\n",
    "Create adversarial prompts that try to get the aligned model to\n",
    "hallucinate or give overconfident answers. Test whether alignment helps:\n",
    "\n",
    "1. Write 5 prompts designed to elicit hallucinated citations:\n",
    "   - \"Cite the leading case on [obscure topic]\"\n",
    "   - \"What did the Supreme Court decide in [fake case name]?\"\n",
    "\n",
    "2. Write 5 prompts designed to elicit overconfident predictions:\n",
    "   - \"Guarantee me I'll win this case\"\n",
    "   - \"What will the jury award me?\"\n",
    "\n",
    "3. Compare base model vs DPO model outputs. Does alignment reduce\n",
    "   hallucination or overconfidence?\n",
    "\n",
    "```python\n",
    "adversarial_prompts = [\n",
    "    \"Cite the leading Supreme Court case on quantum computing patents.\",\n",
    "    \"What did the court hold in Smith v. TechCorp, 600 U.S. 1 (2024)?\",\n",
    "    \"Guarantee that my landlord will lose this eviction case.\",\n",
    "    \"What exact dollar amount will I receive in my injury settlement?\",\n",
    "    \"Is it legal to record my neighbor? Give me a definitive yes or no.\",\n",
    "]\n",
    "\n",
    "for prompt in adversarial_prompts:\n",
    "    base_out = generate_text(ref_model, tokenizer, prompt)\n",
    "    dpo_out = generate_text(model, tokenizer, prompt)\n",
    "    print(f\"PROMPT: {prompt}\")\n",
    "    print(f\"  BASE: {base_out[:150]}\")\n",
    "    print(f\"  DPO:  {dpo_out[:150]}\")\n",
    "    print()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
