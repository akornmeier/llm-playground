{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "context",
   "metadata": {},
   "source": [
    "# 01 - Reward Modeling\n",
    "\n",
    "## Context\n",
    "\n",
    "RLHF (Reinforcement Learning from Human Feedback) aligns language models\n",
    "with human preferences. After supervised fine-tuning (Module 06), a model\n",
    "can follow instructions -- but it has no notion of *which* response is\n",
    "better when multiple valid completions exist. RLHF encodes those\n",
    "preferences into the model.\n",
    "\n",
    "**CoCounsel context:** A legal AI must prefer citing real cases over\n",
    "plausible-sounding fabrications. It must prefer hedged language\n",
    "(\"the court may find...\") over overconfident claims (\"the court will\n",
    "certainly rule...\"). It must prefer accurate statements of law over\n",
    "subtly incorrect ones. Without alignment, an instruction-following model\n",
    "can produce fluent, well-formatted answers that are confidently wrong --\n",
    "the worst failure mode for a legal tool.\n",
    "\n",
    "The full RLHF pipeline has three stages:\n",
    "\n",
    "1. **SFT** -- Train the model to follow instructions (Module 06).\n",
    "2. **Reward Model** -- Train a model to score response quality using\n",
    "   human preference data. This is the focus of this notebook.\n",
    "3. **PPO** -- Use the reward model to further optimize the policy via\n",
    "   reinforcement learning (covered conceptually at the end).\n",
    "\n",
    "In this notebook, we build a preference dataset for legal tasks, implement\n",
    "a reward model from scratch as a PyTorch `nn.Module`, train it using the\n",
    "Bradley-Terry loss, and check whether it correctly ranks good responses\n",
    "above bad ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preference-data-heading",
   "metadata": {},
   "source": [
    "## Preference Data\n",
    "\n",
    "Preference data is the foundation of alignment. For each prompt, we\n",
    "provide two responses:\n",
    "\n",
    "- **Chosen** (good): accurate citations, appropriate hedging, correct law\n",
    "- **Rejected** (bad): hallucinated citations, overconfident claims,\n",
    "  incorrect legal reasoning\n",
    "\n",
    "The reward model learns from these pairs: given a prompt, it should assign\n",
    "a higher scalar score to the chosen response than the rejected one.\n",
    "\n",
    "In production, preference data comes from human annotators (lawyers, in\n",
    "our case) who read two responses and select the better one. Here we\n",
    "create the dataset manually with realistic legal examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preference-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand-crafted preference dataset for legal tasks.\n",
    "# Each entry has a prompt, a chosen (good) response, and a rejected (bad) response.\n",
    "\n",
    "preference_data = [\n",
    "    # --- 1. Real citation vs hallucinated citation ---\n",
    "    {\n",
    "        \"prompt\": \"What is the standard for summary judgment in federal court?\",\n",
    "        \"chosen\": (\n",
    "            \"Under Federal Rule of Civil Procedure 56, summary judgment is \"\n",
    "            \"appropriate when there is no genuine dispute as to any material \"\n",
    "            \"fact and the movant is entitled to judgment as a matter of law. \"\n",
    "            \"The Supreme Court clarified this standard in Celotex Corp. v. \"\n",
    "            \"Catrett, 477 U.S. 317 (1986), holding that the moving party \"\n",
    "            \"bears the initial burden of demonstrating the absence of a \"\n",
    "            \"genuine issue of material fact.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"Summary judgment is granted when there are no facts in dispute. \"\n",
    "            \"The landmark case Henderson v. United States Department of \"\n",
    "            \"Justice, 589 U.S. 42 (2019) established the modern three-part \"\n",
    "            \"test for summary judgment: (1) materiality, (2) genuineness, \"\n",
    "            \"and (3) sufficiency of evidence. This test is universally \"\n",
    "            \"applied in all federal courts.\"\n",
    "        ),\n",
    "    },\n",
    "    # --- 2. Appropriate hedging vs overconfident claim ---\n",
    "    {\n",
    "        \"prompt\": \"Can an employer fire an employee for filing a workers' compensation claim?\",\n",
    "        \"chosen\": (\n",
    "            \"In most jurisdictions, retaliatory termination for filing a \"\n",
    "            \"workers' compensation claim is prohibited. However, the specific \"\n",
    "            \"protections and remedies vary by state. Many states have enacted \"\n",
    "            \"statutes explicitly prohibiting such retaliation, while others \"\n",
    "            \"recognize a common-law tort of retaliatory discharge. An \"\n",
    "            \"employment attorney licensed in the relevant jurisdiction should \"\n",
    "            \"be consulted for case-specific advice.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"No, an employer absolutely cannot fire an employee for filing a \"\n",
    "            \"workers' compensation claim. This is illegal in all 50 states \"\n",
    "            \"and will always result in a successful wrongful termination \"\n",
    "            \"lawsuit. The employee will be awarded damages and the employer \"\n",
    "            \"will face criminal penalties.\"\n",
    "        ),\n",
    "    },\n",
    "    # --- 3. Accurate holding vs incorrect law ---\n",
    "    {\n",
    "        \"prompt\": \"What did the Supreme Court hold in Miranda v. Arizona?\",\n",
    "        \"chosen\": (\n",
    "            \"In Miranda v. Arizona, 384 U.S. 436 (1966), the Supreme Court \"\n",
    "            \"held that the Fifth Amendment's protection against self-\"\n",
    "            \"incrimination requires law enforcement to inform suspects of \"\n",
    "            \"their rights before custodial interrogation. These rights \"\n",
    "            \"include the right to remain silent, the warning that statements \"\n",
    "            \"may be used against them, and the right to an attorney.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"In Miranda v. Arizona (1966), the Supreme Court held that all \"\n",
    "            \"criminal suspects must be read their rights at the time of \"\n",
    "            \"arrest, regardless of whether interrogation occurs. Failure to \"\n",
    "            \"read Miranda rights automatically results in dismissal of all \"\n",
    "            \"charges and the suspect must be immediately released from \"\n",
    "            \"custody.\"\n",
    "        ),\n",
    "    },\n",
    "    # --- 4. Proper citation format vs fabricated reporter ---\n",
    "    {\n",
    "        \"prompt\": \"What is the doctrine of qualified immunity?\",\n",
    "        \"chosen\": (\n",
    "            \"Qualified immunity shields government officials from civil \"\n",
    "            \"liability unless their conduct violates clearly established \"\n",
    "            \"statutory or constitutional rights of which a reasonable person \"\n",
    "            \"would have known. Harlow v. Fitzgerald, 457 U.S. 800 (1982). \"\n",
    "            \"The doctrine balances the need to hold officials accountable \"\n",
    "            \"with the need to protect them from undue interference with \"\n",
    "            \"their duties.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"Qualified immunity means government employees can never be \"\n",
    "            \"sued for actions taken during their official duties. This was \"\n",
    "            \"established in Roberts v. Federal Government Agency, 12 F.Supp. \"\n",
    "            \"4th 892 (D.D.C. 2021). It is an absolute protection that \"\n",
    "            \"cannot be overcome regardless of the severity of the \"\n",
    "            \"constitutional violation.\"\n",
    "        ),\n",
    "    },\n",
    "    # --- 5. Nuanced analysis vs oversimplification ---\n",
    "    {\n",
    "        \"prompt\": \"Is a verbal agreement legally binding?\",\n",
    "        \"chosen\": (\n",
    "            \"A verbal agreement can be legally binding if it meets the \"\n",
    "            \"general requirements of contract formation: offer, acceptance, \"\n",
    "            \"consideration, and mutual assent. However, certain types of \"\n",
    "            \"contracts must be in writing under the Statute of Frauds, \"\n",
    "            \"including contracts for the sale of land, contracts that cannot \"\n",
    "            \"be performed within one year, and contracts for the sale of \"\n",
    "            \"goods over $500 under UCC 2-201. Enforceability also depends \"\n",
    "            \"on the ability to prove the agreement's terms.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"Verbal agreements are never legally binding. You always need a \"\n",
    "            \"written contract signed by both parties for any agreement to \"\n",
    "            \"have legal force. Without a written document, no court will \"\n",
    "            \"enforce the agreement.\"\n",
    "        ),\n",
    "    },\n",
    "    # --- 6. Correct jurisdictional analysis vs wrong jurisdiction ---\n",
    "    {\n",
    "        \"prompt\": \"Does federal or state law govern a slip-and-fall in a grocery store?\",\n",
    "        \"chosen\": (\n",
    "            \"Slip-and-fall cases in grocery stores are typically governed by \"\n",
    "            \"state negligence law. The specific elements and standards vary \"\n",
    "            \"by jurisdiction. In most states, the plaintiff must show the \"\n",
    "            \"store owed a duty of care, breached that duty, and the breach \"\n",
    "            \"caused the plaintiff's injuries. Some states apply comparative \"\n",
    "            \"negligence while others apply contributory negligence. Federal \"\n",
    "            \"courts may hear such cases under diversity jurisdiction if the \"\n",
    "            \"parties are from different states and the amount in controversy \"\n",
    "            \"exceeds $75,000, but state substantive law still applies.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"Slip-and-fall cases are governed by the Federal Premises \"\n",
    "            \"Liability Act, 42 U.S.C. 1983-B. This federal statute \"\n",
    "            \"establishes a strict liability standard for all commercial \"\n",
    "            \"property owners, meaning the grocery store is automatically \"\n",
    "            \"liable for any injury that occurs on its premises.\"\n",
    "        ),\n",
    "    },\n",
    "    # --- 7. Accurate procedural explanation vs confused procedure ---\n",
    "    {\n",
    "        \"prompt\": \"What is the difference between a motion to dismiss and a motion for summary judgment?\",\n",
    "        \"chosen\": (\n",
    "            \"A motion to dismiss under FRCP 12(b)(6) tests the legal \"\n",
    "            \"sufficiency of the complaint, accepting all factual allegations \"\n",
    "            \"as true. It asks whether the complaint states a plausible claim \"\n",
    "            \"for relief. Ashcroft v. Iqbal, 556 U.S. 662 (2009). A motion \"\n",
    "            \"for summary judgment under FRCP 56 comes later, after \"\n",
    "            \"discovery, and tests whether genuine disputes of material fact \"\n",
    "            \"exist. It considers evidence beyond the pleadings.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"A motion to dismiss and a motion for summary judgment are \"\n",
    "            \"essentially the same thing filed at different times. Both ask \"\n",
    "            \"the judge to end the case because the other side has no \"\n",
    "            \"evidence. The only difference is that a motion to dismiss is \"\n",
    "            \"filed before trial and summary judgment is filed during trial.\"\n",
    "        ),\n",
    "    },\n",
    "    # --- 8. Correct constitutional analysis vs wrong amendment ---\n",
    "    {\n",
    "        \"prompt\": \"What constitutional protection applies to unreasonable searches?\",\n",
    "        \"chosen\": (\n",
    "            \"The Fourth Amendment protects against unreasonable searches and \"\n",
    "            \"seizures by the government. Under Katz v. United States, 389 \"\n",
    "            \"U.S. 347 (1967), a search occurs when the government violates \"\n",
    "            \"a person's reasonable expectation of privacy. The exclusionary \"\n",
    "            \"rule, established in Mapp v. Ohio, 367 U.S. 643 (1961), \"\n",
    "            \"generally bars the use of illegally obtained evidence at trial, \"\n",
    "            \"though several exceptions exist.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"The Sixth Amendment protects against unreasonable searches. It \"\n",
    "            \"states that no search can ever be conducted without a warrant \"\n",
    "            \"signed by a federal judge. Any evidence found without a warrant \"\n",
    "            \"is automatically inadmissible in any court proceeding, with no \"\n",
    "            \"exceptions.\"\n",
    "        ),\n",
    "    },\n",
    "    # --- 9. Responsible limitation acknowledgment vs false certainty ---\n",
    "    {\n",
    "        \"prompt\": \"Will I win my discrimination lawsuit against my employer?\",\n",
    "        \"chosen\": (\n",
    "            \"I cannot predict the outcome of a specific case. Employment \"\n",
    "            \"discrimination claims under Title VII require showing that the \"\n",
    "            \"employer took an adverse action because of a protected \"\n",
    "            \"characteristic. The McDonnell Douglas burden-shifting framework \"\n",
    "            \"applies to circumstantial evidence cases. Success depends on \"\n",
    "            \"the specific facts, available evidence, jurisdiction, and many \"\n",
    "            \"other factors. Consulting with an employment attorney who can \"\n",
    "            \"review the details of your situation is strongly recommended.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"Based on what you've described, you will definitely win your \"\n",
    "            \"case. Discrimination lawsuits are almost always successful when \"\n",
    "            \"the employee has been treated unfairly. You should expect to \"\n",
    "            \"receive at least $500,000 in damages plus attorney's fees. I \"\n",
    "            \"recommend filing immediately.\"\n",
    "        ),\n",
    "    },\n",
    "    # --- 10. Correct standard of review vs wrong standard ---\n",
    "    {\n",
    "        \"prompt\": \"What standard of review applies to a district court's factual findings on appeal?\",\n",
    "        \"chosen\": (\n",
    "            \"Appellate courts review a district court's findings of fact \"\n",
    "            \"under the clearly erroneous standard, as required by Federal \"\n",
    "            \"Rule of Civil Procedure 52(a)(6). A finding is clearly \"\n",
    "            \"erroneous when, although there is evidence to support it, the \"\n",
    "            \"reviewing court is left with the definite and firm conviction \"\n",
    "            \"that a mistake has been made. Anderson v. City of Bessemer \"\n",
    "            \"City, 470 U.S. 564 (1985). Legal conclusions are reviewed \"\n",
    "            \"de novo.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"All district court decisions are reviewed de novo on appeal, \"\n",
    "            \"meaning the appellate court starts completely fresh and gives \"\n",
    "            \"no deference to the lower court. The appellate court re-weighs \"\n",
    "            \"all evidence and can substitute its own factual findings for \"\n",
    "            \"those of the district court.\"\n",
    "        ),\n",
    "    },\n",
    "    # --- 11. Accurate statute of limitations vs wrong timeframe ---\n",
    "    {\n",
    "        \"prompt\": \"What is the statute of limitations for personal injury in most states?\",\n",
    "        \"chosen\": (\n",
    "            \"The statute of limitations for personal injury varies by state \"\n",
    "            \"but is commonly two to three years from the date of injury. \"\n",
    "            \"Some states, like Kentucky and Louisiana, set it at one year, \"\n",
    "            \"while others, like Maine, allow up to six years. The discovery \"\n",
    "            \"rule may toll the limitations period in cases where the injury \"\n",
    "            \"was not immediately apparent. It is critical to check the \"\n",
    "            \"specific statute in the applicable jurisdiction.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"The statute of limitations for personal injury is exactly five \"\n",
    "            \"years in every state under the Uniform Personal Injury \"\n",
    "            \"Limitations Act. There are no exceptions or tolling provisions. \"\n",
    "            \"If you miss this deadline, your case is permanently barred.\"\n",
    "        ),\n",
    "    },\n",
    "    # --- 12. Proper legal reasoning vs circular reasoning ---\n",
    "    {\n",
    "        \"prompt\": \"Can a landlord evict a tenant without notice?\",\n",
    "        \"chosen\": (\n",
    "            \"In nearly all jurisdictions, landlords must provide written \"\n",
    "            \"notice before initiating eviction proceedings. The required \"\n",
    "            \"notice period varies: commonly 3 days for nonpayment of rent, \"\n",
    "            \"30 days for month-to-month tenancies, and longer in some rent-\"\n",
    "            \"controlled jurisdictions. Self-help evictions -- such as \"\n",
    "            \"changing locks or shutting off utilities -- are generally \"\n",
    "            \"illegal. The landlord must follow the judicial eviction process \"\n",
    "            \"established by state law.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"A landlord can evict a tenant whenever they want because it is \"\n",
    "            \"the landlord's property. Property rights mean the owner has \"\n",
    "            \"complete control over who lives in the property. The tenant \"\n",
    "            \"has no rights once the landlord decides they should leave.\"\n",
    "        ),\n",
    "    },\n",
    "    # --- 13. Accurate damages explanation vs inflated expectations ---\n",
    "    {\n",
    "        \"prompt\": \"What damages can I recover in a breach of contract case?\",\n",
    "        \"chosen\": (\n",
    "            \"In a breach of contract case, the non-breaching party may \"\n",
    "            \"recover expectation damages -- the amount needed to put them \"\n",
    "            \"in the position they would have been in had the contract been \"\n",
    "            \"performed. This can include direct damages and consequential \"\n",
    "            \"damages that were foreseeable at the time of contracting, per \"\n",
    "            \"Hadley v. Baxendale (1854). Punitive damages are generally not \"\n",
    "            \"available in contract cases. The non-breaching party also has \"\n",
    "            \"a duty to mitigate damages.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"In a breach of contract case, you can recover unlimited \"\n",
    "            \"damages including punitive damages, emotional distress, and \"\n",
    "            \"pain and suffering. Courts typically award triple damages as a \"\n",
    "            \"punishment to the breaching party. There is no limit on what \"\n",
    "            \"you can claim.\"\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Preference dataset: {len(preference_data)} pairs\")\n",
    "print()\n",
    "for i, pair in enumerate(preference_data):\n",
    "    print(f\"  [{i:>2}] {pair['prompt'][:70]}\")\n",
    "    print(f\"       Chosen:   {pair['chosen'][:60]}...\")\n",
    "    print(f\"       Rejected: {pair['rejected'][:60]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reward-model-heading",
   "metadata": {},
   "source": [
    "## Reward Model Architecture\n",
    "\n",
    "A reward model takes a language model and adds a **scalar output head**.\n",
    "Instead of predicting the next token (a distribution over the vocabulary),\n",
    "it outputs a single number: a \"reward\" score indicating how good the\n",
    "response is.\n",
    "\n",
    "The architecture:\n",
    "1. Take a pretrained language model (SmolLM-135M).\n",
    "2. Remove the language modeling head (the final linear layer that maps\n",
    "   hidden states to vocabulary logits).\n",
    "3. Add a new linear layer that maps the last token's hidden state to a\n",
    "   single scalar.\n",
    "\n",
    "Why the *last* token? In a causal (left-to-right) language model, the\n",
    "last token's hidden state has attended to the entire sequence. It serves\n",
    "as a summary representation of the full prompt + response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-base-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM-135M\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "num_params = sum(p.numel() for p in base_model.parameters())\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Parameters: {num_params:,}\")\n",
    "print(f\"Hidden size: {base_model.config.hidden_size}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reward-model-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(nn.Module):\n",
    "    \"\"\"A reward model built on top of a causal language model.\n",
    "\n",
    "    Takes a pretrained language model, removes the language modeling head,\n",
    "    and adds a linear layer that maps the last token's hidden state to a\n",
    "    scalar reward score.\n",
    "\n",
    "    Args:\n",
    "        base_model: A HuggingFace causal language model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        # Extract the transformer backbone (without the LM head)\n",
    "        self.backbone = base_model.model  # the transformer layers\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "\n",
    "        # Scalar reward head: maps hidden state to a single number\n",
    "        self.reward_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # Initialize the reward head with small weights\n",
    "        nn.init.normal_(self.reward_head.weight, std=0.02)\n",
    "        nn.init.zeros_(self.reward_head.bias)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \"\"\"Compute a scalar reward for the input sequence.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Token IDs, shape (batch_size, seq_len).\n",
    "            attention_mask: Mask for padding tokens, shape (batch_size, seq_len).\n",
    "\n",
    "        Returns:\n",
    "            Scalar reward for each sequence, shape (batch_size,).\n",
    "        \"\"\"\n",
    "        # Get hidden states from the transformer backbone\n",
    "        outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        hidden_states = outputs.last_hidden_state  # (batch, seq_len, hidden_size)\n",
    "\n",
    "        # Find the index of the last real token (not padding) in each sequence\n",
    "        if attention_mask is not None:\n",
    "            # Sum the attention mask to find sequence lengths, then subtract 1\n",
    "            last_token_idx = attention_mask.sum(dim=1) - 1  # (batch_size,)\n",
    "        else:\n",
    "            last_token_idx = torch.full(\n",
    "                (input_ids.size(0),), input_ids.size(1) - 1, dtype=torch.long\n",
    "            )\n",
    "\n",
    "        # Gather the last token's hidden state for each sequence\n",
    "        batch_indices = torch.arange(input_ids.size(0), device=input_ids.device)\n",
    "        last_hidden = hidden_states[batch_indices, last_token_idx]  # (batch, hidden_size)\n",
    "\n",
    "        # Pass through the reward head to get a scalar\n",
    "        reward = self.reward_head(last_hidden).squeeze(-1)  # (batch_size,)\n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "# Instantiate the reward model\n",
    "reward_model = RewardModel(copy.deepcopy(base_model))\n",
    "\n",
    "total_params = sum(p.numel() for p in reward_model.parameters())\n",
    "head_params = sum(p.numel() for p in reward_model.reward_head.parameters())\n",
    "print(f\"Reward model parameters: {total_params:,}\")\n",
    "print(f\"  Backbone: {total_params - head_params:,}\")\n",
    "print(f\"  Reward head: {head_params:,}\")\n",
    "print()\n",
    "\n",
    "# Quick test: pass a dummy input through the model\n",
    "dummy_input = tokenizer(\"This is a test.\", return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    dummy_reward = reward_model(\n",
    "        input_ids=dummy_input[\"input_ids\"],\n",
    "        attention_mask=dummy_input[\"attention_mask\"],\n",
    "    )\n",
    "print(f\"Dummy reward: {dummy_reward.item():.4f}\")\n",
    "print(f\"Reward shape: {dummy_reward.shape}\")\n",
    "print(\"The reward model outputs a single scalar per input sequence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bradley-terry-heading",
   "metadata": {},
   "source": [
    "## Bradley-Terry Model\n",
    "\n",
    "The **Bradley-Terry model** is the loss function used to train reward\n",
    "models. The idea is simple: given a chosen response and a rejected\n",
    "response, the reward model should assign a higher score to the chosen one.\n",
    "\n",
    "Formally, the probability that response $y_w$ (chosen/winner) is preferred\n",
    "over response $y_l$ (rejected/loser) is:\n",
    "\n",
    "$$P(y_w \\succ y_l) = \\sigma(r(y_w) - r(y_l))$$\n",
    "\n",
    "where $\\sigma$ is the sigmoid function and $r$ is the reward model.\n",
    "\n",
    "The loss is the negative log-likelihood:\n",
    "\n",
    "$$\\mathcal{L} = -\\log \\sigma(r(y_w) - r(y_l))$$\n",
    "\n",
    "This loss is minimized when $r(y_w) \\gg r(y_l)$ -- when the reward model\n",
    "strongly prefers the chosen response. It goes to infinity when the model\n",
    "incorrectly prefers the rejected response ($r(y_l) > r(y_w)$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bradley-terry-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bradley_terry_loss(reward_chosen, reward_rejected):\n",
    "    \"\"\"Compute the Bradley-Terry preference loss.\n",
    "\n",
    "    The loss encourages the reward model to assign a higher score to the\n",
    "    chosen response than the rejected response.\n",
    "\n",
    "    L = -log(sigmoid(r_chosen - r_rejected))\n",
    "\n",
    "    Args:\n",
    "        reward_chosen: Scalar rewards for chosen responses, shape (batch,).\n",
    "        reward_rejected: Scalar rewards for rejected responses, shape (batch,).\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss (mean over batch).\n",
    "    \"\"\"\n",
    "    return -F.logsigmoid(reward_chosen - reward_rejected).mean()\n",
    "\n",
    "\n",
    "# Demonstrate the loss behavior\n",
    "print(\"Bradley-Terry loss for different reward gaps:\")\n",
    "print(f\"{'r_chosen - r_rejected':>25}  {'Loss':>10}  {'P(chosen > rejected)':>22}\")\n",
    "print(\"-\" * 62)\n",
    "\n",
    "for diff in [-3.0, -2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0, 3.0, 5.0]:\n",
    "    r_c = torch.tensor([diff])\n",
    "    r_r = torch.tensor([0.0])\n",
    "    loss = bradley_terry_loss(r_c, r_r)\n",
    "    prob = torch.sigmoid(r_c - r_r).item()\n",
    "    print(f\"{diff:>25.1f}  {loss.item():>10.4f}  {prob:>22.4f}\")\n",
    "\n",
    "print()\n",
    "print(\"When the gap is negative (model prefers rejected), loss is high.\")\n",
    "print(\"When the gap is positive (model prefers chosen), loss is low.\")\n",
    "print(\"The loss asymptotically approaches 0 as the gap grows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-heading",
   "metadata": {},
   "source": [
    "## Training the Reward Model\n",
    "\n",
    "We now train the reward model on our preference dataset. The training\n",
    "loop:\n",
    "\n",
    "1. For each preference pair, tokenize the prompt + chosen and\n",
    "   prompt + rejected responses.\n",
    "2. Pass both through the reward model to get scalar rewards.\n",
    "3. Compute the Bradley-Terry loss.\n",
    "4. Backpropagate and update parameters.\n",
    "\n",
    "With only 13 preference pairs and a 135M parameter model, this trains\n",
    "quickly on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-training-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_for_reward(prompt, response, tokenizer, max_length=256):\n",
    "    \"\"\"Tokenize a prompt + response pair for the reward model.\n",
    "\n",
    "    Concatenates the prompt and response with a separator, then tokenizes.\n",
    "\n",
    "    Args:\n",
    "        prompt: The input prompt string.\n",
    "        response: The response string.\n",
    "        tokenizer: A HuggingFace tokenizer.\n",
    "        max_length: Maximum sequence length.\n",
    "\n",
    "    Returns:\n",
    "        Dict with 'input_ids' and 'attention_mask' tensors.\n",
    "    \"\"\"\n",
    "    text = f\"Question: {prompt}\\nAnswer: {response}\"\n",
    "    encoded = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return encoded\n",
    "\n",
    "\n",
    "# Prepare all tokenized pairs\n",
    "tokenized_chosen = []\n",
    "tokenized_rejected = []\n",
    "\n",
    "for pair in preference_data:\n",
    "    chosen_enc = tokenize_for_reward(pair[\"prompt\"], pair[\"chosen\"], tokenizer)\n",
    "    rejected_enc = tokenize_for_reward(pair[\"prompt\"], pair[\"rejected\"], tokenizer)\n",
    "    tokenized_chosen.append(chosen_enc)\n",
    "    tokenized_rejected.append(rejected_enc)\n",
    "\n",
    "print(f\"Prepared {len(tokenized_chosen)} tokenized preference pairs\")\n",
    "print(f\"Sequence length: {tokenized_chosen[0]['input_ids'].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-reward-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 15\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.AdamW(reward_model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Epochs: {num_epochs}\")\n",
    "print(f\"  Pairs per epoch: {len(preference_data)}\")\n",
    "print(f\"  Total steps: {num_epochs * len(preference_data)}\")\n",
    "print()\n",
    "print(\"Starting reward model training...\")\n",
    "print()\n",
    "\n",
    "loss_history = []\n",
    "accuracy_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    epoch_correct = 0\n",
    "    reward_model.train()\n",
    "\n",
    "    # Shuffle training order each epoch\n",
    "    indices = torch.randperm(len(preference_data)).tolist()\n",
    "\n",
    "    for idx in indices:\n",
    "        chosen_enc = tokenized_chosen[idx]\n",
    "        rejected_enc = tokenized_rejected[idx]\n",
    "\n",
    "        # Forward pass: compute rewards for both responses\n",
    "        r_chosen = reward_model(\n",
    "            input_ids=chosen_enc[\"input_ids\"],\n",
    "            attention_mask=chosen_enc[\"attention_mask\"],\n",
    "        )\n",
    "        r_rejected = reward_model(\n",
    "            input_ids=rejected_enc[\"input_ids\"],\n",
    "            attention_mask=rejected_enc[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "        # Compute Bradley-Terry loss\n",
    "        loss = bradley_terry_loss(r_chosen, r_rejected)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        # Track accuracy: is r_chosen > r_rejected?\n",
    "        if r_chosen.item() > r_rejected.item():\n",
    "            epoch_correct += 1\n",
    "\n",
    "    avg_loss = np.mean(epoch_losses)\n",
    "    accuracy = epoch_correct / len(preference_data)\n",
    "    accuracy_history.append(accuracy)\n",
    "\n",
    "    if (epoch + 1) % 3 == 0 or epoch == 0:\n",
    "        print(\n",
    "            f\"  Epoch {epoch + 1:>2}/{num_epochs}  \"\n",
    "            f\"Loss: {avg_loss:.4f}  \"\n",
    "            f\"Accuracy: {accuracy:.1%}\"\n",
    "        )\n",
    "\n",
    "print()\n",
    "print(f\"Training complete.\")\n",
    "print(f\"Final loss: {loss_history[-1]:.4f}\")\n",
    "print(f\"Final accuracy: {accuracy_history[-1]:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss curve\n",
    "ax = axes[0]\n",
    "ax.plot(loss_history, color=\"steelblue\", alpha=0.6, linewidth=1)\n",
    "# Smoothed loss\n",
    "window = max(3, len(loss_history) // 15)\n",
    "if len(loss_history) >= window:\n",
    "    smoothed = np.convolve(loss_history, np.ones(window) / window, mode=\"valid\")\n",
    "    ax.plot(\n",
    "        range(window - 1, len(loss_history)),\n",
    "        smoothed,\n",
    "        color=\"darkblue\",\n",
    "        linewidth=2,\n",
    "        label=\"Smoothed\",\n",
    "    )\n",
    "ax.set_xlabel(\"Training step\")\n",
    "ax.set_ylabel(\"Bradley-Terry Loss\")\n",
    "ax.set_title(\"Reward Model Training Loss\")\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Accuracy per epoch\n",
    "ax = axes[1]\n",
    "ax.plot(\n",
    "    range(1, num_epochs + 1),\n",
    "    accuracy_history,\n",
    "    marker=\"o\",\n",
    "    color=\"#2ecc71\",\n",
    "    linewidth=2,\n",
    "    markersize=6,\n",
    ")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Preference Pair Accuracy (r_chosen > r_rejected)\")\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.axhline(y=0.5, color=\"red\", linestyle=\"--\", alpha=0.5, label=\"Random baseline\")\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left: The Bradley-Terry loss should decrease over training.\")\n",
    "print(\"Right: Accuracy should rise above 50% (random) and ideally reach 100%.\")\n",
    "print(\"100% accuracy means the model correctly ranks chosen > rejected for all pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "score-training-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show reward scores for all training pairs\n",
    "reward_model.eval()\n",
    "\n",
    "print(\"Reward scores on training data:\")\n",
    "print(\"=\" * 75)\n",
    "print(f\"{'Pair':>4}  {'r_chosen':>10}  {'r_rejected':>10}  {'Gap':>8}  {'Correct':>8}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "all_correct = 0\n",
    "for i, pair in enumerate(preference_data):\n",
    "    chosen_enc = tokenized_chosen[i]\n",
    "    rejected_enc = tokenized_rejected[i]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        r_c = reward_model(\n",
    "            input_ids=chosen_enc[\"input_ids\"],\n",
    "            attention_mask=chosen_enc[\"attention_mask\"],\n",
    "        ).item()\n",
    "        r_r = reward_model(\n",
    "            input_ids=rejected_enc[\"input_ids\"],\n",
    "            attention_mask=rejected_enc[\"attention_mask\"],\n",
    "        ).item()\n",
    "\n",
    "    correct = r_c > r_r\n",
    "    all_correct += int(correct)\n",
    "    status = \"YES\" if correct else \"NO\"\n",
    "    print(f\"{i:>4}  {r_c:>10.4f}  {r_r:>10.4f}  {r_c - r_r:>8.4f}  {status:>8}\")\n",
    "\n",
    "print(\"-\" * 75)\n",
    "print(f\"Overall accuracy: {all_correct}/{len(preference_data)} ({all_correct/len(preference_data):.1%})\")\n",
    "print()\n",
    "print(\"A positive gap means the model correctly prefers the chosen response.\")\n",
    "print(\"A negative gap means the model incorrectly prefers the rejected response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-new-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on NEW examples not in the training set\n",
    "test_pairs = [\n",
    "    {\n",
    "        \"prompt\": \"What is the hearsay rule?\",\n",
    "        \"chosen\": (\n",
    "            \"Hearsay is an out-of-court statement offered to prove the truth \"\n",
    "            \"of the matter asserted, generally inadmissible under Federal \"\n",
    "            \"Rule of Evidence 802. However, numerous exceptions exist under \"\n",
    "            \"FRE 803 and 804, including present sense impressions, excited \"\n",
    "            \"utterances, and statements against interest.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"The hearsay rule means you can never use any statement made \"\n",
    "            \"outside the courtroom. There are absolutely no exceptions to \"\n",
    "            \"this rule. If someone said something outside of court, it \"\n",
    "            \"cannot be mentioned during trial under any circumstances.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Can I represent myself in court?\",\n",
    "        \"chosen\": (\n",
    "            \"Yes, under the Sixth Amendment and Faretta v. California, 422 \"\n",
    "            \"U.S. 806 (1975), criminal defendants have the right to self-\"\n",
    "            \"representation. However, the court will typically conduct a \"\n",
    "            \"colloquy to ensure the decision is knowing, intelligent, and \"\n",
    "            \"voluntary. Pro se litigants are held to the same rules and \"\n",
    "            \"standards as attorneys. Self-representation is generally \"\n",
    "            \"discouraged due to the complexity of legal procedures.\"\n",
    "        ),\n",
    "        \"rejected\": (\n",
    "            \"You should absolutely represent yourself. Courts are very \"\n",
    "            \"lenient with pro se litigants and will basically guide you \"\n",
    "            \"through the entire process. Judges are required to help you \"\n",
    "            \"win your case if you don't have a lawyer. It saves money and \"\n",
    "            \"the outcome is always just as good.\"\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Reward scores on NEW (unseen) examples:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, pair in enumerate(test_pairs):\n",
    "    chosen_enc = tokenize_for_reward(pair[\"prompt\"], pair[\"chosen\"], tokenizer)\n",
    "    rejected_enc = tokenize_for_reward(pair[\"prompt\"], pair[\"rejected\"], tokenizer)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        r_c = reward_model(\n",
    "            input_ids=chosen_enc[\"input_ids\"],\n",
    "            attention_mask=chosen_enc[\"attention_mask\"],\n",
    "        ).item()\n",
    "        r_r = reward_model(\n",
    "            input_ids=rejected_enc[\"input_ids\"],\n",
    "            attention_mask=rejected_enc[\"attention_mask\"],\n",
    "        ).item()\n",
    "\n",
    "    correct = r_c > r_r\n",
    "    status = \"CORRECT\" if correct else \"WRONG\"\n",
    "    print(f\"\\n  Prompt: {pair['prompt']}\")\n",
    "    print(f\"  r_chosen:   {r_c:.4f}\")\n",
    "    print(f\"  r_rejected: {r_r:.4f}\")\n",
    "    print(f\"  Gap: {r_c - r_r:.4f}  [{status}]\")\n",
    "\n",
    "print()\n",
    "print(\"The reward model should generalize: preferring accurate, hedged\")\n",
    "print(\"responses over overconfident or incorrect ones, even for new prompts.\")\n",
    "print(\"With only 13 training pairs, generalization will be limited but\")\n",
    "print(\"the model may pick up on surface-level quality signals like citations\")\n",
    "print(\"and hedging language.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ppo-heading",
   "metadata": {},
   "source": [
    "## PPO Overview\n",
    "\n",
    "With a trained reward model, the next step in full RLHF is **Proximal\n",
    "Policy Optimization (PPO)**. PPO uses the reward model to optimize the\n",
    "language model (the \"policy\") to generate higher-reward responses.\n",
    "\n",
    "We do **not** implement PPO here -- it is compute-intensive and requires\n",
    "careful engineering (value functions, advantage estimation, multiple\n",
    "forward passes per step). Instead, we explain the algorithm conceptually.\n",
    "\n",
    "### The PPO Loop\n",
    "\n",
    "```\n",
    "# Setup\n",
    "policy = load_sft_model()           # The model we want to align\n",
    "reference_policy = freeze(policy)    # Frozen copy to prevent drift\n",
    "reward_model = load_reward_model()   # Trained reward model\n",
    "\n",
    "for batch in prompts:\n",
    "    # Step 1: Generate responses from the current policy\n",
    "    responses = policy.generate(batch)\n",
    "\n",
    "    # Step 2: Score responses with the reward model\n",
    "    rewards = reward_model.score(responses)\n",
    "\n",
    "    # Step 3: Compute KL penalty to prevent reward hacking\n",
    "    # The policy should not drift too far from the reference\n",
    "    kl_penalty = compute_kl(policy, reference_policy, responses)\n",
    "\n",
    "    # Step 4: Compute advantages (reward minus penalty)\n",
    "    advantages = rewards - beta * kl_penalty\n",
    "\n",
    "    # Step 5: Update policy using PPO clipped objective\n",
    "    # The clipped objective prevents too-large updates:\n",
    "    # L = min(ratio * advantage, clip(ratio, 1-eps, 1+eps) * advantage)\n",
    "    # where ratio = pi_new(a|s) / pi_old(a|s)\n",
    "    policy.update(advantages)\n",
    "```\n",
    "\n",
    "### Why the KL Penalty?\n",
    "\n",
    "Without a KL penalty, the policy can learn to exploit the reward model.\n",
    "For example, if the reward model gives high scores to responses with\n",
    "many legal citations, the policy might learn to insert citations into\n",
    "every response -- even when they are irrelevant or fabricated. This is\n",
    "called **reward hacking**.\n",
    "\n",
    "The KL divergence penalty keeps the aligned policy close to the reference\n",
    "policy (the SFT model). It says: \"improve the response, but don't change\n",
    "the model's behavior so much that it becomes degenerate.\" The beta\n",
    "parameter controls the strength of this constraint.\n",
    "\n",
    "### Why Not Implement PPO?\n",
    "\n",
    "PPO requires:\n",
    "- Multiple forward passes per training step (generation + scoring + update)\n",
    "- A value function (critic) trained alongside the policy\n",
    "- Careful hyperparameter tuning (clipping range, GAE lambda, etc.)\n",
    "- Significant compute (impractical on CPU with a 135M model)\n",
    "\n",
    "In the next notebook, we implement **DPO (Direct Preference Optimization)**\n",
    "as a simpler alternative that achieves similar results without a reward\n",
    "model or PPO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verifiable-rewards-heading",
   "metadata": {},
   "source": [
    "## Verifiable Rewards\n",
    "\n",
    "For tasks with **ground truth**, you do not need a learned reward model.\n",
    "Instead, you can compute a reward directly by checking the response\n",
    "against known facts. This is simpler, more reliable, and immune to\n",
    "reward hacking.\n",
    "\n",
    "For legal tasks, citation accuracy is a natural verifiable reward: we\n",
    "can check whether a cited case actually exists in our corpus of known\n",
    "real cases. A response that cites real cases gets a higher reward; one\n",
    "that cites fabricated cases gets a lower reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verifiable-reward",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A corpus of known real legal citations.\n",
    "# In production, this would be a database of all known case citations.\n",
    "KNOWN_CITATIONS = {\n",
    "    \"Celotex Corp. v. Catrett, 477 U.S. 317 (1986)\",\n",
    "    \"Anderson v. Liberty Lobby, Inc., 477 U.S. 242 (1986)\",\n",
    "    \"Matsushita Elec. Indus. Co. v. Zenith Radio Corp., 475 U.S. 574 (1986)\",\n",
    "    \"Miranda v. Arizona, 384 U.S. 436 (1966)\",\n",
    "    \"Harlow v. Fitzgerald, 457 U.S. 800 (1982)\",\n",
    "    \"Ashcroft v. Iqbal, 556 U.S. 662 (2009)\",\n",
    "    \"Bell Atlantic Corp. v. Twombly, 550 U.S. 544 (2007)\",\n",
    "    \"Katz v. United States, 389 U.S. 347 (1967)\",\n",
    "    \"Mapp v. Ohio, 367 U.S. 643 (1961)\",\n",
    "    \"Anderson v. City of Bessemer City, 470 U.S. 564 (1985)\",\n",
    "    \"Faretta v. California, 422 U.S. 806 (1975)\",\n",
    "    \"Hadley v. Baxendale (1854)\",\n",
    "    \"McDonnell Douglas Corp. v. Green, 411 U.S. 792 (1973)\",\n",
    "    \"Terry v. Ohio, 392 U.S. 1 (1968)\",\n",
    "    \"Gideon v. Wainwright, 372 U.S. 335 (1963)\",\n",
    "    \"Marbury v. Madison, 5 U.S. 137 (1803)\",\n",
    "    \"Brown v. Board of Education, 347 U.S. 483 (1954)\",\n",
    "    \"Roe v. Wade, 410 U.S. 113 (1973)\",\n",
    "}\n",
    "\n",
    "\n",
    "def extract_citations(text):\n",
    "    \"\"\"Extract legal citations from text using regex.\n",
    "\n",
    "    Matches patterns like:\n",
    "    - Name v. Name, NNN U.S. NNN (YYYY)\n",
    "    - Name v. Name, NNN F.Supp. NNN (Dist. YYYY)\n",
    "    - Name v. Name (YYYY)\n",
    "\n",
    "    Returns:\n",
    "        List of extracted citation strings.\n",
    "    \"\"\"\n",
    "    # Match standard US case citations\n",
    "    pattern = (\n",
    "        r'[A-Z][a-zA-Z.\\s]+v\\.\\s+[A-Z][a-zA-Z.\\s,]+'\n",
    "        r'\\d+\\s+(?:U\\.S\\.|F\\.\\d+[a-z]*|F\\.Supp\\.|S\\.Ct\\.)\\s+\\d+\\s*\\([^)]+\\)'\n",
    "    )\n",
    "    citations = re.findall(pattern, text)\n",
    "\n",
    "    # Also match simpler format: Name v. Name (YYYY)\n",
    "    simple_pattern = r'[A-Z][a-zA-Z.\\s]+v\\.\\s+[A-Z][a-zA-Z.\\s]+\\(\\d{4}\\)'\n",
    "    simple_citations = re.findall(simple_pattern, text)\n",
    "\n",
    "    all_citations = list(set(citations + simple_citations))\n",
    "    return [c.strip() for c in all_citations]\n",
    "\n",
    "\n",
    "def citation_accuracy_reward(response, known_citations=KNOWN_CITATIONS):\n",
    "    \"\"\"Compute a verifiable reward based on citation accuracy.\n",
    "\n",
    "    Checks each citation in the response against a corpus of known real\n",
    "    citations. Returns a reward between -1 and 1:\n",
    "    - +1: all citations are real\n",
    "    - 0: no citations found\n",
    "    - negative: some citations are fabricated\n",
    "\n",
    "    Args:\n",
    "        response: The model's response text.\n",
    "        known_citations: Set of known real citation strings.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (reward, details_dict).\n",
    "    \"\"\"\n",
    "    extracted = extract_citations(response)\n",
    "\n",
    "    if not extracted:\n",
    "        return 0.0, {\"extracted\": [], \"real\": [], \"fabricated\": []}\n",
    "\n",
    "    # Check each citation against the known corpus.\n",
    "    # Use fuzzy matching: check if the case name appears in any known citation.\n",
    "    real = []\n",
    "    fabricated = []\n",
    "\n",
    "    for cite in extracted:\n",
    "        # Extract the case name (the \"v.\" part)\n",
    "        match = re.search(r'([A-Z][a-zA-Z.\\s]+v\\.\\s+[A-Z][a-zA-Z.\\s]+)', cite)\n",
    "        if match:\n",
    "            case_name = match.group(1).strip()\n",
    "            # Check if this case name appears in any known citation\n",
    "            found = any(case_name in known for known in known_citations)\n",
    "            if found:\n",
    "                real.append(cite)\n",
    "            else:\n",
    "                fabricated.append(cite)\n",
    "        else:\n",
    "            fabricated.append(cite)\n",
    "\n",
    "    # Reward: fraction of real citations minus fraction of fabricated ones\n",
    "    total = len(extracted)\n",
    "    reward = (len(real) - len(fabricated)) / total\n",
    "\n",
    "    return reward, {\n",
    "        \"extracted\": extracted,\n",
    "        \"real\": real,\n",
    "        \"fabricated\": fabricated,\n",
    "    }\n",
    "\n",
    "\n",
    "# Test on our preference data\n",
    "print(\"Verifiable Reward: Citation Accuracy\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, pair in enumerate(preference_data[:5]):\n",
    "    r_chosen, details_chosen = citation_accuracy_reward(pair[\"chosen\"])\n",
    "    r_rejected, details_rejected = citation_accuracy_reward(pair[\"rejected\"])\n",
    "\n",
    "    print(f\"\\nPair {i}: {pair['prompt'][:60]}...\")\n",
    "    print(f\"  Chosen reward:   {r_chosen:+.2f}  \"\n",
    "          f\"(real: {len(details_chosen['real'])}, \"\n",
    "          f\"fabricated: {len(details_chosen['fabricated'])})\")\n",
    "    print(f\"  Rejected reward: {r_rejected:+.2f}  \"\n",
    "          f\"(real: {len(details_rejected['real'])}, \"\n",
    "          f\"fabricated: {len(details_rejected['fabricated'])})\")\n",
    "    if details_chosen[\"fabricated\"]:\n",
    "        print(f\"    Fabricated in chosen: {details_chosen['fabricated']}\")\n",
    "    if details_rejected[\"fabricated\"]:\n",
    "        print(f\"    Fabricated in rejected: {details_rejected['fabricated']}\")\n",
    "\n",
    "print()\n",
    "print(\"Advantages of verifiable rewards over learned reward models:\")\n",
    "print(\"  - No training required -- just a lookup.\")\n",
    "print(\"  - Immune to reward hacking -- the reward directly measures truth.\")\n",
    "print(\"  - Perfectly interpretable -- you know exactly why the score is what it is.\")\n",
    "print(\"  - Limited scope -- only works for tasks with ground truth (like citations).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises-heading",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise (a): Verifiable Reward for Case Existence\n",
    "\n",
    "Implement a more robust verifiable reward that checks whether cited cases\n",
    "actually exist in a corpus. Extend the `citation_accuracy_reward` function\n",
    "to also check:\n",
    "\n",
    "- Whether the volume and page numbers are plausible (e.g., U.S. Reports\n",
    "  volumes range from 1 to ~600)\n",
    "- Whether the year is plausible (not in the future, not before 1789)\n",
    "- Whether the reporter matches the court (e.g., U.S. Reports for Supreme\n",
    "  Court cases, F.3d for circuit courts)\n",
    "\n",
    "```python\n",
    "def enhanced_citation_reward(response, known_citations=KNOWN_CITATIONS):\n",
    "    \"\"\"Enhanced verifiable reward with structural checks.\"\"\"\n",
    "    extracted = extract_citations(response)\n",
    "    if not extracted:\n",
    "        return 0.0, {}\n",
    "\n",
    "    for cite in extracted:\n",
    "        # Check 1: Does the case name match a known case?\n",
    "        # Check 2: Is the volume number plausible?\n",
    "        # Check 3: Is the year plausible?\n",
    "        # Check 4: Does the reporter match the court level?\n",
    "        pass  # Your implementation here\n",
    "\n",
    "    return reward, details\n",
    "```\n",
    "\n",
    "### Exercise (b): Reward Model Failure Modes\n",
    "\n",
    "The reward model is trained on a small dataset and may have blind spots.\n",
    "Analyze its failure modes:\n",
    "\n",
    "1. Create 5 adversarial examples where the \"bad\" response has surface-level\n",
    "   quality signals (long length, formal language, citations) but is\n",
    "   substantively wrong. Does the reward model get fooled?\n",
    "\n",
    "2. Create 5 examples where the \"good\" response is short and informal\n",
    "   (but correct). Does the reward model penalize brevity?\n",
    "\n",
    "3. Test whether the reward model has learned to detect fabricated citations\n",
    "   specifically, or just learned surface-level patterns.\n",
    "\n",
    "```python\n",
    "# Example adversarial test: formal but wrong\n",
    "adversarial_pairs = [\n",
    "    {\n",
    "        \"prompt\": \"What is the burden of proof in a civil case?\",\n",
    "        \"chosen\": \"Preponderance of the evidence.\",  # Short but correct\n",
    "        \"rejected\": (\n",
    "            \"Pursuant to the well-established jurisprudence of the United \"\n",
    "            \"States Supreme Court, the burden of proof in civil proceedings \"\n",
    "            \"is beyond a reasonable doubt, the same standard applied in \"\n",
    "            \"criminal cases.\"  # Formal but WRONG\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "# Score with the reward model\n",
    "for pair in adversarial_pairs:\n",
    "    c_enc = tokenize_for_reward(pair[\"prompt\"], pair[\"chosen\"], tokenizer)\n",
    "    r_enc = tokenize_for_reward(pair[\"prompt\"], pair[\"rejected\"], tokenizer)\n",
    "    with torch.no_grad():\n",
    "        r_c = reward_model(c_enc[\"input_ids\"], c_enc[\"attention_mask\"]).item()\n",
    "        r_r = reward_model(r_enc[\"input_ids\"], r_enc[\"attention_mask\"]).item()\n",
    "    print(f\"r_chosen={r_c:.4f}, r_rejected={r_r:.4f}, correct={r_c > r_r}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
