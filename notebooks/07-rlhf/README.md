# Module 07: RLHF and Alignment

Build a preference dataset, train a reward model, implement DPO. Compare aligned vs unaligned outputs. By the end of this module, you will have hands-on experience with the full alignment pipeline — from collecting human preferences to training a reward model to applying Direct Preference Optimization — and understand how these techniques shape model behavior.

**Prerequisites:** Module 06.
