{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# 01 - QLoRA Fine-Tuning Lab\n",
    "\n",
    "## Overview\n",
    "\n",
    "This is a self-guided lab for fine-tuning a 7B-parameter language model on\n",
    "legal data using QLoRA (Quantized Low-Rank Adaptation). You will work through\n",
    "five checkpoints at your own pace. Each checkpoint builds on the previous one,\n",
    "but you can stop at any point and resume later.\n",
    "\n",
    "**What is QLoRA?** QLoRA combines two techniques:\n",
    "1. **4-bit quantization** -- compress the model weights from 16-bit to 4-bit,\n",
    "   reducing GPU memory by roughly 4x.\n",
    "2. **LoRA adapters** -- instead of updating all model parameters, add small\n",
    "   trainable matrices to specific layers. These adapters typically represent\n",
    "   less than 1% of the total parameter count.\n",
    "\n",
    "The result: you can fine-tune a 7B model on a single consumer GPU (16 GB VRAM)\n",
    "that would otherwise require 4x A100s for full fine-tuning.\n",
    "\n",
    "**Prerequisites:** Modules 06 (SFT) and 07 (RLHF/alignment). Module 08\n",
    "(evaluation) is recommended for Checkpoint 4.\n",
    "\n",
    "---\n",
    "\n",
    "**GPU REQUIREMENT**: This notebook requires an NVIDIA GPU with at least\n",
    "16 GB VRAM (e.g., RTX 4090, A100, or equivalent cloud GPU). It will not\n",
    "run on CPU-only machines or Apple Silicon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint-1-heading",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint 1: Setup and Quantization\n",
    "\n",
    "### Why quantize?\n",
    "\n",
    "A 7B-parameter model in float16 requires approximately 14 GB of GPU memory\n",
    "just for the weights (7 billion parameters x 2 bytes each). That leaves\n",
    "almost no room for optimizer states, gradients, or activations during training.\n",
    "\n",
    "4-bit quantization reduces the weight memory to roughly 3.5 GB (7B x 0.5 bytes),\n",
    "freeing up space for the training overhead. The key insight behind QLoRA is that\n",
    "you can quantize the base model for memory efficiency while keeping the small\n",
    "LoRA adapter weights in full precision for training stability.\n",
    "\n",
    "### Quantization types: NF4 vs FP4\n",
    "\n",
    "Two common 4-bit formats exist:\n",
    "\n",
    "- **FP4 (4-bit floating point)**: A uniform quantization grid. Works well for\n",
    "  uniformly distributed values, but neural network weights are not uniform --\n",
    "  they tend to follow a normal distribution centered around zero.\n",
    "\n",
    "- **NF4 (4-bit NormalFloat)**: Designed specifically for normally-distributed\n",
    "  weights. The quantization levels are spaced according to a normal distribution,\n",
    "  placing more precision near zero where most weights cluster. This yields better\n",
    "  accuracy than FP4 for transformer models.\n",
    "\n",
    "We use NF4 because pretrained transformer weights are approximately normally\n",
    "distributed, making NF4 the better choice for preserving model quality.\n",
    "\n",
    "### Double quantization\n",
    "\n",
    "Standard quantization stores a scaling factor for each block of weights\n",
    "(typically 64 weights per block). These scaling factors themselves consume\n",
    "memory. Double quantization applies a second round of quantization to the\n",
    "scaling factors, saving an additional ~0.4 GB for a 7B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-gpu",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise RuntimeError(\n",
    "        \"This notebook requires an NVIDIA GPU with CUDA support. \"\n",
    "        \"No CUDA device was detected. If you are on a cloud instance, \"\n",
    "        \"make sure you selected a GPU runtime.\"\n",
    "    )\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "gpu_name = torch.cuda.get_device_name(0)\n",
    "gpu_mem_gb = torch.cuda.get_device_properties(0).total_mem / 1e9\n",
    "\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "print(f\"Total VRAM: {gpu_mem_gb:.1f} GB\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "if gpu_mem_gb < 15:\n",
    "    print(\n",
    "        f\"\\nWARNING: Your GPU has {gpu_mem_gb:.1f} GB VRAM. \"\n",
    "        f\"This lab recommends at least 16 GB. You may encounter \"\n",
    "        f\"out-of-memory errors during training.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install transformers peft bitsandbytes trl datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gpu-memory-before",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gpu_memory(label=\"\"):\n",
    "    \"\"\"Print current GPU memory usage.\"\"\"\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    total = torch.cuda.get_device_properties(0).total_mem / 1e9\n",
    "    print(f\"[{label}] GPU Memory: {allocated:.2f} GB allocated, \"\n",
    "          f\"{reserved:.2f} GB reserved, {total:.1f} GB total\")\n",
    "\n",
    "\n",
    "print_gpu_memory(\"Before loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-quantized-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your model -- Mistral-7B-v0.1 is a strong open base model.\n",
    "# You can also use \"meta-llama/Llama-2-7b-hf\" (requires Llama 2 license acceptance).\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "# Configure 4-bit quantization with NF4\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"Loading {model_name} with 4-bit NF4 quantization...\")\n",
    "print(\"This may take a few minutes on first download.\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(f\"\\nModel loaded successfully.\")\n",
    "print_gpu_memory(\"After loading quantized model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the quantized model structure\n",
    "print(f\"Model type: {type(model).__name__}\")\n",
    "print(f\"Model dtype: {model.dtype}\")\n",
    "print(f\"\\nModel architecture (first few layers):\")\n",
    "\n",
    "# Show the first transformer layer to see quantized linear layers\n",
    "first_layer = model.model.layers[0]\n",
    "print(first_layer)\n",
    "\n",
    "print(f\"\\n--- Memory comparison ---\")\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Estimated fp16 memory: {total_params * 2 / 1e9:.2f} GB\")\n",
    "print(f\"Estimated 4-bit memory: {total_params * 0.5 / 1e9:.2f} GB\")\n",
    "print(f\"Actual GPU usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Memory saved: ~{(total_params * 2 / 1e9) - (torch.cuda.memory_allocated() / 1e9):.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint-1-summary",
   "metadata": {},
   "source": [
    "### Checkpoint 1 Summary\n",
    "\n",
    "You have:\n",
    "- Loaded a 7B model in 4-bit NF4 quantization\n",
    "- Verified GPU memory usage is roughly 3.5-4.5 GB (vs ~14 GB in fp16)\n",
    "- Understood why NF4 is better than FP4 for transformer weights\n",
    "- Seen that double quantization provides additional memory savings\n",
    "\n",
    "You can pause here and resume at Checkpoint 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint-2-heading",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint 2: LoRA Configuration\n",
    "\n",
    "### What is LoRA?\n",
    "\n",
    "Full fine-tuning updates every parameter in the model. For a 7B model, that\n",
    "means storing optimizer states for 7 billion parameters -- roughly 56 GB of\n",
    "memory with Adam (8 bytes per parameter for momentum and variance).\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)** takes a different approach. Instead of updating\n",
    "the full weight matrix W (shape d x d), LoRA freezes W and adds two small\n",
    "matrices:\n",
    "\n",
    "```\n",
    "W' = W + B @ A\n",
    "```\n",
    "\n",
    "where A has shape (d, r) and B has shape (r, d), with r << d (e.g., r=16\n",
    "vs d=4096). The product B @ A is a low-rank approximation of the weight update.\n",
    "\n",
    "Only A and B are trainable. For r=16 and d=4096:\n",
    "- Full weight matrix: 4096 x 4096 = 16.7M parameters\n",
    "- LoRA matrices: (4096 x 16) + (16 x 4096) = 131K parameters\n",
    "- Reduction: **128x fewer** trainable parameters per layer\n",
    "\n",
    "### Key hyperparameters\n",
    "\n",
    "- **r (rank)**: The inner dimension of the A and B matrices. Higher rank\n",
    "  means more capacity but more parameters. Typical values: 8, 16, 32, 64.\n",
    "\n",
    "- **lora_alpha**: A scaling factor applied to the LoRA update. The effective\n",
    "  learning rate for LoRA is scaled by (alpha / r). Common practice: set\n",
    "  alpha = 2 * r.\n",
    "\n",
    "- **target_modules**: Which layers to add adapters to. Attention projections\n",
    "  (q_proj, k_proj, v_proj, o_proj) are the standard choice. Adding MLP layers\n",
    "  (gate_proj, up_proj, down_proj) increases capacity but also parameter count.\n",
    "\n",
    "- **lora_dropout**: Dropout applied to the LoRA path. A small value (0.05)\n",
    "  helps prevent overfitting on small datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-lora",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare the quantized model for training.\n",
    "# This handles gradient checkpointing and layer norm casting.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(\"Model prepared for k-bit training.\")\n",
    "print_gpu_memory(\"After prepare_model_for_kbit_training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configure-lora",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA adapters\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print the trainable parameter summary\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parameter-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed breakdown: trainable vs frozen parameters by module\n",
    "trainable_params = 0\n",
    "frozen_params = 0\n",
    "trainable_by_module = {}\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    num_params = param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += num_params\n",
    "        # Group by module type\n",
    "        module_type = name.split(\".\")[-2] if \".\" in name else name\n",
    "        trainable_by_module[module_type] = (\n",
    "            trainable_by_module.get(module_type, 0) + num_params\n",
    "        )\n",
    "    else:\n",
    "        frozen_params += num_params\n",
    "\n",
    "total_params = trainable_params + frozen_params\n",
    "pct_trainable = 100 * trainable_params / total_params\n",
    "\n",
    "print(\"Parameter Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Frozen parameters:    {frozen_params:>14,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:>14,}\")\n",
    "print(f\"  Total parameters:     {total_params:>14,}\")\n",
    "print(f\"  Trainable:            {pct_trainable:>13.4f}%\")\n",
    "print()\n",
    "print(\"Trainable parameters by module:\")\n",
    "for module, count in sorted(trainable_by_module.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {module:<20} {count:>10,} ({100 * count / trainable_params:.1f}%)\")\n",
    "\n",
    "print(f\"\\n--- Memory comparison for optimizer ---\")\n",
    "print(f\"Full fine-tuning (Adam, 8 bytes/param): {total_params * 8 / 1e9:.1f} GB\")\n",
    "print(f\"LoRA fine-tuning (Adam, 8 bytes/param): {trainable_params * 8 / 1e9:.3f} GB\")\n",
    "print(f\"Optimizer memory reduction: {total_params * 8 / (trainable_params * 8):.0f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-params",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the parameter split\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: total vs trainable\n",
    "ax = axes[0]\n",
    "labels = [\"Frozen (base model)\", \"Trainable (LoRA)\"]\n",
    "sizes = [frozen_params, trainable_params]\n",
    "colors = [\"#95a5a6\", \"#e74c3c\"]\n",
    "ax.pie(\n",
    "    sizes, labels=labels, colors=colors, autopct=\"%1.2f%%\",\n",
    "    startangle=90, textprops={\"fontsize\": 11},\n",
    ")\n",
    "ax.set_title(\"Parameter Distribution\", fontsize=13, fontweight=\"bold\")\n",
    "\n",
    "# Right: trainable parameter breakdown by module\n",
    "ax = axes[1]\n",
    "module_names = list(trainable_by_module.keys())\n",
    "module_counts = list(trainable_by_module.values())\n",
    "bar_colors = plt.cm.Set2(range(len(module_names)))\n",
    "ax.barh(module_names, module_counts, color=bar_colors)\n",
    "ax.set_xlabel(\"Number of Parameters\")\n",
    "ax.set_title(\"Trainable Parameters by Module\", fontsize=13, fontweight=\"bold\")\n",
    "for i, count in enumerate(module_counts):\n",
    "    ax.text(count + max(module_counts) * 0.01, i, f\"{count:,}\", va=\"center\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"LoRA adds only {pct_trainable:.4f}% trainable parameters.\")\n",
    "print(f\"This is what makes fine-tuning a 7B model feasible on a single GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint-2-summary",
   "metadata": {},
   "source": [
    "### Checkpoint 2 Summary\n",
    "\n",
    "You have:\n",
    "- Applied LoRA adapters to the attention layers (q, k, v, o projections)\n",
    "- Verified that trainable parameters are less than 1% of the total\n",
    "- Understood how rank, alpha, and target modules affect adapter size\n",
    "- Seen the optimizer memory savings: LoRA requires orders of magnitude less\n",
    "\n",
    "You can pause here and resume at Checkpoint 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint-3-heading",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint 3: Training\n",
    "\n",
    "We use `trl.SFTTrainer` to fine-tune the model on legal instruction data.\n",
    "SFTTrainer handles the details of supervised fine-tuning: chat template\n",
    "formatting, loss masking (training only on assistant responses), and\n",
    "integration with the PEFT/LoRA setup.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We load the instruction dataset built in Module 06 (or create one from\n",
    "sample data if the Module 06 output is not available). The dataset\n",
    "contains instruction-response pairs derived from court opinions:\n",
    "summarizations, holding extractions, citation identification, and more.\n",
    "\n",
    "### Training configuration\n",
    "\n",
    "Key hyperparameters:\n",
    "- **batch_size=4, gradient_accumulation=4**: effective batch size of 16\n",
    "- **learning_rate=2e-4**: standard for LoRA training (higher than full fine-tuning)\n",
    "- **cosine scheduler with warmup**: gradual warmup prevents early instability\n",
    "- **bf16=True**: mixed precision for faster training on Ampere+ GPUs\n",
    "- **max_seq_length=512**: keeps memory manageable; increase if your data is longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Try to load the dataset from Module 06 first\n",
    "sft_dataset_path = Path(\"../06-sft/sft_dataset.json\")\n",
    "sample_data_path = Path(\"../../datasets/sample/court_opinions.jsonl\")\n",
    "\n",
    "\n",
    "def build_dataset_from_opinions(opinions_path):\n",
    "    \"\"\"Build instruction pairs from court opinions (same approach as Module 06).\"\"\"\n",
    "    opinions = []\n",
    "    with open(opinions_path) as f:\n",
    "        for line in f:\n",
    "            opinions.append(json.loads(line))\n",
    "\n",
    "    dataset_rows = []\n",
    "    for op in opinions:\n",
    "        text = op[\"text\"]\n",
    "        sentences = [s.strip() for s in text.split(\".\") if s.strip()]\n",
    "\n",
    "        # Summarize\n",
    "        summary = \". \".join(sentences[:2]) + \".\" if len(sentences) >= 2 else text[:300]\n",
    "        dataset_rows.append({\n",
    "            \"instruction\": \"Summarize this court opinion.\",\n",
    "            \"input\": text,\n",
    "            \"output\": summary,\n",
    "        })\n",
    "\n",
    "        # Holding\n",
    "        paragraphs = text.split(\"\\n\\n\")\n",
    "        holding = paragraphs[-1].strip() if paragraphs else text[-300:]\n",
    "        dataset_rows.append({\n",
    "            \"instruction\": \"What was the holding in this case?\",\n",
    "            \"input\": text,\n",
    "            \"output\": holding,\n",
    "        })\n",
    "\n",
    "        # Citations\n",
    "        citation_list = \"\\n\".join(f\"- {c}\" for c in op.get(\"citations\", []))\n",
    "        dataset_rows.append({\n",
    "            \"instruction\": \"List the key legal citations in this opinion.\",\n",
    "            \"input\": text,\n",
    "            \"output\": citation_list,\n",
    "        })\n",
    "\n",
    "        # Court\n",
    "        dataset_rows.append({\n",
    "            \"instruction\": \"What court issued this opinion?\",\n",
    "            \"input\": text,\n",
    "            \"output\": op[\"court\"],\n",
    "        })\n",
    "\n",
    "        # Key issues\n",
    "        issue_summary = (\n",
    "            f\"In {op['case_name']}, the key legal issue is: {sentences[0]}.\"\n",
    "            if sentences\n",
    "            else f\"The key issue in {op['case_name']} concerns the matters \"\n",
    "                 f\"described in the opinion.\"\n",
    "        )\n",
    "        dataset_rows.append({\n",
    "            \"instruction\": \"What are the key legal issues in this case?\",\n",
    "            \"input\": text,\n",
    "            \"output\": issue_summary,\n",
    "        })\n",
    "\n",
    "    return dataset_rows\n",
    "\n",
    "\n",
    "# Load or build the dataset\n",
    "if sft_dataset_path.exists():\n",
    "    with open(sft_dataset_path) as f:\n",
    "        raw_dataset = json.load(f)\n",
    "    print(f\"Loaded {len(raw_dataset)} examples from Module 06: {sft_dataset_path}\")\n",
    "elif sample_data_path.exists():\n",
    "    raw_dataset = build_dataset_from_opinions(sample_data_path)\n",
    "    print(f\"Built {len(raw_dataset)} examples from sample court opinions\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        \"No dataset found. Please run Module 06 first or ensure \"\n",
    "        \"datasets/sample/court_opinions.jsonl exists.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Instruction: {raw_dataset[0]['instruction']}\")\n",
    "print(f\"  Input: {raw_dataset[0]['input'][:100]}...\")\n",
    "print(f\"  Output: {raw_dataset[0]['output'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "format-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataset as chat conversations for SFTTrainer\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a legal research assistant. Answer the question about the \"\n",
    "    \"provided court opinion accurately and concisely.\"\n",
    ")\n",
    "\n",
    "\n",
    "def format_as_chat(example):\n",
    "    \"\"\"Convert an instruction example into a chat-format conversation.\"\"\"\n",
    "    user_content = example[\"instruction\"]\n",
    "    if example.get(\"input\"):\n",
    "        user_content += \"\\n\\n\" + example[\"input\"]\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_content},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"output\"]},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "chat_dataset = [format_as_chat(ex) for ex in raw_dataset]\n",
    "train_dataset = Dataset.from_list(chat_dataset)\n",
    "\n",
    "print(f\"Training dataset: {len(train_dataset)} examples\")\n",
    "print(f\"\\nFirst example messages:\")\n",
    "for msg in train_dataset[0][\"messages\"]:\n",
    "    content_preview = msg[\"content\"][:80] + \"...\" if len(msg[\"content\"]) > 80 else msg[\"content\"]\n",
    "    print(f\"  [{msg['role']}]: {content_preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configure-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "output_dir = \"./qlora-legal\"\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=True,\n",
    "    max_seq_length=512,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs:             {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size:         {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Gradient accum:     {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch:    {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate:      {training_args.learning_rate}\")\n",
    "print(f\"  Scheduler:          {training_args.lr_scheduler_type}\")\n",
    "print(f\"  Warmup ratio:       {training_args.warmup_ratio}\")\n",
    "print(f\"  Max seq length:     {training_args.max_seq_length}\")\n",
    "print(f\"  Optimizer:          {training_args.optim}\")\n",
    "print(f\"  Output dir:         {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(f\"Trainer created.\")\n",
    "print(f\"Training steps: {trainer.state.max_steps if hasattr(trainer.state, 'max_steps') else 'will be computed at train time'}\")\n",
    "print_gpu_memory(\"Before training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "print(\"Monitor GPU memory in another terminal with: nvidia-smi -l 5\")\n",
    "print()\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(f\"\\nTraining complete!\")\n",
    "print(f\"  Total steps: {train_result.global_step}\")\n",
    "print(f\"  Training loss: {train_result.training_loss:.4f}\")\n",
    "print_gpu_memory(\"After training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-loss",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss curve\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract training loss entries (filter out eval/save entries)\n",
    "train_losses = [\n",
    "    (entry[\"step\"], entry[\"loss\"])\n",
    "    for entry in log_history\n",
    "    if \"loss\" in entry\n",
    "]\n",
    "\n",
    "if train_losses:\n",
    "    steps, losses = zip(*train_losses)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(steps, losses, color=\"steelblue\", linewidth=2, marker=\"o\", markersize=4)\n",
    "    ax.set_xlabel(\"Training Step\", fontsize=12)\n",
    "    ax.set_ylabel(\"Loss\", fontsize=12)\n",
    "    ax.set_title(\"QLoRA Training Loss\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Mark epoch boundaries\n",
    "    total_steps = steps[-1]\n",
    "    steps_per_epoch = total_steps / training_args.num_train_epochs\n",
    "    for epoch in range(1, int(training_args.num_train_epochs) + 1):\n",
    "        epoch_step = int(epoch * steps_per_epoch)\n",
    "        ax.axvline(x=epoch_step, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "        ax.text(epoch_step, max(losses) * 0.95, f\"Epoch {epoch}\",\n",
    "                ha=\"center\", fontsize=9, color=\"red\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Initial loss: {losses[0]:.4f}\")\n",
    "    print(f\"Final loss:   {losses[-1]:.4f}\")\n",
    "    print(f\"Reduction:    {(1 - losses[-1] / losses[0]) * 100:.1f}%\")\n",
    "else:\n",
    "    print(\"No training loss entries found in log history.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapter weights\n",
    "adapter_path = os.path.join(output_dir, \"final-adapter\")\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "\n",
    "# Check adapter size on disk\n",
    "adapter_size_bytes = sum(\n",
    "    f.stat().st_size for f in Path(adapter_path).rglob(\"*\") if f.is_file()\n",
    ")\n",
    "adapter_size_mb = adapter_size_bytes / 1e6\n",
    "\n",
    "print(f\"Adapter saved to: {adapter_path}\")\n",
    "print(f\"Adapter size on disk: {adapter_size_mb:.1f} MB\")\n",
    "print(f\"\\nFor comparison, the full model would be ~14 GB on disk.\")\n",
    "print(f\"The adapter is {14000 / adapter_size_mb:.0f}x smaller.\")\n",
    "print(f\"\\nFiles saved:\")\n",
    "for f in sorted(Path(adapter_path).iterdir()):\n",
    "    print(f\"  {f.name} ({f.stat().st_size / 1e6:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint-3-summary",
   "metadata": {},
   "source": [
    "### Checkpoint 3 Summary\n",
    "\n",
    "You have:\n",
    "- Loaded and formatted a legal instruction dataset\n",
    "- Configured SFTTrainer with LoRA-appropriate hyperparameters\n",
    "- Trained the QLoRA model and observed the loss curve\n",
    "- Saved the adapter weights (much smaller than the full model)\n",
    "\n",
    "You can pause here and resume at Checkpoint 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint-4-heading",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint 4: Evaluation and Merging\n",
    "\n",
    "Now we evaluate the fine-tuned model by comparing its outputs to the base\n",
    "model on legal prompts. Then we merge the adapters back into the base\n",
    "model weights, producing a single standalone model.\n",
    "\n",
    "### Base model vs QLoRA comparison\n",
    "\n",
    "The base model (Mistral-7B-v0.1 or Llama-2-7B) was trained on general\n",
    "web text. It knows about law in a general sense, but it was not trained\n",
    "to follow instructions or produce structured legal analysis. After QLoRA\n",
    "fine-tuning on our legal dataset, the model should:\n",
    "\n",
    "1. Follow the instruction format (answer what was asked)\n",
    "2. Produce more relevant, focused legal responses\n",
    "3. Reference appropriate legal concepts from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-prompts",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define legal evaluation prompts\n",
    "eval_prompts = [\n",
    "    {\n",
    "        \"instruction\": \"Summarize this court opinion.\",\n",
    "        \"input\": (\n",
    "            \"The plaintiff filed suit alleging violations of the Americans \"\n",
    "            \"with Disabilities Act after being terminated from his position \"\n",
    "            \"as a warehouse supervisor. The district court granted summary \"\n",
    "            \"judgment for the defendant employer, finding that the plaintiff \"\n",
    "            \"failed to establish that he was qualified to perform the \"\n",
    "            \"essential functions of the job with or without reasonable \"\n",
    "            \"accommodation. The Seventh Circuit reviews this de novo.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What was the holding in this case?\",\n",
    "        \"input\": (\n",
    "            \"The court considered whether the SEC had authority to revoke \"\n",
    "            \"the registration of an investment adviser who systematically \"\n",
    "            \"allocated IPO shares to proprietary accounts ahead of client \"\n",
    "            \"orders. The Commission found willful violations of the \"\n",
    "            \"Investment Advisers Act and imposed revocation.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What are the key legal issues in this case?\",\n",
    "        \"input\": (\n",
    "            \"Parents of a child with autism challenged the school district's \"\n",
    "            \"individualized education program under the Individuals with \"\n",
    "            \"Disabilities Education Act. They argued that the district \"\n",
    "            \"failed to consider independent educational evaluations and \"\n",
    "            \"denied their child a free appropriate public education.\"\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(eval_prompts)} evaluation prompts\")\n",
    "for i, prompt in enumerate(eval_prompts):\n",
    "    print(f\"  [{i}] {prompt['instruction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-qlora",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(model, tokenizer, instruction, input_text, max_new_tokens=256):\n",
    "    \"\"\"Generate a response from the model using a chat-formatted prompt.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": f\"{instruction}\\n\\n{input_text}\"},\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    # Decode only the generated tokens (skip the prompt)\n",
    "    generated = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Generate responses with the QLoRA fine-tuned model\n",
    "print(\"Generating responses with the QLoRA fine-tuned model...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "qlora_responses = []\n",
    "for i, prompt in enumerate(eval_prompts):\n",
    "    response = generate_response(\n",
    "        model, tokenizer, prompt[\"instruction\"], prompt[\"input\"]\n",
    "    )\n",
    "    qlora_responses.append(response)\n",
    "    print(f\"\\n--- Prompt {i}: {prompt['instruction']} ---\")\n",
    "    print(f\"QLoRA response:\\n{response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-base",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses with the base model (disable LoRA adapters)\n",
    "print(\"Generating responses with the BASE model (LoRA disabled)...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model.disable_adapter_layers()\n",
    "\n",
    "base_responses = []\n",
    "for i, prompt in enumerate(eval_prompts):\n",
    "    response = generate_response(\n",
    "        model, tokenizer, prompt[\"instruction\"], prompt[\"input\"]\n",
    "    )\n",
    "    base_responses.append(response)\n",
    "    print(f\"\\n--- Prompt {i}: {prompt['instruction']} ---\")\n",
    "    print(f\"Base response:\\n{response}\")\n",
    "    print()\n",
    "\n",
    "# Re-enable adapters\n",
    "model.enable_adapter_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-outputs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "print(\"Side-by-Side Comparison: Base Model vs QLoRA Fine-Tuned\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, prompt in enumerate(eval_prompts):\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Prompt {i}: {prompt['instruction']}\")\n",
    "    print(f\"Input: {prompt['input'][:100]}...\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"\\n  BASE MODEL:\")\n",
    "    print(f\"  {base_responses[i][:300]}\")\n",
    "    print(f\"\\n  QLORA FINE-TUNED:\")\n",
    "    print(f\"  {qlora_responses[i][:300]}\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- The base model may produce continuations rather than direct answers.\")\n",
    "print(\"- The QLoRA model should follow the instruction format more closely.\")\n",
    "print(\"- Quality depends heavily on dataset size and training epochs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified evaluation harness (adapted from Module 08)\n",
    "import re\n",
    "\n",
    "\n",
    "def response_relevance_score(instruction, response):\n",
    "    \"\"\"Score how relevant the response is to the instruction.\n",
    "\n",
    "    Heuristic: measures overlap between instruction keywords and response.\n",
    "    \"\"\"\n",
    "    stopwords = {\n",
    "        \"the\", \"a\", \"an\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\",\n",
    "        \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"will\", \"would\",\n",
    "        \"to\", \"of\", \"in\", \"for\", \"on\", \"with\", \"at\", \"by\", \"from\",\n",
    "        \"and\", \"but\", \"or\", \"not\", \"this\", \"that\", \"it\", \"what\",\n",
    "    }\n",
    "    inst_words = set(re.findall(r\"\\b\\w+\\b\", instruction.lower())) - stopwords\n",
    "    resp_words = set(re.findall(r\"\\b\\w+\\b\", response.lower())) - stopwords\n",
    "\n",
    "    if not inst_words:\n",
    "        return 0.0\n",
    "    return len(inst_words & resp_words) / len(inst_words)\n",
    "\n",
    "\n",
    "def response_length_score(response, target_min=50, target_max=500):\n",
    "    \"\"\"Score whether the response length is in a reasonable range.\"\"\"\n",
    "    length = len(response.split())\n",
    "    if length < target_min:\n",
    "        return length / target_min\n",
    "    elif length > target_max:\n",
    "        return max(0.5, target_max / length)\n",
    "    return 1.0\n",
    "\n",
    "\n",
    "# Score both model outputs\n",
    "print(\"Simplified Evaluation Scores\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Prompt':>8}  {'Model':<12}  {'Relevance':>10}  {'Length':>8}  {'Combined':>9}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, prompt in enumerate(eval_prompts):\n",
    "    for label, response in [(\"Base\", base_responses[i]), (\"QLoRA\", qlora_responses[i])]:\n",
    "        relevance = response_relevance_score(prompt[\"instruction\"], response)\n",
    "        length = response_length_score(response)\n",
    "        combined = 0.6 * relevance + 0.4 * length\n",
    "        print(f\"  {i:>5}  {label:<12}  {relevance:>10.3f}  {length:>8.3f}  {combined:>9.3f}\")\n",
    "\n",
    "print()\n",
    "print(\"Note: These heuristic scores give a rough signal. For rigorous\")\n",
    "print(\"evaluation, use the full harness from Module 08.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "merging-heading",
   "metadata": {},
   "source": [
    "### Merging Adapters\n",
    "\n",
    "LoRA adapters can be used in two ways:\n",
    "\n",
    "1. **Keep adapters separate**: Load the base model, then load adapters on top.\n",
    "   Advantages: you can swap adapters (e.g., one for legal, one for medical),\n",
    "   share the base model across tasks, and the adapter files are tiny.\n",
    "\n",
    "2. **Merge adapters into the base model**: Combine W + BA into a single weight\n",
    "   matrix. Advantages: simpler deployment (one model file), slightly faster\n",
    "   inference (no adapter overhead), compatible with any serving framework.\n",
    "\n",
    "When to keep separate:\n",
    "- You have multiple domain adapters for one base model\n",
    "- You want to A/B test different adapter versions\n",
    "- Storage is a concern (adapters are ~50 MB vs ~14 GB merged)\n",
    "\n",
    "When to merge:\n",
    "- You are deploying a single model for production\n",
    "- You need maximum inference speed\n",
    "- You want to quantize the merged model further (e.g., GGUF for llama.cpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge-adapters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA adapters back into the base model\n",
    "print(\"Merging LoRA adapters into base model...\")\n",
    "print_gpu_memory(\"Before merge\")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "print_gpu_memory(\"After merge\")\n",
    "print(f\"\\nMerged model type: {type(merged_model).__name__}\")\n",
    "print(f\"The model is now a standard transformers model with no adapter layers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-sizes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare adapter-only vs merged model size\n",
    "merged_path = os.path.join(output_dir, \"merged-model\")\n",
    "merged_model.save_pretrained(merged_path)\n",
    "tokenizer.save_pretrained(merged_path)\n",
    "\n",
    "merged_size_bytes = sum(\n",
    "    f.stat().st_size for f in Path(merged_path).rglob(\"*\") if f.is_file()\n",
    ")\n",
    "merged_size_gb = merged_size_bytes / 1e9\n",
    "\n",
    "print(\"Size Comparison\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"  Adapter only:  {adapter_size_mb:>8.1f} MB\")\n",
    "print(f\"  Merged model:  {merged_size_gb:>8.2f} GB\")\n",
    "print(f\"  Ratio:         {merged_size_gb * 1000 / adapter_size_mb:>8.0f}x\")\n",
    "print()\n",
    "print(\"The adapter is tiny because it only stores the LoRA matrices.\")\n",
    "print(\"The merged model includes all base model weights with adapters baked in.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-merged",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the merged model produces the same outputs\n",
    "print(\"Verifying merged model output matches QLoRA adapter output...\")\n",
    "\n",
    "test_prompt = eval_prompts[0]\n",
    "merged_response = generate_response(\n",
    "    merged_model, tokenizer,\n",
    "    test_prompt[\"instruction\"], test_prompt[\"input\"],\n",
    "    max_new_tokens=100,\n",
    ")\n",
    "\n",
    "print(f\"\\nMerged model response:\")\n",
    "print(f\"  {merged_response[:300]}\")\n",
    "print()\n",
    "print(\"Note: Due to sampling (temperature > 0), outputs may differ between\")\n",
    "print(\"runs. Set temperature=0 for deterministic comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint-4-summary",
   "metadata": {},
   "source": [
    "### Checkpoint 4 Summary\n",
    "\n",
    "You have:\n",
    "- Generated and compared outputs from the base model vs QLoRA fine-tuned model\n",
    "- Run a simplified evaluation harness to score responses\n",
    "- Merged LoRA adapters back into the base model\n",
    "- Compared adapter-only size (~50 MB) vs merged model size (~14 GB)\n",
    "- Understood when to keep adapters separate vs merge\n",
    "\n",
    "You can pause here and resume at Checkpoint 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint-5-heading",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checkpoint 5: Experiments\n",
    "\n",
    "This checkpoint provides a framework for experimenting with QLoRA\n",
    "hyperparameters. The goal is to understand how different choices affect\n",
    "model quality and training cost.\n",
    "\n",
    "Each experiment modifies one variable while keeping everything else\n",
    "constant. Record your results in the comparison table at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experiment-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qlora_experiment(\n",
    "    model_name,\n",
    "    lora_r,\n",
    "    lora_alpha,\n",
    "    target_modules,\n",
    "    num_epochs,\n",
    "    train_dataset,\n",
    "    tokenizer,\n",
    "    experiment_name,\n",
    "):\n",
    "    \"\"\"Run a complete QLoRA training experiment with given hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        model_name: HuggingFace model ID.\n",
    "        lora_r: LoRA rank.\n",
    "        lora_alpha: LoRA alpha scaling factor.\n",
    "        target_modules: List of module names to apply LoRA to.\n",
    "        num_epochs: Number of training epochs.\n",
    "        train_dataset: HuggingFace Dataset with 'messages' column.\n",
    "        tokenizer: Tokenizer instance.\n",
    "        experiment_name: Name for the output directory.\n",
    "\n",
    "    Returns:\n",
    "        Dict with experiment results.\n",
    "    \"\"\"\n",
    "    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "    from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Experiment: {experiment_name}\")\n",
    "    print(f\"  r={lora_r}, alpha={lora_alpha}, targets={target_modules}, epochs={num_epochs}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    # Load fresh quantized model\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    exp_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    exp_model = prepare_model_for_kbit_training(exp_model)\n",
    "\n",
    "    # Apply LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    exp_model = get_peft_model(exp_model, lora_config)\n",
    "\n",
    "    trainable = sum(p.numel() for p in exp_model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in exp_model.parameters())\n",
    "    print(f\"  Trainable: {trainable:,} / {total:,} ({100 * trainable / total:.4f}%)\")\n",
    "\n",
    "    # Training\n",
    "    exp_output_dir = f\"./qlora-experiments/{experiment_name}\"\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=exp_output_dir,\n",
    "        num_train_epochs=num_epochs,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.03,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        logging_steps=5,\n",
    "        save_strategy=\"no\",\n",
    "        bf16=True,\n",
    "        max_seq_length=512,\n",
    "        gradient_checkpointing=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=exp_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        processing_class=tokenizer,\n",
    "    )\n",
    "\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    # Extract loss curve\n",
    "    losses = [\n",
    "        (entry[\"step\"], entry[\"loss\"])\n",
    "        for entry in trainer.state.log_history\n",
    "        if \"loss\" in entry\n",
    "    ]\n",
    "\n",
    "    final_loss = losses[-1][1] if losses else float(\"nan\")\n",
    "    initial_loss = losses[0][1] if losses else float(\"nan\")\n",
    "\n",
    "    result = {\n",
    "        \"name\": experiment_name,\n",
    "        \"r\": lora_r,\n",
    "        \"alpha\": lora_alpha,\n",
    "        \"target_modules\": target_modules,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"trainable_params\": trainable,\n",
    "        \"initial_loss\": initial_loss,\n",
    "        \"final_loss\": final_loss,\n",
    "        \"loss_curve\": losses,\n",
    "        \"total_steps\": train_result.global_step,\n",
    "        \"model\": exp_model,\n",
    "    }\n",
    "\n",
    "    print(f\"  Final loss: {final_loss:.4f}\")\n",
    "    print(f\"  Steps: {train_result.global_step}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Experiment runner defined.\")\n",
    "print(\"Run the cells below to execute experiments.\")\n",
    "print(\"Each experiment loads a fresh model, so they are independent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment-1-heading",
   "metadata": {},
   "source": [
    "### Experiment 1: LoRA Rank\n",
    "\n",
    "How does the rank (r) affect model quality? Lower rank means fewer\n",
    "parameters but less capacity to learn. Higher rank means more parameters\n",
    "and potentially better quality, but diminishing returns.\n",
    "\n",
    "We test r=4, r=16, and r=64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experiment-rank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1: Vary LoRA rank\n",
    "# NOTE: This cell runs 3 full training runs. It may take a while.\n",
    "# You can reduce num_epochs to 1 for faster iteration.\n",
    "\n",
    "rank_experiments = []\n",
    "attention_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "\n",
    "for r in [4, 16, 64]:\n",
    "    result = run_qlora_experiment(\n",
    "        model_name=model_name,\n",
    "        lora_r=r,\n",
    "        lora_alpha=2 * r,  # Keep alpha = 2 * r as convention\n",
    "        target_modules=attention_modules,\n",
    "        num_epochs=3,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        experiment_name=f\"rank-{r}\",\n",
    "    )\n",
    "    rank_experiments.append(result)\n",
    "\n",
    "    # Free GPU memory between experiments\n",
    "    del result[\"model\"]\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nAll rank experiments complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-rank-experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rank experiment results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: loss curves\n",
    "ax = axes[0]\n",
    "colors = [\"#e74c3c\", \"#3498db\", \"#2ecc71\"]\n",
    "for exp, color in zip(rank_experiments, colors):\n",
    "    if exp[\"loss_curve\"]:\n",
    "        steps, losses = zip(*exp[\"loss_curve\"])\n",
    "        ax.plot(steps, losses, label=f\"r={exp['r']}\", color=color, linewidth=2)\n",
    "ax.set_xlabel(\"Step\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Training Loss by LoRA Rank\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: final loss vs trainable params\n",
    "ax = axes[1]\n",
    "ranks = [exp[\"r\"] for exp in rank_experiments]\n",
    "final_losses = [exp[\"final_loss\"] for exp in rank_experiments]\n",
    "param_counts = [exp[\"trainable_params\"] / 1e6 for exp in rank_experiments]\n",
    "\n",
    "ax.bar(range(len(ranks)), final_losses, color=colors, alpha=0.8)\n",
    "ax.set_xticks(range(len(ranks)))\n",
    "ax.set_xticklabels([f\"r={r}\\n({p:.1f}M params)\" for r, p in zip(ranks, param_counts)])\n",
    "ax.set_ylabel(\"Final Loss\")\n",
    "ax.set_title(\"Final Loss by LoRA Rank\")\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Results:\")\n",
    "for exp in rank_experiments:\n",
    "    print(f\"  r={exp['r']:>3}: {exp['trainable_params']:>10,} params, \"\n",
    "          f\"final_loss={exp['final_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment-2-heading",
   "metadata": {},
   "source": [
    "### Experiment 2: Target Modules\n",
    "\n",
    "By default, LoRA is applied to attention projections only. Adding MLP\n",
    "layers (gate_proj, up_proj, down_proj) increases adapter capacity but\n",
    "also parameter count. Does the extra capacity help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experiment-modules",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Attention only vs Attention + MLP\n",
    "module_configs = {\n",
    "    \"attention-only\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    \"attention-mlp\": [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "module_experiments = []\n",
    "\n",
    "for name, modules in module_configs.items():\n",
    "    result = run_qlora_experiment(\n",
    "        model_name=model_name,\n",
    "        lora_r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=modules,\n",
    "        num_epochs=3,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        experiment_name=name,\n",
    "    )\n",
    "    module_experiments.append(result)\n",
    "\n",
    "    del result[\"model\"]\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nModule experiments complete.\")\n",
    "for exp in module_experiments:\n",
    "    print(f\"  {exp['name']}: {exp['trainable_params']:,} params, \"\n",
    "          f\"final_loss={exp['final_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment-3-heading",
   "metadata": {},
   "source": [
    "### Experiment 3: Number of Epochs\n",
    "\n",
    "With a small dataset, training for more epochs risks overfitting.\n",
    "We test 1, 3, and 5 epochs to find the sweet spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experiment-epochs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Vary number of epochs\n",
    "epoch_experiments = []\n",
    "\n",
    "for n_epochs in [1, 3, 5]:\n",
    "    result = run_qlora_experiment(\n",
    "        model_name=model_name,\n",
    "        lora_r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        num_epochs=n_epochs,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        experiment_name=f\"epochs-{n_epochs}\",\n",
    "    )\n",
    "    epoch_experiments.append(result)\n",
    "\n",
    "    del result[\"model\"]\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nEpoch experiments complete.\")\n",
    "for exp in epoch_experiments:\n",
    "    print(f\"  {exp['num_epochs']} epochs: final_loss={exp['final_loss']:.4f}, \"\n",
    "          f\"steps={exp['total_steps']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all experiment results into a comparison table\n",
    "all_experiments = rank_experiments + module_experiments + epoch_experiments\n",
    "\n",
    "print(\"Experiment Results Comparison\")\n",
    "print(\"=\" * 90)\n",
    "print(\n",
    "    f\"{'Experiment':<20} {'Rank':>5} {'Modules':>15} {'Epochs':>7} \"\n",
    "    f\"{'Params':>12} {'Init Loss':>10} {'Final Loss':>11}\"\n",
    ")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for exp in all_experiments:\n",
    "    module_str = \"attn\" if len(exp[\"target_modules\"]) <= 4 else \"attn+mlp\"\n",
    "    print(\n",
    "        f\"{exp['name']:<20} {exp['r']:>5} {module_str:>15} {exp['num_epochs']:>7} \"\n",
    "        f\"{exp['trainable_params']:>12,} {exp['initial_loss']:>10.4f} {exp['final_loss']:>11.4f}\"\n",
    "    )\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print()\n",
    "print(\"Use this table to identify the best configuration for your use case.\")\n",
    "print(\"Lower final loss generally indicates better training, but watch for\")\n",
    "print(\"overfitting (final loss much lower than initial with a small dataset).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checkpoint-5-summary",
   "metadata": {},
   "source": [
    "### Checkpoint 5 Summary\n",
    "\n",
    "You have:\n",
    "- Experimented with different LoRA ranks (4, 16, 64)\n",
    "- Compared attention-only vs attention+MLP target modules\n",
    "- Tested different numbers of training epochs (1, 3, 5)\n",
    "- Recorded results in a comparison table\n",
    "\n",
    "Key takeaways:\n",
    "- Higher rank increases capacity but with diminishing returns\n",
    "- Adding MLP modules increases parameters significantly but may improve quality\n",
    "- More epochs help up to a point; then overfitting kicks in\n",
    "- The optimal configuration depends on your dataset size and quality requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "### Exercise (a): LoRA Rank Exploration\n",
    "\n",
    "Experiment with LoRA rank values of 4, 16, and 64. For each rank:\n",
    "\n",
    "1. Train a QLoRA model using `run_qlora_experiment()`.\n",
    "2. Generate responses to the `eval_prompts` defined in Checkpoint 4.\n",
    "3. Score each model using the simplified evaluation harness.\n",
    "4. Record the results in a table: rank, trainable params, final loss,\n",
    "   relevance score, and your subjective quality assessment.\n",
    "\n",
    "Questions to consider:\n",
    "- At what rank do you see diminishing returns?\n",
    "- Is there a rank where the model begins to overfit on this small dataset?\n",
    "- How does training time scale with rank?\n",
    "\n",
    "```python\n",
    "# Starter code for Exercise (a)\n",
    "for r in [4, 16, 64]:\n",
    "    result = run_qlora_experiment(\n",
    "        model_name=model_name,\n",
    "        lora_r=r,\n",
    "        lora_alpha=2 * r,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        num_epochs=3,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        experiment_name=f\"exercise-a-rank-{r}\",\n",
    "    )\n",
    "    # Generate responses and score them here\n",
    "```\n",
    "\n",
    "### Exercise (b): Target Module Comparison\n",
    "\n",
    "Compare two LoRA configurations:\n",
    "1. **Attention only**: `[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]`\n",
    "2. **Attention + MLP**: `[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]`\n",
    "\n",
    "For each configuration:\n",
    "1. Train with r=16, 3 epochs.\n",
    "2. Compare trainable parameter counts.\n",
    "3. Generate and compare responses on the same prompts.\n",
    "4. Measure whether the extra parameters in the MLP configuration\n",
    "   improve response quality enough to justify the increased cost.\n",
    "\n",
    "```python\n",
    "# Starter code for Exercise (b)\n",
    "configs = {\n",
    "    \"attention\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    \"attention_mlp\": [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "}\n",
    "for name, modules in configs.items():\n",
    "    result = run_qlora_experiment(\n",
    "        model_name=model_name,\n",
    "        lora_r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=modules,\n",
    "        num_epochs=3,\n",
    "        train_dataset=train_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        experiment_name=f\"exercise-b-{name}\",\n",
    "    )\n",
    "    # Compare outputs here\n",
    "```\n",
    "\n",
    "### Exercise (c): Larger Dataset\n",
    "\n",
    "The training dataset in this lab is small (derived from 5 court opinions).\n",
    "Try training on a larger dataset:\n",
    "\n",
    "1. Use the scripts in `datasets/scripts/` to fetch more court opinions:\n",
    "   ```bash\n",
    "   python ../../datasets/scripts/fetch_courtlistener.py --limit 50\n",
    "   ```\n",
    "2. Build instruction pairs from the expanded dataset using the same\n",
    "   approach as Module 06.\n",
    "3. Train a QLoRA model on the larger dataset.\n",
    "4. Compare outputs and loss curves with the small-dataset model.\n",
    "\n",
    "Questions to consider:\n",
    "- How does the loss curve change with more data?\n",
    "- Do you need more or fewer epochs with a larger dataset?\n",
    "- Is the quality improvement proportional to the data increase?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "The training outputs can consume significant disk space. Run the cell\n",
    "below to remove experiment artifacts if you no longer need them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to clean up experiment outputs\n",
    "# import shutil\n",
    "# for dir_name in [\"qlora-legal\", \"qlora-experiments\"]:\n",
    "#     if Path(dir_name).exists():\n",
    "#         shutil.rmtree(dir_name)\n",
    "#         print(f\"Removed {dir_name}\")\n",
    "\n",
    "# Free GPU memory\n",
    "# del model, merged_model\n",
    "# torch.cuda.empty_cache()\n",
    "# print_gpu_memory(\"After cleanup\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 5,
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
