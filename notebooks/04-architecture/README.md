# Module 04: Transformer Architecture

Build a minimal transformer block (attention + FFN) in PyTorch. Visualize attention patterns. Walk through GPT and Llama architecture differences (positional encoding, RMSNorm, GQA). No training â€” forward passes with pre-trained weights. By the end of this module, you will have a concrete, code-level understanding of how modern transformer architectures work, from self-attention mechanics to the specific design choices that differentiate GPT from Llama.

**Prerequisites:** Basic PyTorch familiarity.
