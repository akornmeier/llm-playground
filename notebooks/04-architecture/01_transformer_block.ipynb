{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 01 - Building a Transformer Block from Scratch\n",
    "\n",
    "## Context\n",
    "\n",
    "The transformer is the architecture behind every modern large language model --\n",
    "GPT-4, Claude, Llama, Gemini. Introduced in the 2017 paper *Attention Is All\n",
    "You Need*, the transformer replaced recurrent networks with a mechanism called\n",
    "**self-attention** that lets every token in a sequence directly attend to every\n",
    "other token.\n",
    "\n",
    "**CoCounsel context:** Understanding attention helps explain why models sometimes\n",
    "lose track of long legal arguments or miss citations in lengthy briefs. Attention\n",
    "is a weighted average -- when a brief is 10,000 tokens long, the attention\n",
    "weights for any single token are spread across all 10,000 positions. Critical\n",
    "citations can receive vanishingly small weight, causing the model to \"forget\"\n",
    "them. Knowing this helps you structure prompts and chunk documents effectively.\n",
    "\n",
    "In this notebook, we build every component of a transformer block from scratch\n",
    "using PyTorch:\n",
    "\n",
    "1. Scaled dot-product attention\n",
    "2. Multi-head attention\n",
    "3. Position-wise feed-forward network\n",
    "4. Full transformer block with residual connections and layer normalization\n",
    "\n",
    "Then we visualize attention patterns on a legal sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Theory: Self-Attention\n",
    "\n",
    "Self-attention lets each token in a sequence compute a weighted combination of\n",
    "all tokens in the sequence. The weights are learned dynamically based on the\n",
    "content of the tokens themselves -- not fixed by position.\n",
    "\n",
    "The mechanism uses three learned linear projections to transform each input\n",
    "token into three vectors:\n",
    "\n",
    "- **Q (Query)** -- \"What am I looking for?\" Each token generates a query vector\n",
    "  that represents what information it needs from other tokens.\n",
    "- **K (Key)** -- \"What do I contain?\" Each token generates a key vector that\n",
    "  advertises what information it holds.\n",
    "- **V (Value)** -- \"What do I offer?\" Each token generates a value vector that\n",
    "  contains the actual information to pass along.\n",
    "\n",
    "### The Attention Formula\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "Step by step:\n",
    "\n",
    "1. **Compute attention scores**: Multiply Q by K transposed. This produces a\n",
    "   matrix where entry (i, j) measures how much token i should attend to token j.\n",
    "2. **Scale**: Divide by $\\sqrt{d_k}$ (the square root of the key dimension).\n",
    "   Without this scaling, the dot products grow large for high-dimensional\n",
    "   vectors, pushing softmax into regions with tiny gradients.\n",
    "3. **Softmax**: Normalize each row so the attention weights sum to 1. Now each\n",
    "   token has a probability distribution over all tokens.\n",
    "4. **Weighted sum**: Multiply the attention weights by V. Each token's output\n",
    "   is a weighted combination of all value vectors.\n",
    "\n",
    "### Why This Matters for Legal Text\n",
    "\n",
    "Consider the sentence: *\"The court held that the defendant, who had previously\n",
    "filed a motion to dismiss, was liable.\"* The word \"liable\" must attend back\n",
    "to \"defendant\" (its subject), skipping over the relative clause. Self-attention\n",
    "can learn to make this long-range connection directly, without passing\n",
    "information step-by-step through a recurrent chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Implementing Attention from Scratch\n",
    "\n",
    "Before wrapping anything in `nn.Module`, let's build scaled dot-product\n",
    "attention using raw tensor operations. This makes every step explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions\n",
    "seq_len = 6   # number of tokens in our sequence\n",
    "d_k = 8       # dimension of each Q/K/V vector\n",
    "\n",
    "# Create random Q, K, V tensors\n",
    "# In a real model, these come from linear projections of the input embeddings.\n",
    "# Here we use random values to demonstrate the mechanics.\n",
    "Q = torch.randn(seq_len, d_k)\n",
    "K = torch.randn(seq_len, d_k)\n",
    "V = torch.randn(seq_len, d_k)\n",
    "\n",
    "print(f\"Q shape: {Q.shape}  (seq_len x d_k)\")\n",
    "print(f\"K shape: {K.shape}  (seq_len x d_k)\")\n",
    "print(f\"V shape: {V.shape}  (seq_len x d_k)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute raw attention scores (QK^T)\n",
    "# Each entry (i, j) measures how much token i wants to attend to token j.\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "print(f\"Raw attention scores shape: {scores.shape}  (seq_len x seq_len)\")\n",
    "print(f\"\\nRaw scores:\\n{scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Scale by sqrt(d_k)\n",
    "# Without scaling, large dot products push softmax into saturation\n",
    "# (one weight near 1, rest near 0), which kills gradients during training.\n",
    "scale = math.sqrt(d_k)\n",
    "scaled_scores = scores / scale\n",
    "print(f\"Scale factor: sqrt({d_k}) = {scale:.4f}\")\n",
    "print(f\"\\nScaled scores:\\n{scaled_scores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply softmax to get attention weights\n",
    "# Each row sums to 1 -- it's a probability distribution over tokens.\n",
    "attention_weights = torch.softmax(scaled_scores, dim=-1)\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"\\nAttention weights (each row sums to 1):\\n{attention_weights}\")\n",
    "print(f\"\\nRow sums: {attention_weights.sum(dim=-1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compute the weighted sum of values\n",
    "# Each token's output is a weighted combination of all value vectors.\n",
    "output = torch.matmul(attention_weights, V)\n",
    "print(f\"Output shape: {output.shape}  (seq_len x d_k)\")\n",
    "print(f\"\\nOutput (first 3 tokens):\\n{output[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the attention weights as a heatmap\n",
    "token_labels = [f\"tok_{i}\" for i in range(seq_len)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "im = ax.imshow(attention_weights.detach().numpy(), cmap=\"Blues\", vmin=0, vmax=1)\n",
    "ax.set_xticks(range(seq_len))\n",
    "ax.set_yticks(range(seq_len))\n",
    "ax.set_xticklabels(token_labels, rotation=45, ha=\"right\")\n",
    "ax.set_yticklabels(token_labels)\n",
    "ax.set_xlabel(\"Key (attending to)\")\n",
    "ax.set_ylabel(\"Query (attending from)\")\n",
    "ax.set_title(\"Scaled Dot-Product Attention Weights\")\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        val = attention_weights[i, j].item()\n",
    "        color = \"white\" if val > 0.5 else \"black\"\n",
    "        ax.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", color=color, fontsize=9)\n",
    "\n",
    "fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Wrapping Attention as a Function\n",
    "\n",
    "Let's consolidate the four steps into a reusable function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(\n",
    "    Q: torch.Tensor,\n",
    "    K: torch.Tensor,\n",
    "    V: torch.Tensor,\n",
    "    mask: torch.Tensor | None = None,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Compute scaled dot-product attention.\n",
    "\n",
    "    Args:\n",
    "        Q: Query tensor of shape (..., seq_len, d_k).\n",
    "        K: Key tensor of shape (..., seq_len, d_k).\n",
    "        V: Value tensor of shape (..., seq_len, d_v).\n",
    "        mask: Optional boolean mask. True values are masked (not attended to).\n",
    "\n",
    "    Returns:\n",
    "        output: Weighted sum of values, shape (..., seq_len, d_v).\n",
    "        weights: Attention weights, shape (..., seq_len, seq_len).\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float(\"-inf\"))\n",
    "\n",
    "    weights = torch.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(weights, V)\n",
    "    return output, weights\n",
    "\n",
    "\n",
    "# Quick test\n",
    "out, wts = scaled_dot_product_attention(Q, K, V)\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Weights shape: {wts.shape}\")\n",
    "print(f\"Weights row sums: {wts.sum(dim=-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "A single attention head computes one set of attention weights. But different\n",
    "aspects of language require different kinds of attention:\n",
    "\n",
    "- One head might learn to attend to the **syntactic subject** of a verb.\n",
    "- Another head might learn to attend to **nearby adjectives**.\n",
    "- Another might specialize in **coreference** (linking pronouns to nouns).\n",
    "\n",
    "**Multi-head attention** runs multiple attention heads in parallel, each with\n",
    "its own learned Q/K/V projections. The outputs are concatenated and projected\n",
    "back to the model dimension.\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O$$\n",
    "\n",
    "where each $\\text{head}_i = \\text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)$.\n",
    "\n",
    "The key insight: each head operates on a **smaller dimension** ($d_k / h$),\n",
    "so the total computation is roughly the same as single-head attention with\n",
    "the full dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention mechanism.\n",
    "\n",
    "    Splits the model dimension into multiple heads, applies scaled\n",
    "    dot-product attention independently per head, then concatenates\n",
    "    and projects the results.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, n_heads: int) -> None:\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads  # dimension per head\n",
    "\n",
    "        # Linear projections for Q, K, V (all heads packed into one matrix)\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: torch.Tensor | None = None,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len, d_model).\n",
    "            mask: Optional attention mask.\n",
    "\n",
    "        Returns:\n",
    "            output: Shape (batch, seq_len, d_model).\n",
    "            attention_weights: Shape (batch, n_heads, seq_len, seq_len).\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_q(x)  # (batch, seq_len, d_model)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        # Reshape to (batch, n_heads, seq_len, d_k)\n",
    "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Apply scaled dot-product attention per head\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # Concatenate heads: (batch, n_heads, seq_len, d_k) -> (batch, seq_len, d_model)\n",
    "        attn_output = (\n",
    "            attn_output.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, seq_len, self.d_model)\n",
    "        )\n",
    "\n",
    "        # Final linear projection\n",
    "        output = self.W_o(attn_output)\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the multi-head attention module\n",
    "d_model = 64\n",
    "n_heads = 4\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "\n",
    "mha = MultiHeadAttention(d_model=d_model, n_heads=n_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, attn_weights = mha(x)\n",
    "print(f\"Input shape:            {x.shape}\")\n",
    "print(f\"Output shape:           {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"  -> (batch={batch_size}, heads={n_heads}, seq_len={seq_len}, seq_len={seq_len})\")\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in mha.parameters())\n",
    "print(f\"\\nMultiHeadAttention parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Feed-Forward Network\n",
    "\n",
    "After attention, each token passes independently through a position-wise\n",
    "feed-forward network (FFN). This is simply two linear layers with a\n",
    "nonlinearity in between:\n",
    "\n",
    "$$\\text{FFN}(x) = W_2 \\cdot \\text{GELU}(W_1 x + b_1) + b_2$$\n",
    "\n",
    "The inner dimension is typically 4x the model dimension (e.g., d_model=768\n",
    "uses d_ff=3072). This expansion-then-compression pattern lets the network\n",
    "learn richer per-token representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network.\n",
    "\n",
    "    Two linear transformations with a GELU activation in between.\n",
    "    Applied independently to each position (token) in the sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int | None = None) -> None:\n",
    "        super().__init__()\n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass: expand, activate, project back.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len, d_model).\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (batch, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        return self.linear2(self.activation(self.linear1(x)))\n",
    "\n",
    "\n",
    "# Test\n",
    "ffn = FeedForward(d_model=64)\n",
    "x = torch.randn(2, 8, 64)\n",
    "out = ffn(x)\n",
    "print(f\"FFN input shape:  {x.shape}\")\n",
    "print(f\"FFN output shape: {out.shape}\")\n",
    "\n",
    "n_params = sum(p.numel() for p in ffn.parameters())\n",
    "print(f\"FFN parameters:   {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Full Transformer Block\n",
    "\n",
    "A transformer block combines everything above with **residual connections**\n",
    "and **layer normalization**. The structure (using pre-norm, which is standard\n",
    "in modern LLMs):\n",
    "\n",
    "```\n",
    "x -> LayerNorm -> MultiHeadAttention -> + (residual) -> LayerNorm -> FFN -> + (residual) -> output\n",
    "|                                       ^              |                    ^\n",
    "+---------------------------------------+              +--------------------+\n",
    "```\n",
    "\n",
    "**Residual connections** add the input back to the output of each sub-layer.\n",
    "This allows gradients to flow directly through the network, enabling training\n",
    "of very deep models (GPT-3 has 96 layers).\n",
    "\n",
    "**Layer normalization** stabilizes training by normalizing activations to have\n",
    "zero mean and unit variance within each token's representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A single transformer block with pre-norm architecture.\n",
    "\n",
    "    Components:\n",
    "        1. LayerNorm -> Multi-Head Attention -> Residual\n",
    "        2. LayerNorm -> Feed-Forward Network -> Residual\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_heads: int,\n",
    "        d_ff: int | None = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: torch.Tensor | None = None,\n",
    "        verbose: bool = True,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass through the transformer block.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len, d_model).\n",
    "            mask: Optional attention mask.\n",
    "            verbose: If True, print shapes at each stage.\n",
    "\n",
    "        Returns:\n",
    "            output: Shape (batch, seq_len, d_model).\n",
    "            attention_weights: Shape (batch, n_heads, seq_len, seq_len).\n",
    "        \"\"\"\n",
    "        # Sub-layer 1: LayerNorm -> Attention -> Residual\n",
    "        normed = self.norm1(x)\n",
    "        if verbose:\n",
    "            print(f\"  After LayerNorm 1:    {normed.shape}\")\n",
    "\n",
    "        attn_out, attn_weights = self.attn(normed, mask)\n",
    "        if verbose:\n",
    "            print(f\"  After Attention:      {attn_out.shape}\")\n",
    "\n",
    "        x = x + attn_out  # Residual connection\n",
    "        if verbose:\n",
    "            print(f\"  After Residual 1:     {x.shape}\")\n",
    "\n",
    "        # Sub-layer 2: LayerNorm -> FFN -> Residual\n",
    "        normed = self.norm2(x)\n",
    "        if verbose:\n",
    "            print(f\"  After LayerNorm 2:    {normed.shape}\")\n",
    "\n",
    "        ffn_out = self.ffn(normed)\n",
    "        if verbose:\n",
    "            print(f\"  After FFN:            {ffn_out.shape}\")\n",
    "\n",
    "        x = x + ffn_out  # Residual connection\n",
    "        if verbose:\n",
    "            print(f\"  After Residual 2:     {x.shape}\")\n",
    "\n",
    "        return x, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and test the full transformer block\n",
    "d_model = 64\n",
    "n_heads = 4\n",
    "batch_size = 1\n",
    "seq_len = 10\n",
    "\n",
    "block = TransformerBlock(d_model=d_model, n_heads=n_heads)\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"\\nShape at each stage:\")\n",
    "output, attn_weights = block(x)\n",
    "\n",
    "print(f\"\\nFinal output shape:      {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "\n",
    "# Count total parameters\n",
    "n_params = sum(p.numel() for p in block.parameters())\n",
    "print(f\"\\nTotal TransformerBlock parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Visualizing Attention on Legal Text\n",
    "\n",
    "Let's create a simple tokenized legal sentence and pass it through our\n",
    "transformer block. We will extract the attention weights and visualize\n",
    "which words attend to which.\n",
    "\n",
    "Since we have not trained the model, the attention patterns are random --\n",
    "but the visualization technique is exactly what researchers use to\n",
    "interpret trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple legal sentence, manually tokenized into words.\n",
    "# In practice you would use a real tokenizer, but word-level\n",
    "# tokens make the attention heatmap interpretable.\n",
    "legal_tokens = [\n",
    "    \"The\", \"court\", \"held\", \"that\", \"the\",\n",
    "    \"defendant\", \"was\", \"liable\", \"for\", \"damages\",\n",
    "]\n",
    "seq_len = len(legal_tokens)\n",
    "d_model = 64\n",
    "n_heads = 4\n",
    "\n",
    "# Create a simple embedding: random vectors for each token.\n",
    "# In a trained model, these would be learned embeddings.\n",
    "torch.manual_seed(123)\n",
    "embeddings = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "# Build a fresh transformer block (untrained)\n",
    "block = TransformerBlock(d_model=d_model, n_heads=n_heads)\n",
    "\n",
    "print(f\"Legal sentence: {' '.join(legal_tokens)}\")\n",
    "print(f\"Number of tokens: {seq_len}\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, attn_weights = block(embeddings)\n",
    "\n",
    "print(f\"\\nAttention weights shape: {attn_weights.shape}\")\n",
    "print(f\"  -> (batch=1, heads={n_heads}, tokens={seq_len}, tokens={seq_len})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_heads(\n",
    "    attention_weights: torch.Tensor,\n",
    "    tokens: list[str],\n",
    "    n_heads: int,\n",
    ") -> None:\n",
    "    \"\"\"Plot attention heatmaps for each head.\n",
    "\n",
    "    Args:\n",
    "        attention_weights: Tensor of shape (1, n_heads, seq_len, seq_len).\n",
    "        tokens: List of token strings for axis labels.\n",
    "        n_heads: Number of attention heads.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, n_heads, figsize=(5 * n_heads, 5))\n",
    "    if n_heads == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for head_idx in range(n_heads):\n",
    "        ax = axes[head_idx]\n",
    "        weights = attention_weights[0, head_idx].detach().numpy()\n",
    "\n",
    "        im = ax.imshow(weights, cmap=\"Blues\", vmin=0, vmax=weights.max())\n",
    "        ax.set_xticks(range(len(tokens)))\n",
    "        ax.set_yticks(range(len(tokens)))\n",
    "        ax.set_xticklabels(tokens, rotation=45, ha=\"right\", fontsize=9)\n",
    "        ax.set_yticklabels(tokens, fontsize=9)\n",
    "        ax.set_title(f\"Head {head_idx + 1}\", fontsize=12)\n",
    "        ax.set_xlabel(\"Key (attending to)\")\n",
    "        if head_idx == 0:\n",
    "            ax.set_ylabel(\"Query (attending from)\")\n",
    "        fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "    fig.suptitle(\n",
    "        'Attention Weights: \"The court held that the defendant was liable for damages\"',\n",
    "        fontsize=13,\n",
    "        y=1.02,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_attention_heads(attn_weights, legal_tokens, n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also plot the average attention across all heads\n",
    "avg_attention = attn_weights[0].mean(dim=0).detach().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "im = ax.imshow(avg_attention, cmap=\"Blues\", vmin=0)\n",
    "ax.set_xticks(range(seq_len))\n",
    "ax.set_yticks(range(seq_len))\n",
    "ax.set_xticklabels(legal_tokens, rotation=45, ha=\"right\", fontsize=10)\n",
    "ax.set_yticklabels(legal_tokens, fontsize=10)\n",
    "ax.set_xlabel(\"Key (attending to)\", fontsize=11)\n",
    "ax.set_ylabel(\"Query (attending from)\", fontsize=11)\n",
    "ax.set_title(\"Average Attention Across All Heads\", fontsize=13)\n",
    "\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        val = avg_attention[i, j]\n",
    "        color = \"white\" if val > avg_attention.max() * 0.6 else \"black\"\n",
    "        ax.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", color=color, fontsize=8)\n",
    "\n",
    "fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Interpreting the Patterns\n",
    "\n",
    "Since this model is untrained, the attention patterns are essentially random.\n",
    "In a trained model, you would see meaningful patterns:\n",
    "\n",
    "- **\"liable\"** strongly attending to **\"defendant\"** (subject of the predicate)\n",
    "- **\"damages\"** attending to **\"liable\"** (semantic dependency)\n",
    "- **\"that\"** attending to **\"held\"** (syntactic relationship)\n",
    "\n",
    "The visualization technique itself is what matters here. Researchers use\n",
    "exactly this approach (with tools like BertViz) to understand what trained\n",
    "transformers learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise (a): Experiment with Head Count\n",
    "\n",
    "How do attention patterns change with different numbers of heads? Try creating\n",
    "transformer blocks with 1, 4, and 8 heads (keeping `d_model=64`).\n",
    "\n",
    "For each configuration:\n",
    "1. Pass the same legal sentence embeddings through the block.\n",
    "2. Visualize the attention patterns.\n",
    "3. Observe: with 1 head, the model has one \"view\" of the sequence. With 8\n",
    "   heads, each head has a smaller dimension (64/8 = 8) but there are 8\n",
    "   different attention patterns.\n",
    "\n",
    "Questions to consider:\n",
    "- Do more heads produce more diverse attention patterns?\n",
    "- What happens to the per-head dimension as you increase head count?\n",
    "- In a trained model, would you expect 8 heads to capture more linguistic\n",
    "  phenomena than 1 head? Why?\n",
    "\n",
    "```python\n",
    "# Starter code\n",
    "for n_heads in [1, 4, 8]:\n",
    "    block = TransformerBlock(d_model=64, n_heads=n_heads)\n",
    "    with torch.no_grad():\n",
    "        _, weights = block(embeddings, verbose=False)\n",
    "    print(f\"\\n--- {n_heads} head(s), d_k={64 // n_heads} per head ---\")\n",
    "    plot_attention_heads(weights, legal_tokens, n_heads)\n",
    "```\n",
    "\n",
    "### Exercise (b): Remove Layer Normalization\n",
    "\n",
    "Layer normalization is critical for stable training. What happens without it?\n",
    "\n",
    "1. Create a modified `TransformerBlock` that replaces `LayerNorm` with an\n",
    "   identity function (`nn.Identity()`).\n",
    "2. Pass the same input through 10 consecutive forward passes (feed the output\n",
    "   back as input each time).\n",
    "3. After each pass, record the mean and standard deviation of the output tensor\n",
    "   and the attention weights.\n",
    "4. Compare with the original block that uses `LayerNorm`.\n",
    "\n",
    "Questions to consider:\n",
    "- Do the activations grow or shrink without normalization?\n",
    "- How do the attention weight distributions change across passes?\n",
    "- Why is this a problem for training deep networks (which stack many blocks)?\n",
    "\n",
    "```python\n",
    "# Starter code\n",
    "class TransformerBlockNoNorm(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.Identity()  # No normalization\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.norm2 = nn.Identity()\n",
    "        self.ffn = FeedForward(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        normed = self.norm1(x)\n",
    "        attn_out, attn_weights = self.attn(normed, mask)\n",
    "        x = x + attn_out\n",
    "        normed = self.norm2(x)\n",
    "        ffn_out = self.ffn(normed)\n",
    "        x = x + ffn_out\n",
    "        return x, attn_weights\n",
    "\n",
    "# Run 10 forward passes, track stats\n",
    "x_input = embeddings.clone()\n",
    "block_no_norm = TransformerBlockNoNorm(d_model=64, n_heads=4)\n",
    "\n",
    "for i in range(10):\n",
    "    with torch.no_grad():\n",
    "        x_input, weights = block_no_norm(x_input)\n",
    "    print(f\"Pass {i+1}: mean={x_input.mean():.4f}, std={x_input.std():.4f}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
