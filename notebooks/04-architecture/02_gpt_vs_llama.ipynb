{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 02 - GPT vs Llama: Architecture Comparison\n",
    "\n",
    "GPT-2 and Llama represent two generations of transformer design. Both are\n",
    "decoder-only language models, but Llama incorporates several architectural\n",
    "improvements discovered between 2019 and 2023. In this notebook, we load\n",
    "GPT-2, inspect its internals, and implement the key innovations that\n",
    "distinguish Llama from GPT.\n",
    "\n",
    "## Architecture Comparison\n",
    "\n",
    "| Component | GPT-2 | Llama |\n",
    "|---|---|---|\n",
    "| Position encoding | Learned absolute | RoPE (rotary) |\n",
    "| Normalization | LayerNorm (post) | RMSNorm (pre) |\n",
    "| Attention | Multi-head | Grouped Query (GQA) |\n",
    "| Activation | GELU | SwiGLU |\n",
    "| Vocabulary | 50,257 | 32,000 |\n",
    "\n",
    "Each of these differences was motivated by specific engineering or performance\n",
    "trade-offs. We will examine them one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Loading Models\n",
    "\n",
    "### GPT-2 Small\n",
    "\n",
    "GPT-2 small (124M parameters) is freely available from HuggingFace. We load\n",
    "the full model with weights so we can run inference and inspect attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 small\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "gpt2_model = gpt2_model.to(DEVICE)\n",
    "\n",
    "print(\"GPT-2 Small Architecture:\")\n",
    "print(\"=\" * 60)\n",
    "print(gpt2_model.config)\n",
    "print()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in gpt2_model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Memory footprint (float32): {total_params * 4 / 1024**2:.1f} MB\")\n",
    "print(f\"Memory footprint (float16): {total_params * 2 / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the full model architecture\n",
    "print(\"GPT-2 Module Hierarchy:\")\n",
    "print(\"=\" * 60)\n",
    "print(gpt2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter breakdown by component\n",
    "print(\"Parameter breakdown:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "component_params = {}\n",
    "for name, param in gpt2_model.named_parameters():\n",
    "    # Group by top-level component\n",
    "    parts = name.split(\".\")\n",
    "    if \"wte\" in name:\n",
    "        component = \"Token Embeddings\"\n",
    "    elif \"wpe\" in name:\n",
    "        component = \"Position Embeddings\"\n",
    "    elif \"ln_f\" in name:\n",
    "        component = \"Final LayerNorm\"\n",
    "    elif \"attn\" in name:\n",
    "        component = \"Attention Layers\"\n",
    "    elif \"mlp\" in name:\n",
    "        component = \"MLP Layers\"\n",
    "    elif \"ln_\" in name:\n",
    "        component = \"Layer Norms\"\n",
    "    else:\n",
    "        component = \"Other\"\n",
    "    component_params[component] = component_params.get(component, 0) + param.numel()\n",
    "\n",
    "for component, count in sorted(component_params.items(), key=lambda x: -x[1]):\n",
    "    pct = 100 * count / total_params\n",
    "    print(f\"  {component:<25s} {count:>12,} params  ({pct:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "### Llama Architecture (Config Only)\n",
    "\n",
    "Llama model weights require authentication through Meta's access program.\n",
    "Instead of loading the full model, we inspect the configuration to understand\n",
    "the architectural differences. We can also create a small Llama-like model\n",
    "from config for experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load just the Llama-2 configuration (no weights needed)\n",
    "# This does not require authentication or downloading model weights.\n",
    "try:\n",
    "    llama_config = AutoConfig.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "    print(\"Llama-2 7B Configuration:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(llama_config)\n",
    "except Exception as e:\n",
    "    print(f\"Could not fetch Llama config from HuggingFace: {e}\")\n",
    "    print(\"\\nUsing known Llama-2 7B configuration values instead:\")\n",
    "    print(\"=\" * 60)\n",
    "    llama_specs = {\n",
    "        \"hidden_size\": 4096,\n",
    "        \"intermediate_size\": 11008,\n",
    "        \"num_attention_heads\": 32,\n",
    "        \"num_key_value_heads\": 32,\n",
    "        \"num_hidden_layers\": 32,\n",
    "        \"vocab_size\": 32000,\n",
    "        \"max_position_embeddings\": 4096,\n",
    "        \"rms_norm_eps\": 1e-5,\n",
    "        \"rope_theta\": 10000.0,\n",
    "        \"hidden_act\": \"silu\",\n",
    "    }\n",
    "    for key, val in llama_specs.items():\n",
    "        print(f\"  {key}: {val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "print(\"Side-by-Side Comparison:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Component':<30s} {'GPT-2 Small':<20s} {'Llama-2 7B':<20s}\")\n",
    "print(\"-\" * 70)\n",
    "comparisons = [\n",
    "    (\"Hidden size\", \"768\", \"4096\"),\n",
    "    (\"Num layers\", \"12\", \"32\"),\n",
    "    (\"Num attention heads\", \"12\", \"32\"),\n",
    "    (\"Num KV heads\", \"12 (MHA)\", \"32 (MHA / GQA in 70B)\"),\n",
    "    (\"FFN inner dim\", \"3072\", \"11008\"),\n",
    "    (\"Vocab size\", \"50257\", \"32000\"),\n",
    "    (\"Max sequence length\", \"1024\", \"4096\"),\n",
    "    (\"Position encoding\", \"Learned absolute\", \"RoPE (rotary)\"),\n",
    "    (\"Normalization\", \"LayerNorm (post)\", \"RMSNorm (pre)\"),\n",
    "    (\"Activation\", \"GELU\", \"SiLU (SwiGLU)\"),\n",
    "    (\"Parameters\", \"124M\", \"6.7B\"),\n",
    "]\n",
    "for component, gpt2_val, llama_val in comparisons:\n",
    "    print(f\"  {component:<30s} {gpt2_val:<20s} {llama_val:<20s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Inspecting GPT-2 Internals\n",
    "\n",
    "Let's run a legal prompt through GPT-2 and extract attention patterns\n",
    "from specific layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a legal prompt\n",
    "legal_prompt = \"The court held that the defendant was liable for\"\n",
    "\n",
    "inputs = gpt2_tokenizer(legal_prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# Decode each token to see how GPT-2 tokenized the text\n",
    "tokens = [gpt2_tokenizer.decode(tok_id) for tok_id in input_ids[0]]\n",
    "print(f\"Input text: {legal_prompt}\")\n",
    "print(f\"Tokens ({len(tokens)}): {tokens}\")\n",
    "print(f\"Token IDs: {input_ids[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run forward pass with attention output\n",
    "with torch.no_grad():\n",
    "    outputs = gpt2_model(\n",
    "        **inputs,\n",
    "        output_attentions=True,\n",
    "    )\n",
    "\n",
    "# outputs.attentions is a tuple: one tensor per layer\n",
    "# Each tensor has shape (batch, n_heads, seq_len, seq_len)\n",
    "print(f\"Number of layers: {len(outputs.attentions)}\")\n",
    "print(f\"Attention shape per layer: {outputs.attentions[0].shape}\")\n",
    "print(f\"  -> (batch=1, heads=12, tokens={len(tokens)}, tokens={len(tokens)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gpt2_attention(\n",
    "    attentions: tuple,\n",
    "    tokens: list[str],\n",
    "    layer: int,\n",
    "    heads: list[int] | None = None,\n",
    ") -> None:\n",
    "    \"\"\"Plot attention heatmaps for specified heads in a given layer.\n",
    "\n",
    "    Args:\n",
    "        attentions: Tuple of attention tensors from model output.\n",
    "        tokens: List of token strings for axis labels.\n",
    "        layer: Layer index to visualize.\n",
    "        heads: List of head indices. If None, shows first 4 heads.\n",
    "    \"\"\"\n",
    "    if heads is None:\n",
    "        heads = list(range(min(4, attentions[layer].shape[1])))\n",
    "\n",
    "    n_heads = len(heads)\n",
    "    fig, axes = plt.subplots(1, n_heads, figsize=(5 * n_heads, 5))\n",
    "    if n_heads == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    layer_attention = attentions[layer][0].cpu().numpy()  # (n_heads, seq, seq)\n",
    "\n",
    "    for idx, head in enumerate(heads):\n",
    "        ax = axes[idx]\n",
    "        weights = layer_attention[head]\n",
    "        im = ax.imshow(weights, cmap=\"Blues\", vmin=0, vmax=weights.max())\n",
    "        ax.set_xticks(range(len(tokens)))\n",
    "        ax.set_yticks(range(len(tokens)))\n",
    "        ax.set_xticklabels(tokens, rotation=45, ha=\"right\", fontsize=9)\n",
    "        ax.set_yticklabels(tokens, fontsize=9)\n",
    "        ax.set_title(f\"Layer {layer}, Head {head}\", fontsize=11)\n",
    "        ax.set_xlabel(\"Key (attending to)\")\n",
    "        if idx == 0:\n",
    "            ax.set_ylabel(\"Query (attending from)\")\n",
    "        fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "\n",
    "    fig.suptitle(\n",
    "        f'GPT-2 Attention (Layer {layer}): \"{legal_prompt}\"',\n",
    "        fontsize=13,\n",
    "        y=1.02,\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize early layer (layer 0) -- often captures local/positional patterns\n",
    "print(\"Layer 0 (early -- often captures positional/local patterns):\")\n",
    "plot_gpt2_attention(outputs.attentions, tokens, layer=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a middle layer (layer 5) -- often captures syntactic patterns\n",
    "print(\"Layer 5 (middle -- often captures syntactic relationships):\")\n",
    "plot_gpt2_attention(outputs.attentions, tokens, layer=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the last layer (layer 11) -- often captures semantic patterns\n",
    "print(\"Layer 11 (final -- often captures high-level semantic patterns):\")\n",
    "plot_gpt2_attention(outputs.attentions, tokens, layer=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average attention across all heads for each layer\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "layers_to_show = [0, 2, 4, 7, 9, 11]\n",
    "\n",
    "for idx, layer in enumerate(layers_to_show):\n",
    "    ax = axes[idx // 3][idx % 3]\n",
    "    avg_attn = outputs.attentions[layer][0].mean(dim=0).cpu().numpy()\n",
    "    im = ax.imshow(avg_attn, cmap=\"Blues\", vmin=0)\n",
    "    ax.set_xticks(range(len(tokens)))\n",
    "    ax.set_yticks(range(len(tokens)))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha=\"right\", fontsize=8)\n",
    "    ax.set_yticklabels(tokens, fontsize=8)\n",
    "    ax.set_title(f\"Layer {layer} (avg across heads)\", fontsize=10)\n",
    "    fig.colorbar(im, ax=ax, shrink=0.7)\n",
    "\n",
    "fig.suptitle(\n",
    "    f'GPT-2 Average Attention Across Layers: \"{legal_prompt}\"',\n",
    "    fontsize=13,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Typical patterns you will see in the GPT-2 attention maps:\n",
    "\n",
    "- **Layer 0**: Strong diagonal pattern (each token attends to itself or the\n",
    "  previous token). This captures local context.\n",
    "- **Middle layers**: More diffuse attention with some tokens acting as \"hubs\"\n",
    "  that many other tokens attend to. Function words like \"the\" and \"that\" often\n",
    "  receive high attention.\n",
    "- **Final layer**: Attention is often concentrated on a few key positions that\n",
    "  are semantically important for next-token prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## RoPE Explained: Rotary Position Embeddings\n",
    "\n",
    "GPT-2 uses **learned absolute position embeddings**: a lookup table of 1024\n",
    "vectors (one per position), added to the token embeddings. This has two\n",
    "limitations:\n",
    "\n",
    "1. The model cannot generalize to sequences longer than 1024 tokens.\n",
    "2. Position information is added once at the input and must survive through\n",
    "   all layers via residual connections.\n",
    "\n",
    "Llama uses **Rotary Position Embeddings (RoPE)**, which encode position\n",
    "by rotating the query and key vectors at each attention layer. The key\n",
    "properties:\n",
    "\n",
    "- Position is encoded **relative**: the attention score between positions\n",
    "  $m$ and $n$ depends only on $m - n$, not on the absolute values.\n",
    "- The rotation is applied at **every layer**, reinforcing position information.\n",
    "- It supports **extrapolation** to longer sequences (with some degradation).\n",
    "\n",
    "### How RoPE Works\n",
    "\n",
    "RoPE rotates pairs of dimensions in the query/key vectors by an angle\n",
    "proportional to the position. For a 2D example, position $m$ is encoded as:\n",
    "\n",
    "$$R_m = \\begin{pmatrix} \\cos(m\\theta) & -\\sin(m\\theta) \\\\ \\sin(m\\theta) & \\cos(m\\theta) \\end{pmatrix}$$\n",
    "\n",
    "Applied to query $q$ at position $m$ and key $k$ at position $n$:\n",
    "\n",
    "$$q_m^T k_n = (R_m q)^T (R_n k) = q^T R_{n-m} k$$\n",
    "\n",
    "The dot product depends only on the **relative position** $n - m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rope_frequencies(\n",
    "    d_model: int,\n",
    "    max_seq_len: int,\n",
    "    theta: float = 10000.0,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Precompute the RoPE rotation frequencies.\n",
    "\n",
    "    Args:\n",
    "        d_model: Model dimension (must be even).\n",
    "        max_seq_len: Maximum sequence length.\n",
    "        theta: Base frequency (default 10000, as in the original paper).\n",
    "\n",
    "    Returns:\n",
    "        Complex tensor of shape (max_seq_len, d_model // 2) containing\n",
    "        the rotation factors as complex exponentials.\n",
    "    \"\"\"\n",
    "    assert d_model % 2 == 0, \"d_model must be even for RoPE\"\n",
    "\n",
    "    # Frequency for each pair of dimensions: theta_i = 1 / (theta^(2i/d))\n",
    "    dim_indices = torch.arange(0, d_model, 2).float()  # [0, 2, 4, ...]\n",
    "    freqs = 1.0 / (theta ** (dim_indices / d_model))   # (d_model // 2,)\n",
    "\n",
    "    # Positions\n",
    "    positions = torch.arange(max_seq_len).float()  # [0, 1, 2, ..., max_seq_len-1]\n",
    "\n",
    "    # Outer product: angle for each (position, dimension_pair)\n",
    "    angles = torch.outer(positions, freqs)  # (max_seq_len, d_model // 2)\n",
    "\n",
    "    # Convert to complex exponentials: e^(i * angle) = cos(angle) + i*sin(angle)\n",
    "    freqs_complex = torch.polar(torch.ones_like(angles), angles)\n",
    "    return freqs_complex\n",
    "\n",
    "\n",
    "# Precompute frequencies\n",
    "d_model = 16  # Small for visualization\n",
    "max_seq_len = 64\n",
    "rope_freqs = compute_rope_frequencies(d_model, max_seq_len)\n",
    "print(f\"RoPE frequencies shape: {rope_freqs.shape}\")\n",
    "print(f\"  -> (positions={max_seq_len}, dimension_pairs={d_model // 2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rope(\n",
    "    x: torch.Tensor,\n",
    "    freqs: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Apply rotary position embeddings to a tensor.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (batch, seq_len, d_model).\n",
    "        freqs: Precomputed complex frequencies of shape (seq_len, d_model // 2).\n",
    "\n",
    "    Returns:\n",
    "        Rotated tensor of the same shape as x.\n",
    "    \"\"\"\n",
    "    # Reshape x into pairs of dimensions and interpret as complex numbers\n",
    "    batch, seq_len, d = x.shape\n",
    "    x_pairs = x.float().reshape(batch, seq_len, -1, 2)\n",
    "    x_complex = torch.view_as_complex(x_pairs)  # (batch, seq_len, d_model // 2)\n",
    "\n",
    "    # Multiply by rotation factors (broadcasting over batch)\n",
    "    freqs_slice = freqs[:seq_len]  # (seq_len, d_model // 2)\n",
    "    x_rotated = x_complex * freqs_slice.unsqueeze(0)  # element-wise complex mult\n",
    "\n",
    "    # Convert back to real pairs and reshape\n",
    "    x_out = torch.view_as_real(x_rotated)  # (batch, seq_len, d_model // 2, 2)\n",
    "    x_out = x_out.reshape(batch, seq_len, d)\n",
    "    return x_out.type_as(x)\n",
    "\n",
    "\n",
    "# Test: apply RoPE to a random tensor\n",
    "x = torch.randn(1, 8, d_model)\n",
    "x_rotated = apply_rope(x, rope_freqs)\n",
    "print(f\"Input shape:   {x.shape}\")\n",
    "print(f\"Rotated shape: {x_rotated.shape}\")\n",
    "print(f\"\\nFirst token before RoPE: {x[0, 0, :4].tolist()}\")\n",
    "print(f\"First token after RoPE:  {x_rotated[0, 0, :4].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the relative position property:\n",
    "# The dot product of rotated q at position m and rotated k at position n\n",
    "# should depend only on (m - n), not on m and n individually.\n",
    "\n",
    "torch.manual_seed(42)\n",
    "q = torch.randn(1, 1, d_model)  # single query vector\n",
    "k = torch.randn(1, 1, d_model)  # single key vector\n",
    "\n",
    "print(\"Verifying relative position property:\")\n",
    "print(\"  q.k at different absolute positions but same relative distance\\n\")\n",
    "\n",
    "for offset in [0, 5, 10, 20]:\n",
    "    # Place q at position (offset) and k at position (offset + 3)\n",
    "    # Relative distance is always 3\n",
    "    q_at_pos = apply_rope(\n",
    "        q,\n",
    "        rope_freqs[offset : offset + 1],\n",
    "    )\n",
    "    k_at_pos = apply_rope(\n",
    "        k,\n",
    "        rope_freqs[offset + 3 : offset + 4],\n",
    "    )\n",
    "    dot = torch.sum(q_at_pos * k_at_pos).item()\n",
    "    print(f\"  q@pos={offset:2d}, k@pos={offset+3:2d}  (dist=3)  dot={dot:.6f}\")\n",
    "\n",
    "print(\"\\n  All dot products are identical -- position is relative.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the rotation angles across positions and dimensions\n",
    "angles = torch.outer(\n",
    "    torch.arange(max_seq_len).float(),\n",
    "    1.0 / (10000.0 ** (torch.arange(0, d_model, 2).float() / d_model)),\n",
    ")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap of rotation angles\n",
    "im = ax1.imshow(angles.numpy(), aspect=\"auto\", cmap=\"viridis\")\n",
    "ax1.set_xlabel(\"Dimension pair index\")\n",
    "ax1.set_ylabel(\"Position\")\n",
    "ax1.set_title(\"RoPE Rotation Angles\")\n",
    "fig.colorbar(im, ax=ax1, shrink=0.8, label=\"Angle (radians)\")\n",
    "\n",
    "# Sinusoidal patterns for different dimension pairs\n",
    "positions = np.arange(max_seq_len)\n",
    "for dim_pair in [0, 2, 4, 6]:\n",
    "    ax2.plot(\n",
    "        positions,\n",
    "        np.cos(angles[:, dim_pair].numpy()),\n",
    "        label=f\"dim pair {dim_pair}\",\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "ax2.set_xlabel(\"Position\")\n",
    "ax2.set_ylabel(\"cos(angle)\")\n",
    "ax2.set_title(\"RoPE Cosine Components by Dimension Pair\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Low-index dimension pairs oscillate rapidly (fine position encoding).\")\n",
    "print(\"High-index dimension pairs oscillate slowly (coarse position encoding).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## RMSNorm vs LayerNorm\n",
    "\n",
    "GPT-2 uses standard **LayerNorm**, which normalizes by subtracting the mean\n",
    "and dividing by the standard deviation:\n",
    "\n",
    "$$\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sigma + \\epsilon} + \\beta$$\n",
    "\n",
    "Llama uses **RMSNorm** (Root Mean Square Normalization), which is simpler --\n",
    "it skips the mean subtraction:\n",
    "\n",
    "$$\\text{RMSNorm}(x) = \\gamma \\cdot \\frac{x}{\\text{RMS}(x) + \\epsilon}$$\n",
    "\n",
    "where $\\text{RMS}(x) = \\sqrt{\\frac{1}{d} \\sum_{i=1}^d x_i^2}$.\n",
    "\n",
    "**Why RMSNorm?**\n",
    "- Empirically performs as well as LayerNorm for language modeling.\n",
    "- Computationally cheaper: no need to compute the mean or the bias term.\n",
    "- Fewer parameters (no bias $\\beta$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization (as used in Llama).\n",
    "\n",
    "    Normalizes inputs by their RMS value, without mean subtraction.\n",
    "    Simpler and faster than standard LayerNorm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, eps: float = 1e-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))  # learnable scale\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply RMS normalization.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (..., d_model).\n",
    "\n",
    "        Returns:\n",
    "            Normalized tensor of the same shape.\n",
    "        \"\"\"\n",
    "        # RMS = sqrt(mean(x^2))\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        return self.weight * (x / rms)\n",
    "\n",
    "\n",
    "# Compare LayerNorm and RMSNorm\n",
    "d = 64\n",
    "layer_norm = nn.LayerNorm(d)\n",
    "rms_norm = RMSNorm(d)\n",
    "\n",
    "x = torch.randn(2, 10, d)  # (batch, seq_len, d_model)\n",
    "\n",
    "ln_out = layer_norm(x)\n",
    "rms_out = rms_norm(x)\n",
    "\n",
    "print(\"Comparison on a sample input:\")\n",
    "print(f\"  Input mean:     {x.mean(dim=-1)[0, 0]:.4f}\")\n",
    "print(f\"  Input std:      {x.std(dim=-1)[0, 0]:.4f}\")\n",
    "print()\n",
    "print(f\"  LayerNorm mean: {ln_out.mean(dim=-1)[0, 0]:.6f}  (centered to ~0)\")\n",
    "print(f\"  LayerNorm std:  {ln_out.std(dim=-1)[0, 0]:.4f}\")\n",
    "print()\n",
    "print(f\"  RMSNorm mean:   {rms_out.mean(dim=-1)[0, 0]:.6f}  (NOT centered)\")\n",
    "print(f\"  RMSNorm std:    {rms_out.std(dim=-1)[0, 0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computational comparison\n",
    "print(\"Computational difference:\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"LayerNorm:\")\n",
    "print(\"  1. Compute mean:    mu = mean(x)\")\n",
    "print(\"  2. Subtract mean:   x_centered = x - mu\")\n",
    "print(\"  3. Compute var:     var = mean(x_centered^2)\")\n",
    "print(\"  4. Normalize:       x_norm = x_centered / sqrt(var + eps)\")\n",
    "print(\"  5. Scale and shift: out = gamma * x_norm + beta\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in layer_norm.parameters())} (weight + bias)\")\n",
    "print()\n",
    "print(\"RMSNorm:\")\n",
    "print(\"  1. Compute RMS:     rms = sqrt(mean(x^2))\")\n",
    "print(\"  2. Normalize:       x_norm = x / rms\")\n",
    "print(\"  3. Scale:           out = gamma * x_norm\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in rms_norm.parameters())} (weight only, no bias)\")\n",
    "print()\n",
    "print(\"RMSNorm saves: mean computation, mean subtraction, bias addition.\")\n",
    "print(\"At scale (d_model=4096, billions of tokens), this adds up.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the difference between LayerNorm and RMSNorm\n",
    "# on a single vector\n",
    "torch.manual_seed(7)\n",
    "x_sample = torch.randn(1, 1, d) * 3 + 2  # mean ~2, larger std\n",
    "\n",
    "ln_sample = layer_norm(x_sample)\n",
    "rms_sample = rms_norm(x_sample)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "dims = range(d)\n",
    "axes[0].bar(dims, x_sample[0, 0].detach().numpy(), color=\"#6b7280\", width=0.8)\n",
    "axes[0].set_title(\"Original\", fontsize=12)\n",
    "axes[0].set_xlabel(\"Dimension\")\n",
    "axes[0].set_ylabel(\"Value\")\n",
    "axes[0].axhline(y=0, color=\"black\", linewidth=0.5)\n",
    "\n",
    "axes[1].bar(dims, ln_sample[0, 0].detach().numpy(), color=\"#2563eb\", width=0.8)\n",
    "axes[1].set_title(\"After LayerNorm\", fontsize=12)\n",
    "axes[1].set_xlabel(\"Dimension\")\n",
    "axes[1].axhline(y=0, color=\"black\", linewidth=0.5)\n",
    "\n",
    "axes[2].bar(dims, rms_sample[0, 0].detach().numpy(), color=\"#dc2626\", width=0.8)\n",
    "axes[2].set_title(\"After RMSNorm\", fontsize=12)\n",
    "axes[2].set_xlabel(\"Dimension\")\n",
    "axes[2].axhline(y=0, color=\"black\", linewidth=0.5)\n",
    "\n",
    "# Use same y limits for comparison\n",
    "y_min = min(x_sample.min().item(), ln_sample.min().item(), rms_sample.min().item()) - 0.5\n",
    "y_max = max(x_sample.max().item(), ln_sample.max().item(), rms_sample.max().item()) + 0.5\n",
    "for ax in axes:\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "fig.suptitle(\"LayerNorm vs RMSNorm: Effect on a Single Vector\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: LayerNorm centers the values around 0 (mean subtracted).\")\n",
    "print(\"RMSNorm only rescales -- the mean offset is preserved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## SwiGLU Activation (Bonus)\n",
    "\n",
    "GPT-2 uses GELU as its FFN activation. Llama uses **SwiGLU**, which combines\n",
    "the Swish activation with a gating mechanism:\n",
    "\n",
    "$$\\text{SwiGLU}(x) = (x W_1) \\odot \\text{SiLU}(x W_{gate})$$\n",
    "\n",
    "where SiLU (also called Swish) is $\\text{SiLU}(x) = x \\cdot \\sigma(x)$.\n",
    "\n",
    "The gating mechanism lets the network learn to selectively pass information,\n",
    "which empirically improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwiGLUFeedForward(nn.Module):\n",
    "    \"\"\"SwiGLU feed-forward network as used in Llama.\n",
    "\n",
    "    Uses a gated linear unit with SiLU activation instead of\n",
    "    the standard two-layer FFN with GELU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int | None = None) -> None:\n",
    "        super().__init__()\n",
    "        if d_ff is None:\n",
    "            # Llama uses 8/3 * d_model, rounded to nearest multiple of 256\n",
    "            d_ff = int(8 * d_model / 3)\n",
    "            d_ff = ((d_ff + 255) // 256) * 256\n",
    "\n",
    "        self.w1 = nn.Linear(d_model, d_ff, bias=False)    # up projection\n",
    "        self.w2 = nn.Linear(d_ff, d_model, bias=False)     # down projection\n",
    "        self.w_gate = nn.Linear(d_model, d_ff, bias=False)  # gate projection\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass with SwiGLU gating.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len, d_model).\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (batch, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        return self.w2(F.silu(self.w_gate(x)) * self.w1(x))\n",
    "\n",
    "\n",
    "# Compare parameter counts\n",
    "d_model = 64\n",
    "gelu_ffn_params = sum(\n",
    "    p.numel()\n",
    "    for p in nn.Sequential(\n",
    "        nn.Linear(d_model, 4 * d_model),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(4 * d_model, d_model),\n",
    "    ).parameters()\n",
    ")\n",
    "swiglu_ffn = SwiGLUFeedForward(d_model)\n",
    "swiglu_ffn_params = sum(p.numel() for p in swiglu_ffn.parameters())\n",
    "\n",
    "print(f\"GELU FFN parameters:   {gelu_ffn_params:,}\")\n",
    "print(f\"SwiGLU FFN parameters: {swiglu_ffn_params:,}\")\n",
    "print(f\"\\nSwiGLU has ~50% more parameters due to the third weight matrix (gate).\")\n",
    "print(\"But it achieves better loss per parameter, making it a net win.\")\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(1, 8, d_model)\n",
    "out = swiglu_ffn(x)\n",
    "print(f\"\\nSwiGLU FFN input:  {x.shape}\")\n",
    "print(f\"SwiGLU FFN output: {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Grouped Query Attention (GQA)\n",
    "\n",
    "Standard multi-head attention (as in GPT-2) uses separate K/V projections\n",
    "for every head. **Grouped Query Attention** (GQA), used in Llama-2 70B and\n",
    "later models, shares K/V heads across groups of query heads.\n",
    "\n",
    "- **MHA** (GPT-2): 12 Q heads, 12 K heads, 12 V heads\n",
    "- **GQA** (Llama-2 70B): 64 Q heads, 8 K heads, 8 V heads\n",
    "- **MQA** (extreme): N Q heads, 1 K head, 1 V head\n",
    "\n",
    "GQA reduces the memory needed for the KV cache during inference, which is\n",
    "the main bottleneck for long-context generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedQueryAttention(nn.Module):\n",
    "    \"\"\"Grouped Query Attention (GQA) as used in Llama-2 70B.\n",
    "\n",
    "    Multiple query heads share fewer key-value heads, reducing\n",
    "    the KV cache memory during inference.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        n_q_heads: int,\n",
    "        n_kv_heads: int,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        assert d_model % n_q_heads == 0, \"d_model must be divisible by n_q_heads\"\n",
    "        assert n_q_heads % n_kv_heads == 0, \"n_q_heads must be divisible by n_kv_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.n_q_heads = n_q_heads\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.d_k = d_model // n_q_heads\n",
    "        self.n_groups = n_q_heads // n_kv_heads  # Q heads per KV head\n",
    "\n",
    "        # Q has full number of heads; K and V have fewer\n",
    "        self.W_q = nn.Linear(d_model, n_q_heads * self.d_k, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, n_kv_heads * self.d_k, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, n_kv_heads * self.d_k, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: torch.Tensor | None = None,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass with grouped query attention.\n",
    "\n",
    "        Args:\n",
    "            x: Input of shape (batch, seq_len, d_model).\n",
    "            mask: Optional attention mask.\n",
    "\n",
    "        Returns:\n",
    "            output: Shape (batch, seq_len, d_model).\n",
    "            attention_weights: Shape (batch, n_q_heads, seq_len, seq_len).\n",
    "        \"\"\"\n",
    "        batch, seq_len, _ = x.shape\n",
    "\n",
    "        # Project Q, K, V\n",
    "        Q = self.W_q(x).view(batch, seq_len, self.n_q_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch, seq_len, self.n_kv_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch, seq_len, self.n_kv_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Repeat K and V for each group of Q heads\n",
    "        # (batch, n_kv_heads, seq, d_k) -> (batch, n_q_heads, seq, d_k)\n",
    "        K = K.repeat_interleave(self.n_groups, dim=1)\n",
    "        V = V.repeat_interleave(self.n_groups, dim=1)\n",
    "\n",
    "        # Standard scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, float(\"-inf\"))\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_output = torch.matmul(attn_weights, V)\n",
    "\n",
    "        # Concatenate heads\n",
    "        attn_output = (\n",
    "            attn_output.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch, seq_len, self.d_model)\n",
    "        )\n",
    "        output = self.W_o(attn_output)\n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "# Compare parameter counts: MHA vs GQA\n",
    "d_model = 64\n",
    "n_q_heads = 8\n",
    "\n",
    "print(f\"d_model={d_model}, n_q_heads={n_q_heads}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for n_kv_heads in [8, 4, 2, 1]:\n",
    "    gqa = GroupedQueryAttention(d_model, n_q_heads, n_kv_heads)\n",
    "    n_params = sum(p.numel() for p in gqa.parameters())\n",
    "    kv_cache_per_token = 2 * n_kv_heads * (d_model // n_q_heads)  # K + V\n",
    "\n",
    "    if n_kv_heads == n_q_heads:\n",
    "        label = \"MHA\"\n",
    "    elif n_kv_heads == 1:\n",
    "        label = \"MQA\"\n",
    "    else:\n",
    "        label = \"GQA\"\n",
    "\n",
    "    print(\n",
    "        f\"  {label:>3s} (kv_heads={n_kv_heads}): \"\n",
    "        f\"{n_params:>6,} params, \"\n",
    "        f\"KV cache/token: {kv_cache_per_token} values\"\n",
    "    )\n",
    "\n",
    "# Test forward pass\n",
    "gqa = GroupedQueryAttention(d_model=64, n_q_heads=8, n_kv_heads=2)\n",
    "x = torch.randn(1, 10, 64)\n",
    "out, weights = gqa(x)\n",
    "print(f\"\\nGQA test (8 Q heads, 2 KV heads):\")\n",
    "print(f\"  Input:  {x.shape}\")\n",
    "print(f\"  Output: {out.shape}\")\n",
    "print(f\"  Weights: {weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The architectural evolution from GPT-2 to Llama represents three years of\n",
    "practical lessons in scaling transformer models:\n",
    "\n",
    "| Innovation | Why It Matters |\n",
    "|---|---|\n",
    "| **RoPE** | Relative position encoding that generalizes better to long sequences. Critical for legal documents that can run to thousands of tokens. |\n",
    "| **RMSNorm** | Simpler normalization that trains just as well but uses less compute. At 7B+ parameters, every saved operation matters. |\n",
    "| **SwiGLU** | Gated activation that achieves better loss-per-parameter. Empirically outperforms GELU at scale. |\n",
    "| **GQA** | Reduces KV cache memory during inference without hurting quality. Essential for serving long-context models. |\n",
    "\n",
    "For legal AI applications, these improvements directly translate to:\n",
    "- Longer context windows (RoPE enables 4K+ tokens vs GPT-2's 1024)\n",
    "- Faster inference (RMSNorm, GQA)\n",
    "- Better per-parameter performance (SwiGLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise (a): Implement RoPE from Scratch and Verify Rotation\n",
    "\n",
    "The RoPE implementation above uses complex number arithmetic. Reimplement it\n",
    "using only real-valued operations (sin/cos rotations applied to pairs of\n",
    "dimensions).\n",
    "\n",
    "For each pair of dimensions $(x_{2i}, x_{2i+1})$ at position $m$:\n",
    "\n",
    "$$x'_{2i} = x_{2i} \\cos(m\\theta_i) - x_{2i+1} \\sin(m\\theta_i)$$\n",
    "$$x'_{2i+1} = x_{2i} \\sin(m\\theta_i) + x_{2i+1} \\cos(m\\theta_i)$$\n",
    "\n",
    "Verify that your real-valued implementation produces the same results as\n",
    "the complex-number version by comparing outputs on a test vector.\n",
    "\n",
    "```python\n",
    "# Starter code\n",
    "def apply_rope_real(x, freqs_cos, freqs_sin):\n",
    "    \"\"\"Apply RoPE using only real-valued sin/cos operations.\"\"\"\n",
    "    batch, seq_len, d = x.shape\n",
    "    x_pairs = x.reshape(batch, seq_len, -1, 2)\n",
    "    x_even = x_pairs[..., 0]  # (batch, seq, d//2)\n",
    "    x_odd = x_pairs[..., 1]\n",
    "\n",
    "    # Apply rotation\n",
    "    # x_even' = x_even * cos - x_odd * sin\n",
    "    # x_odd'  = x_even * sin + x_odd * cos\n",
    "    # ... your implementation here ...\n",
    "\n",
    "    return x_out\n",
    "```\n",
    "\n",
    "### Exercise (b): Attention Speed Comparison\n",
    "\n",
    "Compare the inference speed of standard multi-head attention (MHA) vs grouped\n",
    "query attention (GQA) at different sequence lengths.\n",
    "\n",
    "1. Create both an MHA module (`n_kv_heads = n_q_heads`) and a GQA module\n",
    "   (`n_kv_heads = n_q_heads // 4`).\n",
    "2. For sequence lengths [64, 128, 256, 512, 1024], time 100 forward passes\n",
    "   of each module.\n",
    "3. Plot the results: sequence length vs. time for both MHA and GQA.\n",
    "\n",
    "Questions to consider:\n",
    "- At what sequence length does GQA start showing a meaningful speedup?\n",
    "- How does the speedup ratio change with sequence length?\n",
    "- Why does GQA matter more for long sequences?\n",
    "\n",
    "```python\n",
    "# Starter code\n",
    "import time\n",
    "\n",
    "d_model = 256\n",
    "n_q_heads = 8\n",
    "seq_lengths = [64, 128, 256, 512, 1024]\n",
    "\n",
    "mha = GroupedQueryAttention(d_model, n_q_heads, n_kv_heads=8)   # Full MHA\n",
    "gqa = GroupedQueryAttention(d_model, n_q_heads, n_kv_heads=2)   # GQA\n",
    "\n",
    "mha_times = []\n",
    "gqa_times = []\n",
    "\n",
    "for seq_len in seq_lengths:\n",
    "    x = torch.randn(1, seq_len, d_model)\n",
    "\n",
    "    # Time MHA\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(100):\n",
    "        with torch.no_grad():\n",
    "            mha(x)\n",
    "    mha_times.append(time.perf_counter() - start)\n",
    "\n",
    "    # Time GQA\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(100):\n",
    "        with torch.no_grad():\n",
    "            gqa(x)\n",
    "    gqa_times.append(time.perf_counter() - start)\n",
    "\n",
    "# Plot results\n",
    "# ... your plotting code here ...\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
