{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Exploring Common Crawl WARC Files\n",
    "\n",
    "## Why Data Collection Matters for Legal AI\n",
    "\n",
    "Every large language model starts with data. GPT, LLaMA, and other foundation\n",
    "models are trained on massive text corpora, and **Common Crawl** is one of the\n",
    "largest publicly available sources. It contains petabytes of web pages collected\n",
    "over more than a decade.\n",
    "\n",
    "For legal AI systems like CoCounsel, the composition of training data directly\n",
    "affects the model's ability to reason about statutes, case law, and legal\n",
    "concepts. Understanding *where* the data comes from -- and how little of a\n",
    "general web crawl is actually legal content -- is the first step toward building\n",
    "better legal AI.\n",
    "\n",
    "### What is the WARC Format?\n",
    "\n",
    "Common Crawl stores its data in **WARC (Web ARChive)** format, an ISO standard\n",
    "(ISO 28500:2017) for archiving web content. A single WARC file bundles together:\n",
    "\n",
    "- **`warcinfo`** records -- metadata about the crawl itself\n",
    "- **`request`** records -- the HTTP request sent to the server\n",
    "- **`response`** records -- the full HTTP response including headers and HTML body\n",
    "- **`metadata`** records -- additional information about the crawl\n",
    "\n",
    "A typical WARC segment from Common Crawl is about 1 GB compressed and contains\n",
    "roughly 30,000-40,000 web pages. The full monthly crawl consists of tens of\n",
    "thousands of such segments.\n",
    "\n",
    "> **Note:** This notebook requires network access to download WARC data from\n",
    "> Common Crawl. Cells that need network access are marked with a\n",
    "> `[REQUIRES NETWORK]` label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install and import the libraries we need:\n",
    "- **warcio** -- reading and writing WARC files\n",
    "- **beautifulsoup4** -- parsing HTML content\n",
    "- **requests** -- downloading files from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# %pip install warcio beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from warcio.archiveiterator import ArchiveIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Helper: download a WARC segment (streaming, with a byte limit so we don't\n",
    "# pull down the entire 1 GB file during exploration).\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "SAMPLE_WARC_URL = (\n",
    "    \"https://data.commoncrawl.org/crawl-data/CC-MAIN-2026-04/\"\n",
    "    \"segments/1768220467618.22/warc/\"\n",
    "    \"CC-MAIN-20260112161239-20260112191239-00000.warc.gz\"\n",
    ")\n",
    "\n",
    "\n",
    "def download_warc_sample(\n",
    "    url: str = SAMPLE_WARC_URL,\n",
    "    max_bytes: int = 5 * 1024 * 1024,  # 5 MB default -- enough for ~100-200 pages\n",
    ") -> io.BytesIO:\n",
    "    \"\"\"Stream the first `max_bytes` of a WARC file into an in-memory buffer.\"\"\"\n",
    "    print(f\"Downloading first {max_bytes / 1024 / 1024:.1f} MB from:\")\n",
    "    print(f\"  {url}\")\n",
    "    headers = {\"Range\": f\"bytes=0-{max_bytes - 1}\"}\n",
    "    resp = requests.get(url, headers=headers, stream=True, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    buf = io.BytesIO(resp.content)\n",
    "    print(f\"Downloaded {len(resp.content):,} bytes.\")\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring WARC Files\n",
    "\n",
    "Let's download a small slice of a real Common Crawl WARC segment and see what's\n",
    "inside. We limit the download to 5 MB so it finishes quickly.\n",
    "\n",
    "> **[REQUIRES NETWORK]** -- The cell below downloads data from\n",
    "> `data.commoncrawl.org`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading first 5.0 MB from:\n",
      "  https://data.commoncrawl.org/crawl-data/CC-MAIN-2026-04/segments/1768220467618.22/warc/CC-MAIN-20260112161239-20260112191239-00000.warc.gz\n",
      "Downloaded 5,242,880 bytes.\n"
     ]
    }
   ],
   "source": [
    "# [REQUIRES NETWORK] -- Download a 5 MB sample of a WARC segment.\n",
    "warc_buffer = download_warc_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records parsed: 932\n"
     ]
    }
   ],
   "source": [
    "# Parse every record in our sample and collect basic stats.\n",
    "records = []\n",
    "\n",
    "warc_buffer.seek(0)\n",
    "for record in ArchiveIterator(warc_buffer):\n",
    "    rec_type = record.rec_type\n",
    "    url = record.rec_headers.get_header(\"WARC-Target-URI\") or \"\"\n",
    "    content_length = int(\n",
    "        record.rec_headers.get_header(\"Content-Length\") or 0\n",
    "    )\n",
    "    # Read the payload so the iterator advances properly\n",
    "    payload = record.content_stream().read()\n",
    "    records.append(\n",
    "        {\n",
    "            \"type\": rec_type,\n",
    "            \"url\": url,\n",
    "            \"content_length\": content_length,\n",
    "            \"payload\": payload,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"Total records parsed: {len(records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record types:\n",
      "  request         311\n",
      "  response        310\n",
      "  metadata        310\n",
      "  warcinfo          1\n"
     ]
    }
   ],
   "source": [
    "# How many records of each type?\n",
    "type_counts = Counter(r[\"type\"] for r in records)\n",
    "print(\"Record types:\")\n",
    "for rtype, count in type_counts.most_common():\n",
    "    print(f\"  {rtype:12s}  {count:>5,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response records: 310\n",
      "\n",
      "Sample URL: http://028700.com/video/310005.html\n",
      "Payload size: 109,599 bytes\n",
      "\n",
      "--- Raw payload (first 500 chars) ---\n",
      "<!DOCTYPE html>\n",
      "<html>\n",
      "\t<head>\n",
      "\t\t<title>《全民诡异：开局掌握零元购·动态漫画-西施程美嘉无码》第1集在线播放-全集选集-高清视频在线播放-蘑菇视频</title>\n",
      "\t\t<meta name=\"keywords\" content=\"全民诡异：开局掌握零元购·动态漫画-西施程美嘉无码正在播放,热播高清电影,免费电视剧,高清综艺,精彩短剧,日韩精品,国产经典,欧美精品,蘑菇视频\" />\n",
      "\t\t<meta name=\"description\" content=\"全民诡异：开局掌握零�\n"
     ]
    }
   ],
   "source": [
    "# Extract HTML content from 'response' records and show a sample.\n",
    "response_records = [r for r in records if r[\"type\"] == \"response\"]\n",
    "print(f\"Response records: {len(response_records)}\")\n",
    "\n",
    "if response_records:\n",
    "    sample = response_records[0]\n",
    "    print(f\"\\nSample URL: {sample['url']}\")\n",
    "    print(f\"Payload size: {len(sample['payload']):,} bytes\")\n",
    "    # Show the first 500 characters of the raw payload\n",
    "    print(\"\\n--- Raw payload (first 500 chars) ---\")\n",
    "    print(sample[\"payload\"][:500].decode(\"utf-8\", errors=\"replace\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Even from this small sample, you can see that:\n",
    "1. Each web page generates multiple WARC records (request + response at minimum).\n",
    "2. The `response` payload includes full HTTP headers followed by the HTML body.\n",
    "3. The raw content is noisy -- navigation menus, ads, JavaScript, and boilerplate\n",
    "   surround the actual page text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering for Legal Content\n",
    "\n",
    "Common Crawl is a *general* web crawl. Let's find out what fraction of the\n",
    "pages we downloaded come from legal domains.\n",
    "\n",
    "We'll define a simple heuristic: a URL is \"legal-related\" if it contains any of\n",
    "the substrings `.gov`, `court`, `law`, or `legal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total response records : 310\n",
      "Legal-domain records   : 4\n",
      "Percentage             : 1.29%\n"
     ]
    }
   ],
   "source": [
    "LEGAL_KEYWORDS = [\"gov\", \"court\", \"law\", \"legal\"]\n",
    "\n",
    "\n",
    "def is_legal_url(url: str) -> bool:\n",
    "    \"\"\"Return True if the URL likely points to a legal-domain page.\"\"\"\n",
    "    url_lower = url.lower()\n",
    "    return any(kw in url_lower for kw in LEGAL_KEYWORDS)\n",
    "\n",
    "\n",
    "legal_records = [r for r in response_records if is_legal_url(r[\"url\"])]\n",
    "total = len(response_records)\n",
    "legal_count = len(legal_records)\n",
    "\n",
    "print(f\"Total response records : {total}\")\n",
    "print(f\"Legal-domain records   : {legal_count}\")\n",
    "if total > 0:\n",
    "    print(f\"Percentage             : {legal_count / total * 100:.2f}%\")\n",
    "else:\n",
    "    print(\"No response records found (is the WARC download empty?)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_html(raw_payload: bytes) -> str:\n",
    "    \"\"\"Extract visible text from an HTTP response payload.\n",
    "\n",
    "    The payload starts with HTTP headers, followed by a blank line, then the\n",
    "    HTML body. We split on the first double-newline to skip the headers.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = raw_payload.decode(\"utf-8\", errors=\"replace\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    # Skip HTTP headers\n",
    "    parts = text.split(\"\\r\\n\\r\\n\", 1)\n",
    "    html = parts[1] if len(parts) > 1 else text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Remove script and style elements\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\"]):\n",
    "        tag.decompose()\n",
    "    return soup.get_text(separator=\" \", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: http://bdlaws.minlaw.gov.bd/act-print-1246/section-print-47268.html\n",
      "Extracted text (first 300 chars):\n",
      "  প্রিন্ট 12/01/2026 Laws of Bangladesh ক্যান্টনমেন্ট আইন, ২০১৮ (\n",
      "                                \n",
      "                                    \n",
      "\n",
      "                                        ২০১৮ সনের ২৭ নং\n",
      "                                        \n",
      "                                            \n",
      "                \n",
      "------------------------------------------------------------------------\n",
      "URL: http://bittencourtadv.com/2023/05/03/betting-company-mostbet-app-online-sports-betting-41/\n",
      "Extracted text (first 300 chars):\n",
      "  Betting Company Mostbet App Online Sports Betting 41 - Bittencourt Advocacia Skip to content Betting Company Mostbet App Online Sports Betting 41 Bittencourt Advocacia > Sem categoria > Betting Company Mostbet App Online Sports Betting 41 Betting Company Mostbet App Online Sports Betting 418 Mostbet\n",
      "------------------------------------------------------------------------\n",
      "URL: http://camplegal.com/features/texting-and-client-communication/\n",
      "Extracted text (first 300 chars):\n",
      "  function openTabFromHash() {\n",
      "    var hash = window.location.hash.replace('#', '');\n",
      "    if (!hash) return;\n",
      "\n",
      "    // Find a tab button with that ID (immigration, app, pricing)\n",
      "    var $btn = $('#' + hash);\n",
      "    if (!$btn.length) return;\n",
      "\n",
      "    // Make sure it’s actually inside our tabs widget\n",
      "   \n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show extracted text from legal-domain pages (if any were found).\n",
    "if legal_records:\n",
    "    for rec in legal_records[:3]:  # show up to 3\n",
    "        text = extract_text_from_html(rec[\"payload\"])\n",
    "        print(f\"URL: {rec['url']}\")\n",
    "        print(f\"Extracted text (first 300 chars):\")\n",
    "        print(f\"  {text[:300]}\")\n",
    "        print(\"-\" * 72)\n",
    "else:\n",
    "    print(\n",
    "        \"No legal-domain pages found in this sample. This is expected --\\n\"\n",
    "        \"legal content is a tiny fraction of the general web.\\n\\n\"\n",
    "        \"Try increasing max_bytes in download_warc_sample() or using a\\n\"\n",
    "        \"different WARC segment.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Did We Learn?\n",
    "\n",
    "Even with a generous URL-based filter, legal content is an extremely small\n",
    "fraction of Common Crawl. Estimates from researchers working with Common Crawl\n",
    "suggest that well under **0.1%** of all crawled pages contain substantive legal\n",
    "text (court opinions, statutes, regulations).\n",
    "\n",
    "This has real consequences:\n",
    "- A foundation model trained on Common Crawl has seen very little legal text\n",
    "  relative to its total training data.\n",
    "- The legal text it *has* seen may come from blogs, law firm marketing pages,\n",
    "  or news articles -- not primary sources like judicial opinions.\n",
    "- For a product like CoCounsel, supplementing with curated legal datasets\n",
    "  is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise (a): Filter by Legal Citations\n",
    "\n",
    "Write a function that scans extracted text for legal citations. Look for\n",
    "patterns matching common case reporters:\n",
    "\n",
    "- **Federal reporters:** `F.2d`, `F.3d`, `F. Supp.`, `F. Supp. 2d`, `F. Supp. 3d`\n",
    "- **Supreme Court:** `U.S.`, `S. Ct.`, `S.Ct.`, `L. Ed.`, `L. Ed. 2d`\n",
    "\n",
    "For example, a regex like:\n",
    "```python\n",
    "CITATION_PATTERN = re.compile(\n",
    "    r'\\d+\\s+(?:F\\.\\s*(?:2d|3d|Supp\\.(?:\\s*[23]d)?)|'\n",
    "    r'U\\.S\\.|S\\.\\s*Ct\\.|L\\.\\s*Ed\\.(?:\\s*2d)?)\\s+\\d+'\n",
    ")\n",
    "```\n",
    "\n",
    "Apply this filter to all response records in your WARC sample. How many pages\n",
    "contain at least one legal citation?\n",
    "\n",
    "### Exercise (b): Estimate Corpus Size\n",
    "\n",
    "Based on your sample:\n",
    "1. What is the average text size (in bytes) of a legal-domain page after\n",
    "   HTML extraction?\n",
    "2. What percentage of pages in a WARC segment are legal?\n",
    "3. A full WARC segment is ~1 GB compressed (~3 GB uncompressed) and contains\n",
    "   ~35,000 pages. Using your percentages, estimate how many segments you would\n",
    "   need to download to collect **1 GB of clean legal text**.\n",
    "\n",
    "Think about what this implies for the cost (bandwidth, storage, compute) of\n",
    "building a legal corpus from Common Crawl alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b_/7qz7y2sn1ys17n9368c_k7pc0000gn/T/ipykernel_88131/1457584858.py:14: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
      "\n",
      "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "\n",
      "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import XMLParsedAsHTMLWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
      "\n",
      "  soup = BeautifulSoup(html, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages with legal citations: 0 out of 310\n"
     ]
    }
   ],
   "source": [
    "CITATION_PATTERN = re.compile(\n",
    "    r'\\d+\\s+(?:F\\.\\s*(?:2d|3d|Supp\\.(?:\\s*[23]d)?)|'\n",
    "    r'U\\.S\\.|S\\.\\s*Ct\\.|L\\.\\s*Ed\\.(?:\\s*2d)?)\\s+\\d+'\n",
    ")\n",
    "\n",
    "citation_pages = []\n",
    "for rec in response_records:\n",
    "    text = extract_text_from_html(rec[\"payload\"])\n",
    "    matches = CITATION_PATTERN.findall(text)\n",
    "    if matches:\n",
    "        citation_pages.append({\"url\": rec[\"url\"], \"matches\": matches, \"text\": text})\n",
    "\n",
    "print(f\"Pages with legal citations: {len(citation_pages)} out of {len(response_records)}\")\n",
    "for page in citation_pages[:5]:\n",
    "    print(f\"\\nURL: {page['url']}\")\n",
    "    print(f\"Citations found: {page['matches'][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal-domain pages found: 4\n",
      "Average text size after extraction: 9,542 bytes (9.3 KB)\n",
      "\n",
      "Legal pages in our sample: 4/310 = 1.29%\n",
      "\n",
      "--- Estimates ---\n",
      "Pages per full segment:        ~35,000\n",
      "Legal pages per segment:       ~452\n",
      "Avg clean text per legal page: ~9,542 bytes\n",
      "Legal text per segment:        ~4.11 MB\n",
      "Segments needed for 1 GB:      ~249\n",
      "Raw WARC data to download:     ~249 GB (1 GB per segment)\n"
     ]
    }
   ],
   "source": [
    "# Exercise (b): Estimate corpus size\n",
    "\n",
    "# Step 1: Average text size of legal-domain pages\n",
    "if legal_records:\n",
    "    legal_texts = [extract_text_from_html(r[\"payload\"]) for r in legal_records]\n",
    "    legal_sizes = [len(t.encode(\"utf-8\")) for t in legal_texts]\n",
    "    avg_legal_size = sum(legal_sizes) / len(legal_sizes)\n",
    "    print(f\"Legal-domain pages found: {len(legal_records)}\")\n",
    "    print(f\"Average text size after extraction: {avg_legal_size:,.0f} bytes ({avg_legal_size/1024:.1f} KB)\")\n",
    "else:\n",
    "    avg_legal_size = 0\n",
    "    print(\"No legal-domain pages found — using estimates below\")\n",
    "\n",
    "# Step 2: What percentage of pages are legal?\n",
    "legal_pct = len(legal_records) / len(response_records) * 100\n",
    "print(f\"\\nLegal pages in our sample: {len(legal_records)}/{len(response_records)} = {legal_pct:.2f}%\")\n",
    "\n",
    "# Step 3: Estimate segments needed for 1 GB of clean legal text\n",
    "pages_per_segment = 35_000  # approximate for a full 1 GB WARC segment\n",
    "legal_pages_per_segment = pages_per_segment * (legal_pct / 100)\n",
    "\n",
    "# Use our measured average, or fall back to a reasonable estimate\n",
    "avg_size = avg_legal_size if avg_legal_size > 0 else 5_000  # 5 KB estimate\n",
    "legal_bytes_per_segment = legal_pages_per_segment * avg_size\n",
    "\n",
    "target = 1 * 1024 ** 3  # 1 GB\n",
    "segments_needed = target / legal_bytes_per_segment if legal_bytes_per_segment > 0 else float(\"inf\")\n",
    "\n",
    "print(f\"\\n--- Estimates ---\")\n",
    "print(f\"Pages per full segment:        ~{pages_per_segment:,}\")\n",
    "print(f\"Legal pages per segment:       ~{legal_pages_per_segment:,.0f}\")\n",
    "print(f\"Avg clean text per legal page: ~{avg_size:,.0f} bytes\")\n",
    "print(f\"Legal text per segment:        ~{legal_bytes_per_segment/1024/1024:.2f} MB\")\n",
    "print(f\"Segments needed for 1 GB:      ~{segments_needed:,.0f}\")\n",
    "print(f\"Raw WARC data to download:     ~{segments_needed:,.0f} GB (1 GB per segment)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
