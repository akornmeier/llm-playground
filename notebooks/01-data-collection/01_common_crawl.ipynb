{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Exploring Common Crawl WARC Files\n",
    "\n",
    "## Why Data Collection Matters for Legal AI\n",
    "\n",
    "Every large language model starts with data. GPT, LLaMA, and other foundation\n",
    "models are trained on massive text corpora, and **Common Crawl** is one of the\n",
    "largest publicly available sources. It contains petabytes of web pages collected\n",
    "over more than a decade.\n",
    "\n",
    "For legal AI systems like CoCounsel, the composition of training data directly\n",
    "affects the model's ability to reason about statutes, case law, and legal\n",
    "concepts. Understanding *where* the data comes from -- and how little of a\n",
    "general web crawl is actually legal content -- is the first step toward building\n",
    "better legal AI.\n",
    "\n",
    "### What is the WARC Format?\n",
    "\n",
    "Common Crawl stores its data in **WARC (Web ARChive)** format, an ISO standard\n",
    "(ISO 28500:2017) for archiving web content. A single WARC file bundles together:\n",
    "\n",
    "- **`warcinfo`** records -- metadata about the crawl itself\n",
    "- **`request`** records -- the HTTP request sent to the server\n",
    "- **`response`** records -- the full HTTP response including headers and HTML body\n",
    "- **`metadata`** records -- additional information about the crawl\n",
    "\n",
    "A typical WARC segment from Common Crawl is about 1 GB compressed and contains\n",
    "roughly 30,000-40,000 web pages. The full monthly crawl consists of tens of\n",
    "thousands of such segments.\n",
    "\n",
    "> **Note:** This notebook requires network access to download WARC data from\n",
    "> Common Crawl. Cells that need network access are marked with a\n",
    "> `[REQUIRES NETWORK]` label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install and import the libraries we need:\n",
    "- **warcio** -- reading and writing WARC files\n",
    "- **beautifulsoup4** -- parsing HTML content\n",
    "- **requests** -- downloading files from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# %pip install warcio beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from warcio.archiveiterator import ArchiveIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# Helper: download a WARC segment (streaming, with a byte limit so we don't\n",
    "# pull down the entire 1 GB file during exploration).\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "SAMPLE_WARC_URL = (\n",
    "    \"https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-10/\"\n",
    "    \"segments/1707947474641.34/warc/\"\n",
    "    \"CC-MAIN-20240222070947-20240222100947-00000.warc.gz\"\n",
    ")\n",
    "\n",
    "\n",
    "def download_warc_sample(\n",
    "    url: str = SAMPLE_WARC_URL,\n",
    "    max_bytes: int = 5 * 1024 * 1024,  # 5 MB default -- enough for ~100-200 pages\n",
    ") -> io.BytesIO:\n",
    "    \"\"\"Stream the first `max_bytes` of a WARC file into an in-memory buffer.\"\"\"\n",
    "    print(f\"Downloading first {max_bytes / 1024 / 1024:.1f} MB from:\")\n",
    "    print(f\"  {url}\")\n",
    "    headers = {\"Range\": f\"bytes=0-{max_bytes - 1}\"}\n",
    "    resp = requests.get(url, headers=headers, stream=True, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    buf = io.BytesIO(resp.content)\n",
    "    print(f\"Downloaded {len(resp.content):,} bytes.\")\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring WARC Files\n",
    "\n",
    "Let's download a small slice of a real Common Crawl WARC segment and see what's\n",
    "inside. We limit the download to 5 MB so it finishes quickly.\n",
    "\n",
    "> **[REQUIRES NETWORK]** -- The cell below downloads data from\n",
    "> `data.commoncrawl.org`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [REQUIRES NETWORK] -- Download a 5 MB sample of a WARC segment.\n",
    "warc_buffer = download_warc_sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse every record in our sample and collect basic stats.\n",
    "records = []\n",
    "\n",
    "warc_buffer.seek(0)\n",
    "for record in ArchiveIterator(warc_buffer):\n",
    "    rec_type = record.rec_type\n",
    "    url = record.rec_headers.get_header(\"WARC-Target-URI\") or \"\"\n",
    "    content_length = int(\n",
    "        record.rec_headers.get_header(\"Content-Length\") or 0\n",
    "    )\n",
    "    # Read the payload so the iterator advances properly\n",
    "    payload = record.content_stream().read()\n",
    "    records.append(\n",
    "        {\n",
    "            \"type\": rec_type,\n",
    "            \"url\": url,\n",
    "            \"content_length\": content_length,\n",
    "            \"payload\": payload,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"Total records parsed: {len(records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many records of each type?\n",
    "type_counts = Counter(r[\"type\"] for r in records)\n",
    "print(\"Record types:\")\n",
    "for rtype, count in type_counts.most_common():\n",
    "    print(f\"  {rtype:12s}  {count:>5,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract HTML content from 'response' records and show a sample.\n",
    "response_records = [r for r in records if r[\"type\"] == \"response\"]\n",
    "print(f\"Response records: {len(response_records)}\")\n",
    "\n",
    "if response_records:\n",
    "    sample = response_records[0]\n",
    "    print(f\"\\nSample URL: {sample['url']}\")\n",
    "    print(f\"Payload size: {len(sample['payload']):,} bytes\")\n",
    "    # Show the first 500 characters of the raw payload\n",
    "    print(\"\\n--- Raw payload (first 500 chars) ---\")\n",
    "    print(sample[\"payload\"][:500].decode(\"utf-8\", errors=\"replace\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Even from this small sample, you can see that:\n",
    "1. Each web page generates multiple WARC records (request + response at minimum).\n",
    "2. The `response` payload includes full HTTP headers followed by the HTML body.\n",
    "3. The raw content is noisy -- navigation menus, ads, JavaScript, and boilerplate\n",
    "   surround the actual page text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering for Legal Content\n",
    "\n",
    "Common Crawl is a *general* web crawl. Let's find out what fraction of the\n",
    "pages we downloaded come from legal domains.\n",
    "\n",
    "We'll define a simple heuristic: a URL is \"legal-related\" if it contains any of\n",
    "the substrings `.gov`, `court`, `law`, or `legal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGAL_KEYWORDS = [\"gov\", \"court\", \"law\", \"legal\"]\n",
    "\n",
    "\n",
    "def is_legal_url(url: str) -> bool:\n",
    "    \"\"\"Return True if the URL likely points to a legal-domain page.\"\"\"\n",
    "    url_lower = url.lower()\n",
    "    return any(kw in url_lower for kw in LEGAL_KEYWORDS)\n",
    "\n",
    "\n",
    "legal_records = [r for r in response_records if is_legal_url(r[\"url\"])]\n",
    "total = len(response_records)\n",
    "legal_count = len(legal_records)\n",
    "\n",
    "print(f\"Total response records : {total}\")\n",
    "print(f\"Legal-domain records   : {legal_count}\")\n",
    "if total > 0:\n",
    "    print(f\"Percentage             : {legal_count / total * 100:.2f}%\")\n",
    "else:\n",
    "    print(\"No response records found (is the WARC download empty?)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_html(raw_payload: bytes) -> str:\n",
    "    \"\"\"Extract visible text from an HTTP response payload.\n",
    "\n",
    "    The payload starts with HTTP headers, followed by a blank line, then the\n",
    "    HTML body. We split on the first double-newline to skip the headers.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = raw_payload.decode(\"utf-8\", errors=\"replace\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "    # Skip HTTP headers\n",
    "    parts = text.split(\"\\r\\n\\r\\n\", 1)\n",
    "    html = parts[1] if len(parts) > 1 else text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # Remove script and style elements\n",
    "    for tag in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\"]):\n",
    "        tag.decompose()\n",
    "    return soup.get_text(separator=\" \", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show extracted text from legal-domain pages (if any were found).\n",
    "if legal_records:\n",
    "    for rec in legal_records[:3]:  # show up to 3\n",
    "        text = extract_text_from_html(rec[\"payload\"])\n",
    "        print(f\"URL: {rec['url']}\")\n",
    "        print(f\"Extracted text (first 300 chars):\")\n",
    "        print(f\"  {text[:300]}\")\n",
    "        print(\"-\" * 72)\n",
    "else:\n",
    "    print(\n",
    "        \"No legal-domain pages found in this sample. This is expected --\\n\"\n",
    "        \"legal content is a tiny fraction of the general web.\\n\\n\"\n",
    "        \"Try increasing max_bytes in download_warc_sample() or using a\\n\"\n",
    "        \"different WARC segment.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Did We Learn?\n",
    "\n",
    "Even with a generous URL-based filter, legal content is an extremely small\n",
    "fraction of Common Crawl. Estimates from researchers working with Common Crawl\n",
    "suggest that well under **0.1%** of all crawled pages contain substantive legal\n",
    "text (court opinions, statutes, regulations).\n",
    "\n",
    "This has real consequences:\n",
    "- A foundation model trained on Common Crawl has seen very little legal text\n",
    "  relative to its total training data.\n",
    "- The legal text it *has* seen may come from blogs, law firm marketing pages,\n",
    "  or news articles -- not primary sources like judicial opinions.\n",
    "- For a product like CoCounsel, supplementing with curated legal datasets\n",
    "  is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise (a): Filter by Legal Citations\n",
    "\n",
    "Write a function that scans extracted text for legal citations. Look for\n",
    "patterns matching common case reporters:\n",
    "\n",
    "- **Federal reporters:** `F.2d`, `F.3d`, `F. Supp.`, `F. Supp. 2d`, `F. Supp. 3d`\n",
    "- **Supreme Court:** `U.S.`, `S. Ct.`, `S.Ct.`, `L. Ed.`, `L. Ed. 2d`\n",
    "\n",
    "For example, a regex like:\n",
    "```python\n",
    "CITATION_PATTERN = re.compile(\n",
    "    r'\\d+\\s+(?:F\\.\\s*(?:2d|3d|Supp\\.(?:\\s*[23]d)?)|'\n",
    "    r'U\\.S\\.|S\\.\\s*Ct\\.|L\\.\\s*Ed\\.(?:\\s*2d)?)\\s+\\d+'\n",
    ")\n",
    "```\n",
    "\n",
    "Apply this filter to all response records in your WARC sample. How many pages\n",
    "contain at least one legal citation?\n",
    "\n",
    "### Exercise (b): Estimate Corpus Size\n",
    "\n",
    "Based on your sample:\n",
    "1. What is the average text size (in bytes) of a legal-domain page after\n",
    "   HTML extraction?\n",
    "2. What percentage of pages in a WARC segment are legal?\n",
    "3. A full WARC segment is ~1 GB compressed (~3 GB uncompressed) and contains\n",
    "   ~35,000 pages. Using your percentages, estimate how many segments you would\n",
    "   need to download to collect **1 GB of clean legal text**.\n",
    "\n",
    "Think about what this implies for the cost (bandwidth, storage, compute) of\n",
    "building a legal corpus from Common Crawl alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
