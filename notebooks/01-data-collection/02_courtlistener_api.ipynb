{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - CourtListener: Curated Legal Data\n",
    "\n",
    "## Purpose-Built Legal Data vs. General Web Crawls\n",
    "\n",
    "In the previous notebook we saw that legal content is a vanishingly small\n",
    "fraction of Common Crawl's general web archive. Building a legal AI system on\n",
    "raw web data alone would be like panning for gold in the ocean.\n",
    "\n",
    "**CourtListener** (https://www.courtlistener.com) takes the opposite approach.\n",
    "Operated by Free Law Project, it is an open platform that collects, curates,\n",
    "and serves U.S. court opinions, oral arguments, and judicial records. The data\n",
    "is:\n",
    "\n",
    "- **Structured** -- each opinion comes with metadata (court, date, case name,\n",
    "  citations)\n",
    "- **Clean** -- text is extracted and normalized from court filing systems\n",
    "- **Authoritative** -- sourced directly from PACER, court websites, and\n",
    "  official repositories\n",
    "\n",
    "### CourtListener REST API\n",
    "\n",
    "CourtListener provides a free REST API at `https://www.courtlistener.com/api/rest/v4/`\n",
    "that lets you search and retrieve opinions programmatically. Key endpoints:\n",
    "\n",
    "| Endpoint | Description |\n",
    "|----------|-------------|\n",
    "| `/opinions/` | Full text of court opinions |\n",
    "| `/clusters/` | Groups of opinions for a single case |\n",
    "| `/courts/` | Court metadata |\n",
    "| `/search/` | Full-text search across opinions |\n",
    "\n",
    "In this notebook we will primarily work with **sample data bundled in this\n",
    "repository** so everything runs offline. We also show how you would query the\n",
    "live API for reference.\n",
    "\n",
    "> **Network access:** Only the API example cells require network access. All\n",
    "> analysis cells use the local sample data and run fully offline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Court Opinions\n",
    "\n",
    "### Option A: Local Sample Data (offline)\n",
    "\n",
    "This repository includes a small sample of court opinions in JSONL format.\n",
    "Each line is a JSON object with fields: `id`, `case_name`, `court`,\n",
    "`date_filed`, `text`, and `citations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_PATH = Path(\"../../datasets/sample/court_opinions.jsonl\")\n",
    "\n",
    "\n",
    "def load_opinions(path: Path = SAMPLE_PATH) -> list[dict]:\n",
    "    \"\"\"Load court opinions from a JSONL file.\"\"\"\n",
    "    opinions = []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                opinions.append(json.loads(line))\n",
    "    return opinions\n",
    "\n",
    "\n",
    "opinions = load_opinions()\n",
    "print(f\"Loaded {len(opinions)} opinions from {SAMPLE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first opinion as a formatted example.\n",
    "sample = opinions[0]\n",
    "print(f\"Case Name  : {sample['case_name']}\")\n",
    "print(f\"Court      : {sample['court']}\")\n",
    "print(f\"Date Filed : {sample['date_filed']}\")\n",
    "print(f\"Citations  : {sample['citations']}\")\n",
    "print(f\"\\nOpinion text (first 500 chars):\")\n",
    "print(f\"  {sample['text'][:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all opinions in a compact summary table.\n",
    "print(f\"{'ID':>6}  {'Date':10}  {'Court (short)':30}  {'Case Name (truncated)'}\")\n",
    "print(\"-\" * 90)\n",
    "for op in opinions:\n",
    "    court_short = op[\"court\"][:30]\n",
    "    name_short = op[\"case_name\"][:40]\n",
    "    print(f\"{op['id']:>6}  {op['date_filed']:10}  {court_short:30}  {name_short}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: CourtListener REST API (requires network)\n",
    "\n",
    "The cell below demonstrates how to query the live CourtListener API.\n",
    "You can run it when you have network access, or simply read through the\n",
    "code to understand the pattern.\n",
    "\n",
    "> **[REQUIRES NETWORK]** -- The next cell makes HTTP requests to\n",
    "> `courtlistener.com`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [REQUIRES NETWORK] -- Uncomment and run to query the live API.\n",
    "#\n",
    "# import requests\n",
    "#\n",
    "# API_BASE = \"https://www.courtlistener.com/api/rest/v4\"\n",
    "#\n",
    "# def fetch_opinions(\n",
    "#     court: str = \"scotus\",\n",
    "#     date_after: str = \"2023-01-01\",\n",
    "#     page_size: int = 5,\n",
    "# ) -> list[dict]:\n",
    "#     \"\"\"Fetch opinions from the CourtListener API.\"\"\"\n",
    "#     params = {\n",
    "#         \"court\": court,\n",
    "#         \"date_created__gte\": date_after,\n",
    "#         \"page_size\": page_size,\n",
    "#         \"format\": \"json\",\n",
    "#     }\n",
    "#     resp = requests.get(f\"{API_BASE}/opinions/\", params=params, timeout=15)\n",
    "#     resp.raise_for_status()\n",
    "#     data = resp.json()\n",
    "#     print(f\"Total results available: {data['count']}\")\n",
    "#     return data[\"results\"]\n",
    "#\n",
    "# api_opinions = fetch_opinions()\n",
    "# for op in api_opinions:\n",
    "#     print(f\"  [{op['id']}] {op.get('download_url', 'N/A')[:80]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Data Quality\n",
    "\n",
    "How does curated legal data compare to what you might extract from Common\n",
    "Crawl? Let's look at the differences side by side.\n",
    "\n",
    "### Common Crawl Extract (simulated)\n",
    "\n",
    "Below we simulate a typical raw web extract of a legal page. This represents\n",
    "what you would get after downloading a WARC response and running it through\n",
    "BeautifulSoup -- navigation chrome, ads, and formatting artifacts mixed in\n",
    "with the substantive text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated Common Crawl extract -- this is what raw web scraping looks like\n",
    "# after HTML-to-text conversion.\n",
    "SIMULATED_WEB_EXTRACT = \"\"\"Home | About | Contact | Login\n",
    "Search our database of over 10 million cases...\n",
    "ADVERTISEMENT - Legal Services - Click Here\n",
    "================================================\n",
    "Henderson v. Meridian Health Systems, Inc.\n",
    "No. 24-1847\n",
    "United States Court of Appeals for the Seventh Circuit\n",
    "Filed: March 15, 2024\n",
    "\n",
    "Before the Court is the appeal of plaintiff James Henderson from the district\n",
    "court's grant of summary judgment in favor of defendant Meridian Health Systems.\n",
    "Henderson alleges that Meridian violated the Americans with Disabilities Act...\n",
    "[content continues]\n",
    "================================================\n",
    "Related Cases | Share | Print | Download PDF\n",
    "Footer: Copyright 2024 LegalCasesOnline.com | Privacy Policy | Terms of Use\n",
    "Subscribe to our newsletter for weekly case updates!\"\"\"\n",
    "\n",
    "print(\"=\" * 72)\n",
    "print(\"COMMON CRAWL EXTRACT (simulated)\")\n",
    "print(\"=\" * 72)\n",
    "print(SIMULATED_WEB_EXTRACT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CourtListener data -- clean, structured, authoritative.\n",
    "cl_opinion = opinions[0]  # Henderson v. Meridian -- same case\n",
    "\n",
    "print(\"=\" * 72)\n",
    "print(\"COURTLISTENER DATA\")\n",
    "print(\"=\" * 72)\n",
    "print(f\"Case Name  : {cl_opinion['case_name']}\")\n",
    "print(f\"Court      : {cl_opinion['court']}\")\n",
    "print(f\"Date Filed : {cl_opinion['date_filed']}\")\n",
    "print(f\"Citations  : {cl_opinion['citations']}\")\n",
    "print(f\"\\nOpinion text (first 400 chars):\")\n",
    "print(cl_opinion[\"text\"][:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side-by-Side Comparison\n",
    "\n",
    "| Dimension | Common Crawl | CourtListener |\n",
    "|-----------|-------------|---------------|\n",
    "| **Metadata** | Must be inferred from HTML/URL | Structured fields (court, date, case name) |\n",
    "| **Text quality** | Noisy -- nav bars, ads, footers mixed in | Clean opinion text only |\n",
    "| **Citations** | Embedded in raw text, must be parsed | Provided as a structured list |\n",
    "| **Provenance** | Unknown third-party website | Official court records |\n",
    "| **Coverage** | Unpredictable; depends on what was crawled | Systematic collection from court systems |\n",
    "| **Freshness** | Crawl lag (weeks to months) | Near real-time updates |\n",
    "\n",
    "For legal AI, the CourtListener approach is clearly superior for building a\n",
    "training corpus. But Common Crawl still has value: it captures secondary\n",
    "sources (law review articles, legal blogs, news coverage) that can provide\n",
    "broader context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Statistics\n",
    "\n",
    "Let's compute basic statistics over our sample corpus to understand its\n",
    "characteristics. These metrics are useful baselines when evaluating any\n",
    "text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Document count ---\n",
    "doc_count = len(opinions)\n",
    "print(f\"Document count: {doc_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Average length (characters and words) ---\n",
    "char_lengths = [len(op[\"text\"]) for op in opinions]\n",
    "word_lengths = [len(op[\"text\"].split()) for op in opinions]\n",
    "\n",
    "avg_chars = sum(char_lengths) / doc_count\n",
    "avg_words = sum(word_lengths) / doc_count\n",
    "\n",
    "print(f\"Average length (characters): {avg_chars:,.0f}\")\n",
    "print(f\"Average length (words)     : {avg_words:,.0f}\")\n",
    "print(f\"Min words: {min(word_lengths):,}  |  Max words: {max(word_lengths):,}\")\n",
    "print()\n",
    "print(\"Per-document breakdown:\")\n",
    "for op, wc, cc in zip(opinions, word_lengths, char_lengths):\n",
    "    print(f\"  [{op['id']}] {op['case_name'][:45]:45s}  {wc:>5} words  {cc:>6} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Vocabulary size (unique words) ---\n",
    "# We do minimal normalization: lowercase and strip punctuation.\n",
    "all_words = []\n",
    "for op in opinions:\n",
    "    tokens = re.findall(r\"[a-zA-Z]+(?:'[a-zA-Z]+)?\", op[\"text\"].lower())\n",
    "    all_words.extend(tokens)\n",
    "\n",
    "vocab = set(all_words)\n",
    "total_tokens = len(all_words)\n",
    "\n",
    "print(f\"Total tokens (words)  : {total_tokens:,}\")\n",
    "print(f\"Vocabulary size       : {len(vocab):,} unique words\")\n",
    "print(f\"Type-token ratio      : {len(vocab) / total_tokens:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Most common legal terms ---\n",
    "# We filter out common English stop words to surface domain-specific vocabulary.\n",
    "STOP_WORDS = {\n",
    "    \"the\", \"of\", \"and\", \"to\", \"a\", \"in\", \"that\", \"is\", \"for\", \"it\", \"was\",\n",
    "    \"on\", \"with\", \"as\", \"at\", \"by\", \"an\", \"be\", \"this\", \"from\", \"or\", \"are\",\n",
    "    \"not\", \"but\", \"its\", \"his\", \"her\", \"he\", \"she\", \"has\", \"have\", \"had\",\n",
    "    \"been\", \"were\", \"which\", \"their\", \"we\", \"they\", \"will\", \"would\", \"can\",\n",
    "    \"could\", \"may\", \"should\", \"shall\", \"do\", \"does\", \"did\", \"no\", \"if\",\n",
    "    \"all\", \"more\", \"than\", \"other\", \"into\", \"also\", \"any\", \"such\", \"when\",\n",
    "    \"who\", \"what\", \"there\", \"each\", \"about\", \"up\", \"out\", \"so\", \"said\",\n",
    "    \"under\", \"after\", \"before\", \"between\", \"over\", \"because\", \"our\", \"you\",\n",
    "}\n",
    "\n",
    "legal_word_counts = Counter(\n",
    "    w for w in all_words if w not in STOP_WORDS and len(w) > 2\n",
    ")\n",
    "\n",
    "print(\"Most common terms (excluding stop words):\")\n",
    "print()\n",
    "for word, count in legal_word_counts.most_common(25):\n",
    "    bar = \"|\" * min(count, 50)\n",
    "    print(f\"  {word:20s}  {count:>4}  {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Citation analysis ---\n",
    "all_citations = []\n",
    "for op in opinions:\n",
    "    all_citations.extend(op.get(\"citations\", []))\n",
    "\n",
    "print(f\"Total citations across corpus: {len(all_citations)}\")\n",
    "print(f\"Unique citations             : {len(set(all_citations))}\")\n",
    "print()\n",
    "print(\"All citations:\")\n",
    "for cite in sorted(set(all_citations)):\n",
    "    freq = all_citations.count(cite)\n",
    "    print(f\"  [{freq}x] {cite}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics\n",
    "\n",
    "Even with just 5 sample opinions, we can observe several characteristics\n",
    "of legal text:\n",
    "\n",
    "1. **Long documents** -- Court opinions are substantially longer than typical\n",
    "   web pages, often running to several thousand words.\n",
    "2. **Specialized vocabulary** -- Terms like \"court\", \"defendant\", \"plaintiff\",\n",
    "   \"motion\", and \"commission\" appear frequently.\n",
    "3. **Dense citation networks** -- Opinions reference other opinions, creating\n",
    "   a graph structure that is valuable for understanding legal reasoning.\n",
    "4. **Formal register** -- The language is precise and formulaic, quite\n",
    "   different from informal web text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise (a): Fetch Opinions from a Specific Court and Date Range\n",
    "\n",
    "Using the CourtListener API (requires network access), write code to:\n",
    "1. Fetch the 10 most recent opinions from the Second Circuit (`ca2`).\n",
    "2. Filter for opinions filed after `2024-01-01`.\n",
    "3. Display the case name, date, and first 200 characters of each opinion.\n",
    "\n",
    "Starter code:\n",
    "```python\n",
    "import requests\n",
    "\n",
    "API_BASE = \"https://www.courtlistener.com/api/rest/v4\"\n",
    "\n",
    "params = {\n",
    "    \"court\": \"ca2\",\n",
    "    \"date_created__gte\": \"2024-01-01\",\n",
    "    \"page_size\": 10,\n",
    "    \"ordering\": \"-date_created\",\n",
    "    \"format\": \"json\",\n",
    "}\n",
    "\n",
    "resp = requests.get(f\"{API_BASE}/clusters/\", params=params, timeout=15)\n",
    "resp.raise_for_status()\n",
    "results = resp.json()[\"results\"]\n",
    "\n",
    "for r in results:\n",
    "    print(r[\"case_name\"], r[\"date_filed\"])\n",
    "```\n",
    "\n",
    "### Exercise (b): Compute Corpus Statistics on a Larger Dataset\n",
    "\n",
    "Using either the local sample data or opinions fetched from the API, compute:\n",
    "\n",
    "1. **Document count** -- How many opinions are in the corpus?\n",
    "2. **Average length** -- Mean number of characters and words per opinion.\n",
    "3. **Vocabulary size** -- Number of unique words after lowercasing.\n",
    "4. **Top 20 legal terms** -- Most frequent words after removing stop words.\n",
    "\n",
    "Compare your results to the statistics we computed above. If you used API data,\n",
    "how do the numbers differ from our 5-document sample? What does this tell you\n",
    "about the representativeness of small samples?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
