{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 02 - Deduplication for Training Data\n",
    "\n",
    "## Why Deduplication Matters\n",
    "\n",
    "Duplicate and near-duplicate documents in training data cause real problems:\n",
    "\n",
    "- **Memorization** -- The model memorizes repeated passages verbatim and\n",
    "  regurgitates them at inference time, which is especially dangerous for\n",
    "  legal text that may contain PII or privileged information.\n",
    "- **Benchmark contamination** -- If evaluation data appears in the training\n",
    "  set (even slightly modified), benchmark scores are inflated and unreliable.\n",
    "- **Wasted compute** -- Training on duplicates burns GPU hours without\n",
    "  teaching the model anything new.\n",
    "\n",
    "### Exact vs. Fuzzy Deduplication\n",
    "\n",
    "There are two levels of deduplication:\n",
    "\n",
    "| Approach | Method | Catches |\n",
    "|----------|--------|---------|\n",
    "| **Exact dedup** | Hash each document; remove identical hashes | Byte-for-byte duplicates |\n",
    "| **Fuzzy (near) dedup** | MinHash + LSH | Documents that differ by minor edits, formatting changes, or OCR variations |\n",
    "\n",
    "In legal corpora, near-duplicates are common: the same opinion may appear\n",
    "in multiple databases with slightly different formatting, headers, or\n",
    "pagination. A court may also issue amended opinions that differ by only a\n",
    "few sentences from the original.\n",
    "\n",
    "### MinHash + LSH in Brief\n",
    "\n",
    "**MinHash** (Min-wise Independent Permutations) is a technique for quickly\n",
    "estimating the Jaccard similarity between two sets:\n",
    "\n",
    "1. Represent each document as a set of **shingles** (contiguous word n-grams).\n",
    "2. Apply multiple hash functions to the shingle set and keep the **minimum**\n",
    "   hash value for each function. This produces a compact *signature*.\n",
    "3. The probability that two documents share the same minimum hash equals\n",
    "   their Jaccard similarity.\n",
    "\n",
    "**LSH** (Locality-Sensitive Hashing) makes the search efficient by hashing\n",
    "signatures into *bands*. Documents that share at least one band are candidate\n",
    "pairs, which are then verified with a full similarity check. This reduces\n",
    "pairwise comparison from O(n^2) to near-linear time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install and import the libraries we need:\n",
    "- **datasketch** -- MinHash and LSH implementation\n",
    "- **hashlib** -- hash-based exact deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# %pip install datasketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "from datasketch import MinHash, MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../../datasets/sample/court_opinions.jsonl\")\n",
    "\n",
    "\n",
    "def load_opinions(path: Path = DATA_PATH) -> list[dict[str, Any]]:\n",
    "    \"\"\"Load court opinions from a JSONL file.\"\"\"\n",
    "    opinions = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                opinions.append(json.loads(line))\n",
    "    return opinions\n",
    "\n",
    "\n",
    "opinions = load_opinions()\n",
    "print(f\"Loaded {len(opinions)} opinions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Exact Deduplication\n",
    "\n",
    "The simplest form of deduplication: compute a cryptographic hash of each\n",
    "document's text and remove duplicates. We use SHA-256, which has a\n",
    "negligible collision probability.\n",
    "\n",
    "This catches byte-for-byte identical copies but misses documents that\n",
    "differ even by a single character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hash(text: str) -> str:\n",
    "    \"\"\"Compute a SHA-256 hash of the text.\"\"\"\n",
    "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "def exact_dedup(\n",
    "    docs: list[dict[str, Any]],\n",
    ") -> tuple[list[dict[str, Any]], list[dict[str, Any]]]:\n",
    "    \"\"\"Remove exact duplicates based on text hash.\n",
    "\n",
    "    Returns (unique_docs, duplicate_docs).\n",
    "    \"\"\"\n",
    "    seen_hashes: set[str] = set()\n",
    "    unique = []\n",
    "    duplicates = []\n",
    "\n",
    "    for doc in docs:\n",
    "        h = compute_hash(doc[\"text\"])\n",
    "        if h in seen_hashes:\n",
    "            duplicates.append(doc)\n",
    "        else:\n",
    "            seen_hashes.add(h)\n",
    "            unique.append(doc)\n",
    "\n",
    "    return unique, duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run exact dedup on the original data\n",
    "unique, duplicates = exact_dedup(opinions)\n",
    "print(f\"Original documents:  {len(opinions)}\")\n",
    "print(f\"Unique documents:    {len(unique)}\")\n",
    "print(f\"Exact duplicates:    {len(duplicates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate exact dedup by adding a known duplicate\n",
    "opinions_with_dup = opinions + [opinions[0].copy()]  # duplicate first opinion\n",
    "print(f\"Documents with added duplicate: {len(opinions_with_dup)}\")\n",
    "\n",
    "unique2, dups2 = exact_dedup(opinions_with_dup)\n",
    "print(f\"After exact dedup:             {len(unique2)}\")\n",
    "print(f\"Duplicates found:              {len(dups2)}\")\n",
    "if dups2:\n",
    "    print(f\"Duplicate case: {dups2[0]['case_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Near-Duplicate Detection with MinHash\n",
    "\n",
    "Exact dedup misses near-duplicates -- documents that are almost identical\n",
    "but differ in minor ways (formatting, typos, amended text). MinHash + LSH\n",
    "solves this efficiently.\n",
    "\n",
    "### Step-by-Step\n",
    "\n",
    "1. **Shingling** -- Convert each document into a set of overlapping word\n",
    "   n-grams (\"shingles\"). For example, with 3-word shingles:\n",
    "   \n",
    "   ```\n",
    "   \"the court finds\" -> {\"the court finds\"}\n",
    "   \"court finds that\" -> {\"court finds that\"}\n",
    "   ```\n",
    "\n",
    "2. **MinHash signature** -- For each shingle set, compute a fixed-size\n",
    "   signature using `num_perm` random hash permutations. Each element of the\n",
    "   signature is the minimum hash value across all shingles for that\n",
    "   permutation.\n",
    "\n",
    "3. **LSH indexing** -- Insert signatures into an LSH index that groups\n",
    "   similar documents into the same buckets.\n",
    "\n",
    "4. **Query** -- For each document, query the LSH index for candidates.\n",
    "   Candidates sharing a bucket are likely near-duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shingles(text: str, k: int = 5) -> set[str]:\n",
    "    \"\"\"Create a set of word k-shingles from text.\n",
    "\n",
    "    Args:\n",
    "        text: Input text.\n",
    "        k: Number of words per shingle.\n",
    "\n",
    "    Returns:\n",
    "        Set of shingle strings.\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    if len(words) < k:\n",
    "        return {\" \".join(words)} if words else set()\n",
    "    return {\" \".join(words[i : i + k]) for i in range(len(words) - k + 1)}\n",
    "\n",
    "\n",
    "# Demonstrate shingling\n",
    "sample_text = \"The court finds that the defendant is liable.\"\n",
    "shingles = create_shingles(sample_text, k=3)\n",
    "print(f\"Text: {sample_text!r}\")\n",
    "print(f\"3-word shingles ({len(shingles)}):\")\n",
    "for s in sorted(shingles):\n",
    "    print(f\"  {s!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_minhash(shingles: set[str], num_perm: int = 128) -> MinHash:\n",
    "    \"\"\"Build a MinHash signature from a set of shingles.\"\"\"\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for s in shingles:\n",
    "        m.update(s.encode(\"utf-8\"))\n",
    "    return m\n",
    "\n",
    "\n",
    "def compute_minhash_signatures(\n",
    "    docs: list[dict[str, Any]],\n",
    "    shingle_k: int = 5,\n",
    "    num_perm: int = 128,\n",
    ") -> list[tuple[dict[str, Any], MinHash]]:\n",
    "    \"\"\"Compute MinHash signatures for all documents.\"\"\"\n",
    "    results = []\n",
    "    for doc in docs:\n",
    "        shingles = create_shingles(doc[\"text\"], k=shingle_k)\n",
    "        mh = build_minhash(shingles, num_perm=num_perm)\n",
    "        results.append((doc, mh))\n",
    "    return results\n",
    "\n",
    "\n",
    "# Compute signatures for our corpus\n",
    "signatures = compute_minhash_signatures(opinions)\n",
    "print(f\"Computed MinHash signatures for {len(signatures)} documents.\")\n",
    "print(f\"Signature size: {signatures[0][1].num_perm} permutations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show pairwise Jaccard similarity estimates between all documents\n",
    "print(\"Estimated Jaccard similarities (from MinHash):\")\n",
    "print()\n",
    "\n",
    "# Header\n",
    "short_names = [doc[\"case_name\"].split(\" v.\")[0].split(\"v.\")[0].strip()[:15]\n",
    "               for doc, _ in signatures]\n",
    "print(f\"{'':>16s}\", end=\"\")\n",
    "for name in short_names:\n",
    "    print(f\"{name:>16s}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i, (doc_i, mh_i) in enumerate(signatures):\n",
    "    print(f\"{short_names[i]:>16s}\", end=\"\")\n",
    "    for j, (doc_j, mh_j) in enumerate(signatures):\n",
    "        sim = mh_i.jaccard(mh_j)\n",
    "        print(f\"{sim:>16.3f}\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_near_duplicates(\n",
    "    docs: list[dict[str, Any]],\n",
    "    threshold: float = 0.5,\n",
    "    num_perm: int = 128,\n",
    "    shingle_k: int = 5,\n",
    ") -> list[tuple[int, int, float]]:\n",
    "    \"\"\"Find near-duplicate pairs using MinHash + LSH.\n",
    "\n",
    "    Args:\n",
    "        docs: List of document dictionaries with 'text' field.\n",
    "        threshold: Jaccard similarity threshold for near-duplicates.\n",
    "        num_perm: Number of hash permutations for MinHash.\n",
    "        shingle_k: Number of words per shingle.\n",
    "\n",
    "    Returns:\n",
    "        List of (idx_a, idx_b, similarity) tuples.\n",
    "    \"\"\"\n",
    "    # Build MinHash signatures\n",
    "    sigs = compute_minhash_signatures(docs, shingle_k=shingle_k, num_perm=num_perm)\n",
    "\n",
    "    # Create LSH index\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "    for i, (doc, mh) in enumerate(sigs):\n",
    "        lsh.insert(str(i), mh)\n",
    "\n",
    "    # Query for each document\n",
    "    duplicate_pairs = []\n",
    "    seen_pairs: set[tuple[int, int]] = set()\n",
    "\n",
    "    for i, (doc, mh) in enumerate(sigs):\n",
    "        candidates = lsh.query(mh)\n",
    "        for cand in candidates:\n",
    "            j = int(cand)\n",
    "            if i >= j:\n",
    "                continue  # skip self and already-seen pairs\n",
    "            pair = (min(i, j), max(i, j))\n",
    "            if pair in seen_pairs:\n",
    "                continue\n",
    "            seen_pairs.add(pair)\n",
    "            sim = sigs[i][1].jaccard(sigs[j][1])\n",
    "            duplicate_pairs.append((i, j, sim))\n",
    "\n",
    "    return sorted(duplicate_pairs, key=lambda x: -x[2])\n",
    "\n",
    "\n",
    "# Run near-dedup on the original corpus\n",
    "near_dups = find_near_duplicates(opinions, threshold=0.5)\n",
    "print(f\"Near-duplicate pairs found (threshold=0.5): {len(near_dups)}\")\n",
    "for i, j, sim in near_dups:\n",
    "    print(f\"  [{i}] {opinions[i]['case_name'][:40]}\")\n",
    "    print(f\"  [{j}] {opinions[j]['case_name'][:40]}\")\n",
    "    print(f\"  Similarity: {sim:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Demonstration: Catching Synthetic Near-Duplicates\n",
    "\n",
    "To show that MinHash works for fuzzy matching, we create synthetic\n",
    "near-duplicates by taking an existing opinion and making minor edits:\n",
    "replacing words, changing formatting, and adding/removing sentences.\n",
    "\n",
    "This simulates what happens in practice when the same opinion appears\n",
    "in multiple databases with different OCR outputs or editorial formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def create_near_duplicate(text: str, edit_fraction: float = 0.05) -> str:\n",
    "    \"\"\"Create a near-duplicate by randomly replacing words.\n",
    "\n",
    "    Args:\n",
    "        text: Original text.\n",
    "        edit_fraction: Fraction of words to replace.\n",
    "\n",
    "    Returns:\n",
    "        Modified text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    n_edits = max(1, int(len(words) * edit_fraction))\n",
    "    indices = random.sample(range(len(words)), min(n_edits, len(words)))\n",
    "\n",
    "    replacement_words = [\n",
    "        \"the\", \"court\", \"hereby\", \"finds\", \"that\", \"pursuant\",\n",
    "        \"said\", \"aforementioned\", \"respectively\", \"therein\",\n",
    "    ]\n",
    "\n",
    "    for idx in indices:\n",
    "        words[idx] = random.choice(replacement_words)\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# Create near-duplicates of the first two opinions\n",
    "synthetic_docs = []\n",
    "for op in opinions[:2]:\n",
    "    near_dup = op.copy()\n",
    "    near_dup[\"text\"] = create_near_duplicate(op[\"text\"], edit_fraction=0.05)\n",
    "    near_dup[\"case_name\"] = op[\"case_name\"] + \" [NEAR-DUP]\"\n",
    "    near_dup[\"id\"] = op[\"id\"] + 9000\n",
    "    synthetic_docs.append(near_dup)\n",
    "\n",
    "# Combine original + synthetic\n",
    "augmented_corpus = opinions + synthetic_docs\n",
    "print(f\"Augmented corpus: {len(augmented_corpus)} documents\")\n",
    "print(f\"  Original:  {len(opinions)}\")\n",
    "print(f\"  Synthetic: {len(synthetic_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First: exact dedup will NOT catch the near-duplicates\n",
    "unique_exact, dups_exact = exact_dedup(augmented_corpus)\n",
    "print(\"Exact dedup results:\")\n",
    "print(f\"  Unique:     {len(unique_exact)}\")\n",
    "print(f\"  Duplicates: {len(dups_exact)}\")\n",
    "print(\"  (Exact dedup misses the near-duplicates, as expected.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now: MinHash + LSH catches them\n",
    "near_dups_aug = find_near_duplicates(augmented_corpus, threshold=0.5)\n",
    "print(f\"Near-duplicate pairs found: {len(near_dups_aug)}\")\n",
    "print()\n",
    "for i, j, sim in near_dups_aug:\n",
    "    name_i = augmented_corpus[i][\"case_name\"][:50]\n",
    "    name_j = augmented_corpus[j][\"case_name\"][:50]\n",
    "    print(f\"  Pair (similarity={sim:.3f}):\")\n",
    "    print(f\"    [{i}] {name_i}\")\n",
    "    print(f\"    [{j}] {name_j}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_corpus(\n",
    "    docs: list[dict[str, Any]],\n",
    "    threshold: float = 0.5,\n",
    "    num_perm: int = 128,\n",
    ") -> tuple[list[dict[str, Any]], list[list[int]]]:\n",
    "    \"\"\"Remove near-duplicates from a corpus.\n",
    "\n",
    "    For each cluster of near-duplicates, keeps the first document\n",
    "    (by position in the input list).\n",
    "\n",
    "    Returns:\n",
    "        (deduplicated_docs, clusters) where clusters is a list of\n",
    "        duplicate groups (lists of indices).\n",
    "    \"\"\"\n",
    "    pairs = find_near_duplicates(\n",
    "        docs, threshold=threshold, num_perm=num_perm\n",
    "    )\n",
    "\n",
    "    # Build clusters using union-find\n",
    "    parent: dict[int, int] = {}\n",
    "\n",
    "    def find(x: int) -> int:\n",
    "        while parent.get(x, x) != x:\n",
    "            parent[x] = parent.get(parent[x], parent[x])\n",
    "            x = parent[x]\n",
    "        return x\n",
    "\n",
    "    def union(a: int, b: int) -> None:\n",
    "        ra, rb = find(a), find(b)\n",
    "        if ra != rb:\n",
    "            # Keep the lower index as root (so we keep the earlier document)\n",
    "            if ra < rb:\n",
    "                parent[rb] = ra\n",
    "            else:\n",
    "                parent[ra] = rb\n",
    "\n",
    "    for i, j, _ in pairs:\n",
    "        union(i, j)\n",
    "\n",
    "    # Group by cluster root\n",
    "    clusters_map: dict[int, list[int]] = {}\n",
    "    all_indices = set()\n",
    "    for i, j, _ in pairs:\n",
    "        all_indices.add(i)\n",
    "        all_indices.add(j)\n",
    "\n",
    "    for idx in all_indices:\n",
    "        root = find(idx)\n",
    "        clusters_map.setdefault(root, []).append(idx)\n",
    "\n",
    "    # For each cluster, mark all but the root for removal\n",
    "    to_remove: set[int] = set()\n",
    "    clusters = []\n",
    "    for root, members in clusters_map.items():\n",
    "        members = sorted(set(members))\n",
    "        clusters.append(members)\n",
    "        for idx in members:\n",
    "            if idx != root:\n",
    "                to_remove.add(idx)\n",
    "\n",
    "    deduped = [doc for i, doc in enumerate(docs) if i not in to_remove]\n",
    "    return deduped, clusters\n",
    "\n",
    "\n",
    "# Deduplicate the augmented corpus\n",
    "deduped, clusters = deduplicate_corpus(augmented_corpus, threshold=0.5)\n",
    "print(f\"Before dedup: {len(augmented_corpus)} documents\")\n",
    "print(f\"After dedup:  {len(deduped)} documents\")\n",
    "print(f\"Removed:      {len(augmented_corpus) - len(deduped)} near-duplicates\")\n",
    "print(f\"\\nDuplicate clusters found: {len(clusters)}\")\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print(f\"  Cluster {i + 1}: indices {cluster}\")\n",
    "    for idx in cluster:\n",
    "        print(f\"    {augmented_corpus[idx]['case_name'][:60]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Parameter Tuning\n",
    "\n",
    "MinHash + LSH has two key parameters:\n",
    "\n",
    "- **`num_perm`** -- Number of hash permutations. More permutations give a\n",
    "  more accurate Jaccard estimate but use more memory and time.\n",
    "- **`threshold`** -- Jaccard similarity threshold for considering two\n",
    "  documents as near-duplicates. Lower threshold = more aggressive dedup\n",
    "  (catches more distant duplicates but risks false positives).\n",
    "\n",
    "Let's experiment with both parameters to understand the trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create near-duplicates at varying edit distances\n",
    "edit_levels = [0.02, 0.05, 0.10, 0.20, 0.30]\n",
    "test_docs = list(opinions)  # start with originals\n",
    "\n",
    "for frac in edit_levels:\n",
    "    dup = opinions[0].copy()\n",
    "    dup[\"text\"] = create_near_duplicate(opinions[0][\"text\"], edit_fraction=frac)\n",
    "    dup[\"case_name\"] = f\"{opinions[0]['case_name']} [edit={frac:.0%}]\"\n",
    "    dup[\"id\"] = 9000 + int(frac * 100)\n",
    "    test_docs.append(dup)\n",
    "\n",
    "print(f\"Test corpus: {len(test_docs)} documents\")\n",
    "print(f\"  Original opinions:    {len(opinions)}\")\n",
    "print(f\"  Synthetic variants:   {len(edit_levels)}\")\n",
    "print(f\"  Edit levels tested:   {edit_levels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different threshold values\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "print(f\"{'Threshold':>10s}  {'Pairs Found':>12s}  {'Docs Removed':>13s}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for thresh in thresholds:\n",
    "    pairs = find_near_duplicates(test_docs, threshold=thresh, num_perm=128)\n",
    "    deduped_t, _ = deduplicate_corpus(test_docs, threshold=thresh, num_perm=128)\n",
    "    removed = len(test_docs) - len(deduped_t)\n",
    "    print(f\"{thresh:>10.1f}  {len(pairs):>12d}  {removed:>13d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different num_perm values at a fixed threshold\n",
    "perm_values = [32, 64, 128, 256]\n",
    "fixed_threshold = 0.5\n",
    "\n",
    "print(f\"Threshold fixed at {fixed_threshold}\")\n",
    "print(f\"{'num_perm':>10s}  {'Pairs Found':>12s}  {'Docs Removed':>13s}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for np in perm_values:\n",
    "    pairs = find_near_duplicates(\n",
    "        test_docs, threshold=fixed_threshold, num_perm=np\n",
    "    )\n",
    "    deduped_p, _ = deduplicate_corpus(\n",
    "        test_docs, threshold=fixed_threshold, num_perm=np\n",
    "    )\n",
    "    removed = len(test_docs) - len(deduped_p)\n",
    "    print(f\"{np:>10d}  {len(pairs):>12d}  {removed:>13d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "- **Lower thresholds** catch more near-duplicates (including documents with\n",
    "  larger edit distances) but increase the risk of false positives -- removing\n",
    "  documents that are genuinely different but happen to share legal terminology.\n",
    "- **Higher `num_perm`** gives more precise similarity estimates. With too few\n",
    "  permutations, the Jaccard estimate is noisy and results are less\n",
    "  reproducible.\n",
    "- For legal text, a threshold of **0.5-0.7** and **128 permutations** is a\n",
    "  reasonable starting point. Legal documents naturally share vocabulary\n",
    "  (\"the court finds\", \"summary judgment\"), so overly aggressive dedup can\n",
    "  remove distinct opinions that happen to discuss similar legal standards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise (a): Precision/Recall of Dedup\n",
    "\n",
    "Using the synthetic near-duplicates as ground truth:\n",
    "\n",
    "1. Create a test set with known duplicate pairs at various edit distances\n",
    "   (5%, 10%, 20%, 30%, 50% of words replaced).\n",
    "2. For each combination of `num_perm` (64, 128, 256) and `threshold`\n",
    "   (0.3, 0.5, 0.7, 0.9), compute:\n",
    "   - **Precision**: What fraction of detected pairs are actual duplicates?\n",
    "   - **Recall**: What fraction of actual duplicates are detected?\n",
    "3. Plot precision vs. recall curves for different parameter settings.\n",
    "\n",
    "### Exercise (b): Dedup Ratio by Document Type\n",
    "\n",
    "Legal corpora contain different types of documents with different\n",
    "duplication characteristics:\n",
    "\n",
    "- **Appellate opinions** -- Often unique, but amended opinions create\n",
    "  near-duplicates.\n",
    "- **Orders** -- Short, formulaic documents that may look very similar\n",
    "  across cases.\n",
    "- **Motions** -- Often use template language, leading to high false-positive\n",
    "  rates in dedup.\n",
    "\n",
    "Using the `court` field in our sample data:\n",
    "1. Group documents by court.\n",
    "2. Run dedup within each group and across groups.\n",
    "3. Estimate which court or document type has the highest natural duplication\n",
    "   rate.\n",
    "4. Discuss: should dedup thresholds vary by document type?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
