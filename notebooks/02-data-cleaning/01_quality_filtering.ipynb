{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 01 - Quality Filtering for Legal Text\n",
    "\n",
    "## Why Cleaning Matters: Garbage In, Garbage Out\n",
    "\n",
    "The quality of an LLM's training data has a direct, measurable impact on model\n",
    "performance. The **RefinedWeb** paper (Penedo et al., 2023) demonstrated that a\n",
    "carefully filtered web corpus could match or exceed the performance of curated\n",
    "datasets like The Pile, despite starting from the same raw Common Crawl data.\n",
    "The **FineWeb** paper (Penedo et al., 2024) pushed this further, showing that\n",
    "systematic quality filtering at scale produces state-of-the-art pretraining\n",
    "corpora.\n",
    "\n",
    "### Legal Text Has Unique Noise\n",
    "\n",
    "For a legal AI system like CoCounsel, data cleaning is especially important.\n",
    "Legal documents collected from court filing systems and public records often\n",
    "contain:\n",
    "\n",
    "- **OCR artifacts** -- page numbers, headers, footers, and garbled text from\n",
    "  scanned PDF opinions\n",
    "- **Boilerplate** -- standard jurisdictional headers, procedural notices, and\n",
    "  filing stamps that repeat across thousands of opinions\n",
    "- **Non-English content** -- foreign-language documents mixed into multilingual\n",
    "  court systems\n",
    "- **PII leakage** -- social security numbers, phone numbers, and addresses that\n",
    "  should be redacted before training\n",
    "\n",
    "In this notebook we build a quality filtering pipeline for legal text, drawing\n",
    "on techniques from RefinedWeb and FineWeb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install and import the libraries we need:\n",
    "- **langdetect** -- language identification\n",
    "- **json** / **pathlib** -- loading JSONL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment if needed)\n",
    "# %pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "# Make langdetect deterministic\n",
    "DetectorFactory.seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "We load court opinions from our sample dataset. Each record has an `id`,\n",
    "`case_name`, `court`, `date_filed`, `text`, and `citations` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../../datasets/sample/court_opinions.jsonl\")\n",
    "\n",
    "\n",
    "def load_opinions(path: Path = DATA_PATH) -> list[dict[str, Any]]:\n",
    "    \"\"\"Load court opinions from a JSONL file.\"\"\"\n",
    "    opinions = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                opinions.append(json.loads(line))\n",
    "    return opinions\n",
    "\n",
    "\n",
    "opinions = load_opinions()\n",
    "print(f\"Loaded {len(opinions)} opinions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a sample record\n",
    "sample = opinions[0]\n",
    "print(f\"Case : {sample['case_name']}\")\n",
    "print(f\"Court: {sample['court']}\")\n",
    "print(f\"Date : {sample['date_filed']}\")\n",
    "print(f\"Text length: {len(sample['text']):,} characters\")\n",
    "print(f\"\\nFirst 500 characters:\\n{sample['text'][:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Language Detection\n",
    "\n",
    "The first step in any multilingual pipeline is to identify and filter by\n",
    "language. Even datasets that are nominally English-only can contain\n",
    "foreign-language documents -- especially in legal contexts where courts may\n",
    "reproduce testimony, contracts, or statutes in their original language.\n",
    "\n",
    "We use the `langdetect` library, which implements Google's language detection\n",
    "algorithm. It works well on passages of 50+ words but can be unreliable on\n",
    "very short texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text: str) -> str:\n",
    "    \"\"\"Detect the language of a text string.\n",
    "\n",
    "    Returns a two-letter ISO 639-1 code (e.g. 'en' for English).\n",
    "    Returns 'unknown' if detection fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "# Detect languages for all opinions\n",
    "for opinion in opinions:\n",
    "    opinion[\"language\"] = detect_language(opinion[\"text\"])\n",
    "\n",
    "lang_counts = Counter(op[\"language\"] for op in opinions)\n",
    "print(\"Language distribution:\")\n",
    "for lang, count in lang_counts.most_common():\n",
    "    print(f\"  {lang}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_language(\n",
    "    docs: list[dict[str, Any]], target_lang: str = \"en\"\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"Keep only documents in the target language.\"\"\"\n",
    "    return [doc for doc in docs if doc.get(\"language\") == target_lang]\n",
    "\n",
    "\n",
    "english_opinions = filter_by_language(opinions)\n",
    "print(f\"Before language filter: {len(opinions)} documents\")\n",
    "print(f\"After language filter:  {len(english_opinions)} documents\")\n",
    "print(\n",
    "    f\"Removed: {len(opinions) - len(english_opinions)} non-English documents\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Content Quality Heuristics\n",
    "\n",
    "RefinedWeb and FineWeb apply a battery of heuristic filters to remove\n",
    "low-quality documents. We adapt four of these for legal text:\n",
    "\n",
    "1. **Line length filter** -- documents with very short average line lengths\n",
    "   are often OCR noise or formatting artifacts.\n",
    "2. **Symbol ratio filter** -- a high ratio of special characters suggests\n",
    "   corrupted text or code, not prose.\n",
    "3. **Repetition filter** -- repeated n-grams indicate boilerplate or\n",
    "   extraction errors.\n",
    "4. **Boilerplate filter** -- standard legal headers that appear verbatim\n",
    "   across many documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Filter 1: Line Length\n",
    "\n",
    "Documents with very short average line lengths are often the result of OCR\n",
    "errors, misformatted tables, or extracted metadata rather than prose. We\n",
    "flag documents where the average line length falls below a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_line_length(text: str) -> float:\n",
    "    \"\"\"Compute the average line length (in characters) of a text.\"\"\"\n",
    "    lines = text.split(\"\\n\")\n",
    "    if not lines:\n",
    "        return 0.0\n",
    "    return sum(len(line) for line in lines) / len(lines)\n",
    "\n",
    "\n",
    "def line_length_filter(\n",
    "    docs: list[dict[str, Any]], min_avg_length: float = 40.0\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"Remove documents with average line length below the threshold.\"\"\"\n",
    "    return [\n",
    "        doc for doc in docs if avg_line_length(doc[\"text\"]) >= min_avg_length\n",
    "    ]\n",
    "\n",
    "\n",
    "# Show line length statistics before filtering\n",
    "print(\"Average line lengths:\")\n",
    "for op in english_opinions:\n",
    "    avg = avg_line_length(op[\"text\"])\n",
    "    print(f\"  {op['case_name'][:50]:50s}  {avg:>8.1f} chars\")\n",
    "\n",
    "after_line = line_length_filter(english_opinions)\n",
    "print(f\"\\nBefore: {len(english_opinions)} | After: {len(after_line)}\")\n",
    "print(f\"Removed: {len(english_opinions) - len(after_line)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Filter 2: Symbol Ratio\n",
    "\n",
    "Documents with an unusually high ratio of special characters (symbols,\n",
    "punctuation, digits relative to alphabetic characters) are often corrupted\n",
    "or non-prose content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbol_ratio(text: str) -> float:\n",
    "    \"\"\"Compute the ratio of non-alphanumeric, non-space characters to total.\"\"\"\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    symbols = sum(1 for c in text if not c.isalnum() and not c.isspace())\n",
    "    return symbols / len(text)\n",
    "\n",
    "\n",
    "def symbol_ratio_filter(\n",
    "    docs: list[dict[str, Any]], max_ratio: float = 0.3\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"Remove documents where the symbol ratio exceeds the threshold.\"\"\"\n",
    "    return [doc for doc in docs if symbol_ratio(doc[\"text\"]) <= max_ratio]\n",
    "\n",
    "\n",
    "# Show symbol ratios\n",
    "print(\"Symbol ratios:\")\n",
    "for op in after_line:\n",
    "    ratio = symbol_ratio(op[\"text\"])\n",
    "    print(f\"  {op['case_name'][:50]:50s}  {ratio:.4f}\")\n",
    "\n",
    "after_symbol = symbol_ratio_filter(after_line)\n",
    "print(f\"\\nBefore: {len(after_line)} | After: {len(after_symbol)}\")\n",
    "print(f\"Removed: {len(after_line) - len(after_symbol)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "### Filter 3: Repetition\n",
    "\n",
    "Repeated n-grams are a strong signal of extraction errors or boilerplate.\n",
    "For example, a page header that appears on every page of a scanned PDF\n",
    "will produce many repeated 5-grams. RefinedWeb filters documents where\n",
    "more than a threshold fraction of n-grams are duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeated_ngram_ratio(text: str, n: int = 5) -> float:\n",
    "    \"\"\"Compute the fraction of n-grams that appear more than once.\n",
    "\n",
    "    A high value means the text is repetitive.\n",
    "    \"\"\"\n",
    "    words = text.lower().split()\n",
    "    if len(words) < n:\n",
    "        return 0.0\n",
    "    ngrams = [tuple(words[i : i + n]) for i in range(len(words) - n + 1)]\n",
    "    counts = Counter(ngrams)\n",
    "    repeated = sum(c - 1 for c in counts.values() if c > 1)\n",
    "    return repeated / len(ngrams)\n",
    "\n",
    "\n",
    "def repetition_filter(\n",
    "    docs: list[dict[str, Any]], max_repetition: float = 0.3\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"Remove documents with excessive n-gram repetition.\"\"\"\n",
    "    return [\n",
    "        doc\n",
    "        for doc in docs\n",
    "        if repeated_ngram_ratio(doc[\"text\"]) <= max_repetition\n",
    "    ]\n",
    "\n",
    "\n",
    "# Show repetition ratios\n",
    "print(\"Repetition ratios (5-gram):\")\n",
    "for op in after_symbol:\n",
    "    ratio = repeated_ngram_ratio(op[\"text\"])\n",
    "    print(f\"  {op['case_name'][:50]:50s}  {ratio:.4f}\")\n",
    "\n",
    "after_rep = repetition_filter(after_symbol)\n",
    "print(f\"\\nBefore: {len(after_symbol)} | After: {len(after_rep)}\")\n",
    "print(f\"Removed: {len(after_symbol) - len(after_rep)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Filter 4: Boilerplate Detection\n",
    "\n",
    "Legal opinions often begin with standardized text that is specific to the\n",
    "court or filing system rather than the substantive content of the opinion.\n",
    "We detect common boilerplate patterns and flag documents that consist\n",
    "primarily of boilerplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common boilerplate patterns found in court opinions\n",
    "BOILERPLATE_PATTERNS = [\n",
    "    re.compile(r\"NOT FOR PUBLICATION\", re.IGNORECASE),\n",
    "    re.compile(r\"THIS OPINION IS NOT PRECEDENTIAL\", re.IGNORECASE),\n",
    "    re.compile(r\"FILED\\s+\\w+\\s+\\d{1,2},?\\s+\\d{4}\", re.IGNORECASE),\n",
    "    re.compile(r\"Page\\s+\\d+\\s+of\\s+\\d+\", re.IGNORECASE),\n",
    "    re.compile(\n",
    "        r\"Case\\s+\\d+:\\d+-\\w+-\\d+\\s+Document\\s+\\d+\", re.IGNORECASE\n",
    "    ),\n",
    "    re.compile(r\"UNITED STATES (?:DISTRICT|CIRCUIT) COURT\", re.IGNORECASE),\n",
    "]\n",
    "\n",
    "\n",
    "def boilerplate_score(text: str) -> int:\n",
    "    \"\"\"Count how many boilerplate patterns match in the text.\"\"\"\n",
    "    return sum(1 for pat in BOILERPLATE_PATTERNS if pat.search(text))\n",
    "\n",
    "\n",
    "def boilerplate_filter(\n",
    "    docs: list[dict[str, Any]], max_matches: int = 4\n",
    ") -> list[dict[str, Any]]:\n",
    "    \"\"\"Remove documents that match too many boilerplate patterns.\n",
    "\n",
    "    A high number of matches suggests the document is mostly boilerplate\n",
    "    rather than substantive legal reasoning.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        doc for doc in docs if boilerplate_score(doc[\"text\"]) <= max_matches\n",
    "    ]\n",
    "\n",
    "\n",
    "# Show boilerplate scores\n",
    "print(\"Boilerplate pattern matches:\")\n",
    "for op in after_rep:\n",
    "    score = boilerplate_score(op[\"text\"])\n",
    "    print(f\"  {op['case_name'][:50]:50s}  {score} matches\")\n",
    "\n",
    "after_boilerplate = boilerplate_filter(after_rep)\n",
    "print(f\"\\nBefore: {len(after_rep)} | After: {len(after_boilerplate)}\")\n",
    "print(f\"Removed: {len(after_rep) - len(after_boilerplate)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## PII Detection\n",
    "\n",
    "Training data should be scrubbed of personally identifiable information (PII)\n",
    "to avoid the model memorizing and reproducing sensitive data. We implement\n",
    "regex-based detectors for three common PII types:\n",
    "\n",
    "- **Social Security Numbers** (XXX-XX-XXXX)\n",
    "- **Phone numbers** (various US formats)\n",
    "- **Email addresses**\n",
    "\n",
    "> **Note:** We demonstrate on synthetic examples below. The sample court\n",
    "> opinions do not contain real PII."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "PII_PATTERNS = {\n",
    "    \"ssn\": re.compile(r\"\\b\\d{3}-\\d{2}-\\d{4}\\b\"),\n",
    "    \"phone\": re.compile(\n",
    "        r\"\\b(?:\\+?1[-.\\s]?)?\"\n",
    "        r\"(?:\\(?\\d{3}\\)?[-.\\s]?)\"\n",
    "        r\"\\d{3}[-.\\s]?\\d{4}\\b\"\n",
    "    ),\n",
    "    \"email\": re.compile(\n",
    "        r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def detect_pii(text: str) -> dict[str, list[str]]:\n",
    "    \"\"\"Find all PII matches in a text string.\n",
    "\n",
    "    Returns a dictionary mapping PII type to list of matches.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for pii_type, pattern in PII_PATTERNS.items():\n",
    "        matches = pattern.findall(text)\n",
    "        if matches:\n",
    "            results[pii_type] = matches\n",
    "    return results\n",
    "\n",
    "\n",
    "def redact_pii(text: str) -> str:\n",
    "    \"\"\"Replace all detected PII with redaction markers.\"\"\"\n",
    "    redacted = text\n",
    "    redacted = PII_PATTERNS[\"ssn\"].sub(\"[SSN REDACTED]\", redacted)\n",
    "    redacted = PII_PATTERNS[\"phone\"].sub(\"[PHONE REDACTED]\", redacted)\n",
    "    redacted = PII_PATTERNS[\"email\"].sub(\"[EMAIL REDACTED]\", redacted)\n",
    "    return redacted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate PII detection on synthetic examples\n",
    "# (These are fictional -- never put real PII in notebooks!)\n",
    "synthetic_texts = [\n",
    "    (\n",
    "        \"The plaintiff, John Doe (SSN: 123-45-6789), filed his claim \"\n",
    "        \"on January 15, 2024. He can be reached at john.doe@example.com \"\n",
    "        \"or (555) 123-4567.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Witness testimony was provided by Jane Smith, whose contact \"\n",
    "        \"information is on file with the court.\"\n",
    "    ),\n",
    "    (\n",
    "        \"Records indicate the account holder's SSN is 987-65-4321 and \"\n",
    "        \"the alternate phone number listed is 555-987-6543. Email \"\n",
    "        \"correspondence was sent to legal.team@lawfirm.org.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "for i, text in enumerate(synthetic_texts):\n",
    "    print(f\"--- Example {i + 1} ---\")\n",
    "    pii_found = detect_pii(text)\n",
    "    if pii_found:\n",
    "        for pii_type, matches in pii_found.items():\n",
    "            print(f\"  {pii_type}: {matches}\")\n",
    "        print(f\"\\n  Redacted: {redact_pii(text)}\")\n",
    "    else:\n",
    "        print(\"  No PII detected.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our real sample data for PII\n",
    "print(\"PII scan of sample court opinions:\")\n",
    "for op in opinions:\n",
    "    pii_found = detect_pii(op[\"text\"])\n",
    "    status = \"PII FOUND\" if pii_found else \"clean\"\n",
    "    print(f\"  {op['case_name'][:50]:50s}  [{status}]\")\n",
    "    if pii_found:\n",
    "        for pii_type, matches in pii_found.items():\n",
    "            print(f\"    {pii_type}: {matches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Full Pipeline\n",
    "\n",
    "Now we chain all the filters together into a single `clean_document`\n",
    "function and run the complete pipeline on our sample corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document(\n",
    "    text: str,\n",
    "    *,\n",
    "    target_lang: str = \"en\",\n",
    "    min_avg_line_length: float = 40.0,\n",
    "    max_symbol_ratio: float = 0.3,\n",
    "    max_repetition_ratio: float = 0.3,\n",
    "    max_boilerplate_matches: int = 4,\n",
    "    redact: bool = True,\n",
    ") -> tuple[str | None, dict[str, Any]]:\n",
    "    \"\"\"Run the full cleaning pipeline on a single document.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (cleaned_text, metadata). If the document is filtered\n",
    "        out, cleaned_text is None and metadata contains the reason.\n",
    "    \"\"\"\n",
    "    metadata: dict[str, Any] = {\"filters_passed\": []}\n",
    "\n",
    "    # 1. Language detection\n",
    "    lang = detect_language(text)\n",
    "    metadata[\"language\"] = lang\n",
    "    if lang != target_lang:\n",
    "        metadata[\"filtered_by\"] = \"language\"\n",
    "        return None, metadata\n",
    "    metadata[\"filters_passed\"].append(\"language\")\n",
    "\n",
    "    # 2. Line length\n",
    "    avg_ll = avg_line_length(text)\n",
    "    metadata[\"avg_line_length\"] = avg_ll\n",
    "    if avg_ll < min_avg_line_length:\n",
    "        metadata[\"filtered_by\"] = \"line_length\"\n",
    "        return None, metadata\n",
    "    metadata[\"filters_passed\"].append(\"line_length\")\n",
    "\n",
    "    # 3. Symbol ratio\n",
    "    sr = symbol_ratio(text)\n",
    "    metadata[\"symbol_ratio\"] = sr\n",
    "    if sr > max_symbol_ratio:\n",
    "        metadata[\"filtered_by\"] = \"symbol_ratio\"\n",
    "        return None, metadata\n",
    "    metadata[\"filters_passed\"].append(\"symbol_ratio\")\n",
    "\n",
    "    # 4. Repetition\n",
    "    rr = repeated_ngram_ratio(text)\n",
    "    metadata[\"repetition_ratio\"] = rr\n",
    "    if rr > max_repetition_ratio:\n",
    "        metadata[\"filtered_by\"] = \"repetition\"\n",
    "        return None, metadata\n",
    "    metadata[\"filters_passed\"].append(\"repetition\")\n",
    "\n",
    "    # 5. Boilerplate\n",
    "    bp = boilerplate_score(text)\n",
    "    metadata[\"boilerplate_matches\"] = bp\n",
    "    if bp > max_boilerplate_matches:\n",
    "        metadata[\"filtered_by\"] = \"boilerplate\"\n",
    "        return None, metadata\n",
    "    metadata[\"filters_passed\"].append(\"boilerplate\")\n",
    "\n",
    "    # 6. PII detection and redaction\n",
    "    pii_found = detect_pii(text)\n",
    "    metadata[\"pii_found\"] = bool(pii_found)\n",
    "    if redact and pii_found:\n",
    "        text = redact_pii(text)\n",
    "        metadata[\"pii_redacted\"] = True\n",
    "    metadata[\"filters_passed\"].append(\"pii\")\n",
    "\n",
    "    return text, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full pipeline on the sample corpus\n",
    "cleaned_opinions = []\n",
    "pipeline_stats: dict[str, int] = {\n",
    "    \"total\": len(opinions),\n",
    "    \"passed\": 0,\n",
    "    \"filtered_language\": 0,\n",
    "    \"filtered_line_length\": 0,\n",
    "    \"filtered_symbol_ratio\": 0,\n",
    "    \"filtered_repetition\": 0,\n",
    "    \"filtered_boilerplate\": 0,\n",
    "    \"pii_redacted\": 0,\n",
    "}\n",
    "\n",
    "total_chars_before = 0\n",
    "total_chars_after = 0\n",
    "\n",
    "for op in opinions:\n",
    "    total_chars_before += len(op[\"text\"])\n",
    "    cleaned_text, meta = clean_document(op[\"text\"])\n",
    "\n",
    "    if cleaned_text is not None:\n",
    "        pipeline_stats[\"passed\"] += 1\n",
    "        total_chars_after += len(cleaned_text)\n",
    "        cleaned_opinions.append({**op, \"text\": cleaned_text, \"meta\": meta})\n",
    "        if meta.get(\"pii_redacted\"):\n",
    "            pipeline_stats[\"pii_redacted\"] += 1\n",
    "    else:\n",
    "        reason = meta.get(\"filtered_by\", \"unknown\")\n",
    "        key = f\"filtered_{reason}\"\n",
    "        pipeline_stats[key] = pipeline_stats.get(key, 0) + 1\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total documents:      {pipeline_stats['total']}\")\n",
    "print(f\"Passed all filters:   {pipeline_stats['passed']}\")\n",
    "print(f\"Filtered (language):  {pipeline_stats['filtered_language']}\")\n",
    "print(f\"Filtered (line len):  {pipeline_stats['filtered_line_length']}\")\n",
    "print(f\"Filtered (symbols):   {pipeline_stats['filtered_symbol_ratio']}\")\n",
    "print(f\"Filtered (repetition):{pipeline_stats['filtered_repetition']}\")\n",
    "print(f\"Filtered (boilerplate):{pipeline_stats['filtered_boilerplate']}\")\n",
    "print(f\"PII redacted:         {pipeline_stats['pii_redacted']}\")\n",
    "print(f\"\\nText volume:\")\n",
    "print(f\"  Before: {total_chars_before:>10,} characters\")\n",
    "print(f\"  After:  {total_chars_after:>10,} characters\")\n",
    "if total_chars_before > 0:\n",
    "    pct = (1 - total_chars_after / total_chars_before) * 100\n",
    "    print(f\"  Removed: {pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### Exercise (a): Custom Boilerplate Filter\n",
    "\n",
    "Add a custom filter for court-specific boilerplate. Many courts use\n",
    "standardized headers that appear verbatim in every opinion from that court.\n",
    "For example:\n",
    "\n",
    "```\n",
    "UNITED STATES COURT OF APPEALS FOR THE SEVENTH CIRCUIT\n",
    "No. 24-1234\n",
    "```\n",
    "\n",
    "Write a function that:\n",
    "1. Groups opinions by court.\n",
    "2. Identifies text patterns that appear in **all** opinions from a given court.\n",
    "3. Strips those patterns from the text before further processing.\n",
    "\n",
    "Test it on the sample data and measure how much text is removed.\n",
    "\n",
    "### Exercise (b): Threshold Tuning\n",
    "\n",
    "The quality filters use hard-coded thresholds (`min_avg_line_length=40`,\n",
    "`max_symbol_ratio=0.3`, etc.). Experiment with different values:\n",
    "\n",
    "1. Sweep each threshold across a range of values.\n",
    "2. For each setting, record how many documents pass and the total text volume.\n",
    "3. Plot corpus size vs. threshold to understand the trade-off between\n",
    "   aggressive filtering (higher quality but less data) and permissive\n",
    "   filtering (more data but potentially lower quality).\n",
    "\n",
    "Which thresholds have the most impact on your legal corpus?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
